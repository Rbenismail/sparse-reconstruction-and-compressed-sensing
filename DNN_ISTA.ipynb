{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "64d4921d399b42199fe57b03e3fd4876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a10e99e649e451a97133807acff88a1",
              "IPY_MODEL_d578f42836004f63b66ccbfdba2d4c8e",
              "IPY_MODEL_f6b01c8963a64d32a88370e23927cd46"
            ],
            "layout": "IPY_MODEL_6931cb967e4b4a279ee13f5e0f593852"
          }
        },
        "0a10e99e649e451a97133807acff88a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_facf3932a896480f8a4c4f3311d0e292",
            "placeholder": "​",
            "style": "IPY_MODEL_fdf2802567194d72b0b3f0e95d13e080",
            "value": "100%"
          }
        },
        "d578f42836004f63b66ccbfdba2d4c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60c42754c43e4640b6575ee8802a4538",
            "max": 1500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28157f9e0b0e41d2b110546bde6eb8ab",
            "value": 1500
          }
        },
        "f6b01c8963a64d32a88370e23927cd46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9b507f66f654710bc7277e54c4c7b5d",
            "placeholder": "​",
            "style": "IPY_MODEL_8bf4cd2b9f1b4b3eb441b70bec363262",
            "value": " 1500/1500 [00:17&lt;00:00, 82.29it/s]"
          }
        },
        "6931cb967e4b4a279ee13f5e0f593852": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "facf3932a896480f8a4c4f3311d0e292": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdf2802567194d72b0b3f0e95d13e080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60c42754c43e4640b6575ee8802a4538": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28157f9e0b0e41d2b110546bde6eb8ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9b507f66f654710bc7277e54c4c7b5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bf4cd2b9f1b4b3eb441b70bec363262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries & dependancies\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Fx7YQ7q46kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n"
      ],
      "metadata": {
        "id": "NLFnTO_LFBe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data"
      ],
      "metadata": {
        "id": "gSC0DwRwCI1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=500\n",
        "m=100\n",
        "k=10\n",
        "np.random.seed(12)\n",
        "A = np.random.rand(m, n)\n",
        "x = np.zeros(n)\n",
        "indices = np.random.choice(n, size=k, replace=False)\n",
        "values = np.random.randn(k)\n",
        "x[indices] = values\n",
        "\n",
        "y=A@x+ np.random.normal(0, 1, size=(m,))"
      ],
      "metadata": {
        "id": "HI0iaYO4He-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZHWmmLvd19p",
        "outputId": "5a473cca-2bf8-40a9-caca-21b71bfe1821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.15416284, 0.7400497 , 0.26331502, ..., 0.09204979, 0.08285077,\n",
              "        0.53911986],\n",
              "       [0.30964212, 0.67163854, 0.28185479, ..., 0.86826822, 0.76266046,\n",
              "        0.21399387],\n",
              "       [0.69191492, 0.6328783 , 0.58106497, ..., 0.8094376 , 0.14948792,\n",
              "        0.9370676 ],\n",
              "       ...,\n",
              "       [0.68627919, 0.47962638, 0.83515003, ..., 0.48488857, 0.62103788,\n",
              "        0.00873272],\n",
              "       [0.41390959, 0.41047041, 0.11179232, ..., 0.68323414, 0.73791214,\n",
              "        0.7440044 ],\n",
              "       [0.87741819, 0.39747798, 0.10934323, ..., 0.34997345, 0.98414673,\n",
              "        0.29557335]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvalues = np.linalg.eigvals(A.T@A)\n",
        "max_eig_val=np.max(eigenvalues)\n",
        "max_eig_val"
      ],
      "metadata": {
        "id": "nkiN7e8qefid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7b6b02-b961-4cbc-bd04-0f98792e462e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12511.493163497176+0j)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fwyRYmAC9ar",
        "outputId": "58f64d33-0012-4ee1-8e01-0ac220aba0f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.29072659,  0.5713919 ,  0.19379024,  2.04739526,  1.81798013,\n",
              "       -0.95598171,  1.07480985, -0.14166145, -0.03806401,  3.40330903,\n",
              "        0.68394006,  2.52085208, -0.97795664,  0.7301648 , -0.44498955,\n",
              "       -0.6134693 ,  1.64866167, -0.52495066, -1.44110601,  3.39668846,\n",
              "        0.63034741,  0.01341712,  1.18942838,  2.44936807,  1.05330705,\n",
              "        2.14253785, -0.34845516,  2.20214953, -0.34119959,  3.91086807,\n",
              "        0.6444279 ,  1.68963354,  1.57198997,  0.56980381,  0.72009842,\n",
              "        0.11822223,  1.9484303 ,  0.61771095,  2.47965937,  4.43597362,\n",
              "       -0.09240354,  0.87271281,  1.46281497,  0.5555422 ,  1.25202835,\n",
              "        1.32584618, -0.26062006,  1.15320869,  0.12736543,  0.6106174 ,\n",
              "        1.31187488,  2.60837724,  2.28315479,  0.31051732,  1.06275077,\n",
              "        0.54192044, -1.21219223,  2.66100984,  0.72747214, -0.90733697,\n",
              "        1.79788546,  3.01863154,  0.71682457,  2.96729539,  2.09335203,\n",
              "       -1.69004344,  1.39279679,  2.60128174,  0.13584206,  0.97317039,\n",
              "       -1.3765049 ,  0.14743685, -1.16181027,  0.20658955, -0.21196779,\n",
              "        1.23232562,  0.69667517, -1.59229555,  1.91399846,  1.54164374,\n",
              "        1.85688339, -1.17782171, -0.09163436,  0.14722338,  0.62789241,\n",
              "        1.08484055, -0.09484711, -0.1628345 ,  2.51706191, -0.18187436,\n",
              "        1.36782016,  2.51085687,  2.06685995,  0.88074058,  1.95978211,\n",
              "        1.07723843, -0.48133986,  1.33135612,  0.03749467,  1.39490168])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWQ9VaXJDI7u",
        "outputId": "30af22ed-31c7-47da-ac67-dfcce899d0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  2.18618937,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        , -0.29735911,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  1.77458224,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "       -0.72206718,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        1.91713684,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , -2.03480818,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , -0.76620436,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "       -0.02818969,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        , -0.96235982,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.68138706,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A=torch.Tensor(A)\n",
        "x=torch.Tensor(x)\n",
        "y=torch.Tensor(y)"
      ],
      "metadata": {
        "id": "f1mSLbf9IkRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqXAFs3SIsfe",
        "outputId": "7663507a-ca5e-4b06-8add-626819c9298a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1862,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2974,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  1.7746,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000, -0.7221,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9171,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000, -2.0348,  0.0000,  0.0000,  0.0000,  0.0000, -0.7662,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0282,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000, -0.9624,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.6814,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model"
      ],
      "metadata": {
        "id": "3RnO9vYFB8AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ISTAStep(nn.Module):\n",
        "    def __init__(self, A=A,y=y,alpha=12620,lamda=1):\n",
        "        super(ISTAStep, self).__init__()\n",
        "        self.A = A\n",
        "        self.B = nn.Parameter(torch.tensor(torch.matmul((1/alpha)*A.transpose(0, 1),y), requires_grad=True))\n",
        "        self.H = nn.Parameter(torch.tensor(torch.eye((torch.matmul(A.t(), A)).shape[0]) - (1/alpha) * torch.matmul(A.t(), A),requires_grad=True))\n",
        "        self.t = nn.Parameter(torch.tensor((lamda/alpha)*torch.ones((n,))))\n",
        "        self.y = y\n",
        "    \n",
        "    def soft_thresholding(self, u):\n",
        "        sign = torch.sign(u)\n",
        "        abs_u = torch.abs(u)\n",
        "        thresholded_values = torch.max(abs_u - self.t, torch.zeros_like(abs_u))\n",
        "        thresholded_values = sign * thresholded_values\n",
        "        return thresholded_values   \n",
        "\n",
        "    \n",
        "    def forward(self, x ):\n",
        "        c= self.B+ torch.matmul(self.H,x)\n",
        "        x = self.soft_thresholding(c)\n",
        "        return x\n",
        "\n",
        "\n",
        "class  ISTA(nn.Module):\n",
        "    def __init__(self, T=2, **dnet_args ):\n",
        "        super(ISTA, self).__init__()\n",
        "        self.dnets = nn.ModuleList([ISTAStep( **dnet_args) for i in range(T)])\n",
        "    \n",
        "    def step(self, u, i):\n",
        "        return self.dnets[i].forward(u)\n",
        "    \n",
        "    def forward(self, u):\n",
        "        u=torch.zeros(n)\n",
        "        for i in range(len(self.dnets)):\n",
        "            u = self.step(u, i)\n",
        "        return u"
      ],
      "metadata": {
        "id": "z1BzR5fyB6l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=ISTA()"
      ],
      "metadata": {
        "id": "cjae41K1X86L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0364d593-030c-4da0-f602-a8d7bb1d352e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-39c2cfc46200>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.B = nn.Parameter(torch.tensor(torch.matmul((1/alpha)*A.transpose(0, 1),y), requires_grad=True))\n",
            "<ipython-input-17-39c2cfc46200>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.H = nn.Parameter(torch.tensor(torch.eye((torch.matmul(A.t(), A)).shape[0]) - (1/alpha) * torch.matmul(A.t(), A),requires_grad=True))\n",
            "<ipython-input-17-39c2cfc46200>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.t = nn.Parameter(torch.tensor((lamda/alpha)*torch.ones((n,))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train"
      ],
      "metadata": {
        "id": "-xBvgEtVM7-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(x,y,model: torch.nn.Module, \n",
        "               loss_fn: torch.nn.Module, \n",
        "               optimizer: torch.optim.Optimizer):\n",
        "    model.train()\n",
        "  \n",
        "    train_loss= 0\n",
        "  \n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(y)\n",
        "\n",
        "    loss = loss_fn(output, x)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item() \n",
        "\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "lU_JDetNfDaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(x,y,model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module):\n",
        "    model.eval() \n",
        "  \n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "            output = model(y)\n",
        "            loss = loss_fn(output, x)\n",
        "            test_loss += loss.item()\n",
        " \n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "bl6lX-ThZEMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(x,y,model: torch.nn.Module,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs):\n",
        "    \n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "      \"test_loss\": []\n",
        "    }\n",
        "  \n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss = train_step(x,y,model=model,\n",
        "                                loss_fn=loss_fn,\n",
        "                                optimizer=optimizer)\n",
        "                                \n",
        "        \n",
        "        test_loss = test_step(x,y,model=model,\n",
        "                              loss_fn=loss_fn\n",
        "                              )\n",
        "      \n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ],
      "metadata": {
        "id": "AHb1aNrdZf1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn =   nn.MSELoss()\n",
        "model_resulrs = train(x,y,model=model, \n",
        "     optimizer=optimizer,\n",
        "     loss_fn=loss_fn,\n",
        "     epochs=1500,\n",
        "     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "64d4921d399b42199fe57b03e3fd4876",
            "0a10e99e649e451a97133807acff88a1",
            "d578f42836004f63b66ccbfdba2d4c8e",
            "f6b01c8963a64d32a88370e23927cd46",
            "6931cb967e4b4a279ee13f5e0f593852",
            "facf3932a896480f8a4c4f3311d0e292",
            "fdf2802567194d72b0b3f0e95d13e080",
            "60c42754c43e4640b6575ee8802a4538",
            "28157f9e0b0e41d2b110546bde6eb8ab",
            "f9b507f66f654710bc7277e54c4c7b5d",
            "8bf4cd2b9f1b4b3eb441b70bec363262"
          ]
        },
        "id": "WVJOjUdJaCLm",
        "outputId": "e2bf2798-48aa-4384-837d-7c54d23ba7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64d4921d399b42199fe57b03e3fd4876"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.0366 | test_loss: 0.0365 | \n",
            "Epoch: 2 | train_loss: 0.0365 | test_loss: 0.0365 | \n",
            "Epoch: 3 | train_loss: 0.0365 | test_loss: 0.0365 | \n",
            "Epoch: 4 | train_loss: 0.0365 | test_loss: 0.0365 | \n",
            "Epoch: 5 | train_loss: 0.0365 | test_loss: 0.0364 | \n",
            "Epoch: 6 | train_loss: 0.0364 | test_loss: 0.0364 | \n",
            "Epoch: 7 | train_loss: 0.0364 | test_loss: 0.0364 | \n",
            "Epoch: 8 | train_loss: 0.0364 | test_loss: 0.0364 | \n",
            "Epoch: 9 | train_loss: 0.0364 | test_loss: 0.0364 | \n",
            "Epoch: 10 | train_loss: 0.0364 | test_loss: 0.0363 | \n",
            "Epoch: 11 | train_loss: 0.0363 | test_loss: 0.0363 | \n",
            "Epoch: 12 | train_loss: 0.0363 | test_loss: 0.0363 | \n",
            "Epoch: 13 | train_loss: 0.0363 | test_loss: 0.0363 | \n",
            "Epoch: 14 | train_loss: 0.0363 | test_loss: 0.0363 | \n",
            "Epoch: 15 | train_loss: 0.0363 | test_loss: 0.0363 | \n",
            "Epoch: 16 | train_loss: 0.0363 | test_loss: 0.0363 | \n",
            "Epoch: 17 | train_loss: 0.0363 | test_loss: 0.0362 | \n",
            "Epoch: 18 | train_loss: 0.0362 | test_loss: 0.0362 | \n",
            "Epoch: 19 | train_loss: 0.0362 | test_loss: 0.0362 | \n",
            "Epoch: 20 | train_loss: 0.0362 | test_loss: 0.0362 | \n",
            "Epoch: 21 | train_loss: 0.0362 | test_loss: 0.0362 | \n",
            "Epoch: 22 | train_loss: 0.0362 | test_loss: 0.0362 | \n",
            "Epoch: 23 | train_loss: 0.0362 | test_loss: 0.0362 | \n",
            "Epoch: 24 | train_loss: 0.0362 | test_loss: 0.0362 | \n",
            "Epoch: 25 | train_loss: 0.0362 | test_loss: 0.0362 | \n",
            "Epoch: 26 | train_loss: 0.0362 | test_loss: 0.0361 | \n",
            "Epoch: 27 | train_loss: 0.0361 | test_loss: 0.0361 | \n",
            "Epoch: 28 | train_loss: 0.0361 | test_loss: 0.0361 | \n",
            "Epoch: 29 | train_loss: 0.0361 | test_loss: 0.0361 | \n",
            "Epoch: 30 | train_loss: 0.0361 | test_loss: 0.0361 | \n",
            "Epoch: 31 | train_loss: 0.0361 | test_loss: 0.0361 | \n",
            "Epoch: 32 | train_loss: 0.0361 | test_loss: 0.0361 | \n",
            "Epoch: 33 | train_loss: 0.0361 | test_loss: 0.0361 | \n",
            "Epoch: 34 | train_loss: 0.0361 | test_loss: 0.0361 | \n",
            "Epoch: 35 | train_loss: 0.0361 | test_loss: 0.0361 | \n",
            "Epoch: 36 | train_loss: 0.0361 | test_loss: 0.0360 | \n",
            "Epoch: 37 | train_loss: 0.0360 | test_loss: 0.0360 | \n",
            "Epoch: 38 | train_loss: 0.0360 | test_loss: 0.0360 | \n",
            "Epoch: 39 | train_loss: 0.0360 | test_loss: 0.0360 | \n",
            "Epoch: 40 | train_loss: 0.0360 | test_loss: 0.0360 | \n",
            "Epoch: 41 | train_loss: 0.0360 | test_loss: 0.0360 | \n",
            "Epoch: 42 | train_loss: 0.0360 | test_loss: 0.0360 | \n",
            "Epoch: 43 | train_loss: 0.0360 | test_loss: 0.0360 | \n",
            "Epoch: 44 | train_loss: 0.0360 | test_loss: 0.0360 | \n",
            "Epoch: 45 | train_loss: 0.0360 | test_loss: 0.0359 | \n",
            "Epoch: 46 | train_loss: 0.0359 | test_loss: 0.0359 | \n",
            "Epoch: 47 | train_loss: 0.0359 | test_loss: 0.0359 | \n",
            "Epoch: 48 | train_loss: 0.0359 | test_loss: 0.0359 | \n",
            "Epoch: 49 | train_loss: 0.0359 | test_loss: 0.0359 | \n",
            "Epoch: 50 | train_loss: 0.0359 | test_loss: 0.0359 | \n",
            "Epoch: 51 | train_loss: 0.0359 | test_loss: 0.0359 | \n",
            "Epoch: 52 | train_loss: 0.0359 | test_loss: 0.0359 | \n",
            "Epoch: 53 | train_loss: 0.0359 | test_loss: 0.0359 | \n",
            "Epoch: 54 | train_loss: 0.0359 | test_loss: 0.0358 | \n",
            "Epoch: 55 | train_loss: 0.0358 | test_loss: 0.0358 | \n",
            "Epoch: 56 | train_loss: 0.0358 | test_loss: 0.0358 | \n",
            "Epoch: 57 | train_loss: 0.0358 | test_loss: 0.0358 | \n",
            "Epoch: 58 | train_loss: 0.0358 | test_loss: 0.0358 | \n",
            "Epoch: 59 | train_loss: 0.0358 | test_loss: 0.0358 | \n",
            "Epoch: 60 | train_loss: 0.0358 | test_loss: 0.0358 | \n",
            "Epoch: 61 | train_loss: 0.0358 | test_loss: 0.0358 | \n",
            "Epoch: 62 | train_loss: 0.0358 | test_loss: 0.0358 | \n",
            "Epoch: 63 | train_loss: 0.0358 | test_loss: 0.0357 | \n",
            "Epoch: 64 | train_loss: 0.0357 | test_loss: 0.0357 | \n",
            "Epoch: 65 | train_loss: 0.0357 | test_loss: 0.0357 | \n",
            "Epoch: 66 | train_loss: 0.0357 | test_loss: 0.0357 | \n",
            "Epoch: 67 | train_loss: 0.0357 | test_loss: 0.0357 | \n",
            "Epoch: 68 | train_loss: 0.0357 | test_loss: 0.0357 | \n",
            "Epoch: 69 | train_loss: 0.0357 | test_loss: 0.0357 | \n",
            "Epoch: 70 | train_loss: 0.0357 | test_loss: 0.0357 | \n",
            "Epoch: 71 | train_loss: 0.0357 | test_loss: 0.0356 | \n",
            "Epoch: 72 | train_loss: 0.0356 | test_loss: 0.0356 | \n",
            "Epoch: 73 | train_loss: 0.0356 | test_loss: 0.0356 | \n",
            "Epoch: 74 | train_loss: 0.0356 | test_loss: 0.0356 | \n",
            "Epoch: 75 | train_loss: 0.0356 | test_loss: 0.0356 | \n",
            "Epoch: 76 | train_loss: 0.0356 | test_loss: 0.0356 | \n",
            "Epoch: 77 | train_loss: 0.0356 | test_loss: 0.0356 | \n",
            "Epoch: 78 | train_loss: 0.0356 | test_loss: 0.0356 | \n",
            "Epoch: 79 | train_loss: 0.0356 | test_loss: 0.0355 | \n",
            "Epoch: 80 | train_loss: 0.0355 | test_loss: 0.0355 | \n",
            "Epoch: 81 | train_loss: 0.0355 | test_loss: 0.0355 | \n",
            "Epoch: 82 | train_loss: 0.0355 | test_loss: 0.0355 | \n",
            "Epoch: 83 | train_loss: 0.0355 | test_loss: 0.0355 | \n",
            "Epoch: 84 | train_loss: 0.0355 | test_loss: 0.0355 | \n",
            "Epoch: 85 | train_loss: 0.0355 | test_loss: 0.0355 | \n",
            "Epoch: 86 | train_loss: 0.0355 | test_loss: 0.0355 | \n",
            "Epoch: 87 | train_loss: 0.0355 | test_loss: 0.0354 | \n",
            "Epoch: 88 | train_loss: 0.0354 | test_loss: 0.0354 | \n",
            "Epoch: 89 | train_loss: 0.0354 | test_loss: 0.0354 | \n",
            "Epoch: 90 | train_loss: 0.0354 | test_loss: 0.0354 | \n",
            "Epoch: 91 | train_loss: 0.0354 | test_loss: 0.0354 | \n",
            "Epoch: 92 | train_loss: 0.0354 | test_loss: 0.0354 | \n",
            "Epoch: 93 | train_loss: 0.0354 | test_loss: 0.0354 | \n",
            "Epoch: 94 | train_loss: 0.0354 | test_loss: 0.0353 | \n",
            "Epoch: 95 | train_loss: 0.0353 | test_loss: 0.0353 | \n",
            "Epoch: 96 | train_loss: 0.0353 | test_loss: 0.0353 | \n",
            "Epoch: 97 | train_loss: 0.0353 | test_loss: 0.0353 | \n",
            "Epoch: 98 | train_loss: 0.0353 | test_loss: 0.0353 | \n",
            "Epoch: 99 | train_loss: 0.0353 | test_loss: 0.0353 | \n",
            "Epoch: 100 | train_loss: 0.0353 | test_loss: 0.0353 | \n",
            "Epoch: 101 | train_loss: 0.0353 | test_loss: 0.0353 | \n",
            "Epoch: 102 | train_loss: 0.0353 | test_loss: 0.0352 | \n",
            "Epoch: 103 | train_loss: 0.0352 | test_loss: 0.0352 | \n",
            "Epoch: 104 | train_loss: 0.0352 | test_loss: 0.0352 | \n",
            "Epoch: 105 | train_loss: 0.0352 | test_loss: 0.0352 | \n",
            "Epoch: 106 | train_loss: 0.0352 | test_loss: 0.0352 | \n",
            "Epoch: 107 | train_loss: 0.0352 | test_loss: 0.0352 | \n",
            "Epoch: 108 | train_loss: 0.0352 | test_loss: 0.0352 | \n",
            "Epoch: 109 | train_loss: 0.0352 | test_loss: 0.0351 | \n",
            "Epoch: 110 | train_loss: 0.0351 | test_loss: 0.0351 | \n",
            "Epoch: 111 | train_loss: 0.0351 | test_loss: 0.0351 | \n",
            "Epoch: 112 | train_loss: 0.0351 | test_loss: 0.0351 | \n",
            "Epoch: 113 | train_loss: 0.0351 | test_loss: 0.0351 | \n",
            "Epoch: 114 | train_loss: 0.0351 | test_loss: 0.0351 | \n",
            "Epoch: 115 | train_loss: 0.0351 | test_loss: 0.0351 | \n",
            "Epoch: 116 | train_loss: 0.0351 | test_loss: 0.0350 | \n",
            "Epoch: 117 | train_loss: 0.0350 | test_loss: 0.0350 | \n",
            "Epoch: 118 | train_loss: 0.0350 | test_loss: 0.0350 | \n",
            "Epoch: 119 | train_loss: 0.0350 | test_loss: 0.0350 | \n",
            "Epoch: 120 | train_loss: 0.0350 | test_loss: 0.0350 | \n",
            "Epoch: 121 | train_loss: 0.0350 | test_loss: 0.0350 | \n",
            "Epoch: 122 | train_loss: 0.0350 | test_loss: 0.0349 | \n",
            "Epoch: 123 | train_loss: 0.0349 | test_loss: 0.0349 | \n",
            "Epoch: 124 | train_loss: 0.0349 | test_loss: 0.0349 | \n",
            "Epoch: 125 | train_loss: 0.0349 | test_loss: 0.0349 | \n",
            "Epoch: 126 | train_loss: 0.0349 | test_loss: 0.0349 | \n",
            "Epoch: 127 | train_loss: 0.0349 | test_loss: 0.0349 | \n",
            "Epoch: 128 | train_loss: 0.0349 | test_loss: 0.0349 | \n",
            "Epoch: 129 | train_loss: 0.0349 | test_loss: 0.0348 | \n",
            "Epoch: 130 | train_loss: 0.0348 | test_loss: 0.0348 | \n",
            "Epoch: 131 | train_loss: 0.0348 | test_loss: 0.0348 | \n",
            "Epoch: 132 | train_loss: 0.0348 | test_loss: 0.0348 | \n",
            "Epoch: 133 | train_loss: 0.0348 | test_loss: 0.0348 | \n",
            "Epoch: 134 | train_loss: 0.0348 | test_loss: 0.0348 | \n",
            "Epoch: 135 | train_loss: 0.0348 | test_loss: 0.0347 | \n",
            "Epoch: 136 | train_loss: 0.0347 | test_loss: 0.0347 | \n",
            "Epoch: 137 | train_loss: 0.0347 | test_loss: 0.0347 | \n",
            "Epoch: 138 | train_loss: 0.0347 | test_loss: 0.0347 | \n",
            "Epoch: 139 | train_loss: 0.0347 | test_loss: 0.0347 | \n",
            "Epoch: 140 | train_loss: 0.0347 | test_loss: 0.0347 | \n",
            "Epoch: 141 | train_loss: 0.0347 | test_loss: 0.0346 | \n",
            "Epoch: 142 | train_loss: 0.0346 | test_loss: 0.0346 | \n",
            "Epoch: 143 | train_loss: 0.0346 | test_loss: 0.0346 | \n",
            "Epoch: 144 | train_loss: 0.0346 | test_loss: 0.0346 | \n",
            "Epoch: 145 | train_loss: 0.0346 | test_loss: 0.0346 | \n",
            "Epoch: 146 | train_loss: 0.0346 | test_loss: 0.0346 | \n",
            "Epoch: 147 | train_loss: 0.0346 | test_loss: 0.0345 | \n",
            "Epoch: 148 | train_loss: 0.0345 | test_loss: 0.0345 | \n",
            "Epoch: 149 | train_loss: 0.0345 | test_loss: 0.0345 | \n",
            "Epoch: 150 | train_loss: 0.0345 | test_loss: 0.0345 | \n",
            "Epoch: 151 | train_loss: 0.0345 | test_loss: 0.0345 | \n",
            "Epoch: 152 | train_loss: 0.0345 | test_loss: 0.0344 | \n",
            "Epoch: 153 | train_loss: 0.0344 | test_loss: 0.0344 | \n",
            "Epoch: 154 | train_loss: 0.0344 | test_loss: 0.0344 | \n",
            "Epoch: 155 | train_loss: 0.0344 | test_loss: 0.0344 | \n",
            "Epoch: 156 | train_loss: 0.0344 | test_loss: 0.0343 | \n",
            "Epoch: 157 | train_loss: 0.0343 | test_loss: 0.0343 | \n",
            "Epoch: 158 | train_loss: 0.0343 | test_loss: 0.0343 | \n",
            "Epoch: 159 | train_loss: 0.0343 | test_loss: 0.0343 | \n",
            "Epoch: 160 | train_loss: 0.0343 | test_loss: 0.0343 | \n",
            "Epoch: 161 | train_loss: 0.0343 | test_loss: 0.0342 | \n",
            "Epoch: 162 | train_loss: 0.0342 | test_loss: 0.0342 | \n",
            "Epoch: 163 | train_loss: 0.0342 | test_loss: 0.0342 | \n",
            "Epoch: 164 | train_loss: 0.0342 | test_loss: 0.0342 | \n",
            "Epoch: 165 | train_loss: 0.0342 | test_loss: 0.0341 | \n",
            "Epoch: 166 | train_loss: 0.0341 | test_loss: 0.0341 | \n",
            "Epoch: 167 | train_loss: 0.0341 | test_loss: 0.0341 | \n",
            "Epoch: 168 | train_loss: 0.0341 | test_loss: 0.0341 | \n",
            "Epoch: 169 | train_loss: 0.0341 | test_loss: 0.0340 | \n",
            "Epoch: 170 | train_loss: 0.0340 | test_loss: 0.0340 | \n",
            "Epoch: 171 | train_loss: 0.0340 | test_loss: 0.0340 | \n",
            "Epoch: 172 | train_loss: 0.0340 | test_loss: 0.0340 | \n",
            "Epoch: 173 | train_loss: 0.0340 | test_loss: 0.0339 | \n",
            "Epoch: 174 | train_loss: 0.0339 | test_loss: 0.0339 | \n",
            "Epoch: 175 | train_loss: 0.0339 | test_loss: 0.0339 | \n",
            "Epoch: 176 | train_loss: 0.0339 | test_loss: 0.0338 | \n",
            "Epoch: 177 | train_loss: 0.0338 | test_loss: 0.0338 | \n",
            "Epoch: 178 | train_loss: 0.0338 | test_loss: 0.0338 | \n",
            "Epoch: 179 | train_loss: 0.0338 | test_loss: 0.0337 | \n",
            "Epoch: 180 | train_loss: 0.0337 | test_loss: 0.0337 | \n",
            "Epoch: 181 | train_loss: 0.0337 | test_loss: 0.0337 | \n",
            "Epoch: 182 | train_loss: 0.0337 | test_loss: 0.0337 | \n",
            "Epoch: 183 | train_loss: 0.0337 | test_loss: 0.0336 | \n",
            "Epoch: 184 | train_loss: 0.0336 | test_loss: 0.0336 | \n",
            "Epoch: 185 | train_loss: 0.0336 | test_loss: 0.0336 | \n",
            "Epoch: 186 | train_loss: 0.0336 | test_loss: 0.0335 | \n",
            "Epoch: 187 | train_loss: 0.0335 | test_loss: 0.0335 | \n",
            "Epoch: 188 | train_loss: 0.0335 | test_loss: 0.0335 | \n",
            "Epoch: 189 | train_loss: 0.0335 | test_loss: 0.0334 | \n",
            "Epoch: 190 | train_loss: 0.0334 | test_loss: 0.0334 | \n",
            "Epoch: 191 | train_loss: 0.0334 | test_loss: 0.0334 | \n",
            "Epoch: 192 | train_loss: 0.0334 | test_loss: 0.0333 | \n",
            "Epoch: 193 | train_loss: 0.0333 | test_loss: 0.0333 | \n",
            "Epoch: 194 | train_loss: 0.0333 | test_loss: 0.0333 | \n",
            "Epoch: 195 | train_loss: 0.0333 | test_loss: 0.0332 | \n",
            "Epoch: 196 | train_loss: 0.0332 | test_loss: 0.0332 | \n",
            "Epoch: 197 | train_loss: 0.0332 | test_loss: 0.0332 | \n",
            "Epoch: 198 | train_loss: 0.0332 | test_loss: 0.0331 | \n",
            "Epoch: 199 | train_loss: 0.0331 | test_loss: 0.0331 | \n",
            "Epoch: 200 | train_loss: 0.0331 | test_loss: 0.0331 | \n",
            "Epoch: 201 | train_loss: 0.0331 | test_loss: 0.0330 | \n",
            "Epoch: 202 | train_loss: 0.0330 | test_loss: 0.0330 | \n",
            "Epoch: 203 | train_loss: 0.0330 | test_loss: 0.0330 | \n",
            "Epoch: 204 | train_loss: 0.0330 | test_loss: 0.0329 | \n",
            "Epoch: 205 | train_loss: 0.0329 | test_loss: 0.0329 | \n",
            "Epoch: 206 | train_loss: 0.0329 | test_loss: 0.0329 | \n",
            "Epoch: 207 | train_loss: 0.0329 | test_loss: 0.0328 | \n",
            "Epoch: 208 | train_loss: 0.0328 | test_loss: 0.0328 | \n",
            "Epoch: 209 | train_loss: 0.0328 | test_loss: 0.0328 | \n",
            "Epoch: 210 | train_loss: 0.0328 | test_loss: 0.0327 | \n",
            "Epoch: 211 | train_loss: 0.0327 | test_loss: 0.0327 | \n",
            "Epoch: 212 | train_loss: 0.0327 | test_loss: 0.0327 | \n",
            "Epoch: 213 | train_loss: 0.0327 | test_loss: 0.0326 | \n",
            "Epoch: 214 | train_loss: 0.0326 | test_loss: 0.0326 | \n",
            "Epoch: 215 | train_loss: 0.0326 | test_loss: 0.0326 | \n",
            "Epoch: 216 | train_loss: 0.0326 | test_loss: 0.0325 | \n",
            "Epoch: 217 | train_loss: 0.0325 | test_loss: 0.0325 | \n",
            "Epoch: 218 | train_loss: 0.0325 | test_loss: 0.0325 | \n",
            "Epoch: 219 | train_loss: 0.0325 | test_loss: 0.0324 | \n",
            "Epoch: 220 | train_loss: 0.0324 | test_loss: 0.0324 | \n",
            "Epoch: 221 | train_loss: 0.0324 | test_loss: 0.0324 | \n",
            "Epoch: 222 | train_loss: 0.0324 | test_loss: 0.0323 | \n",
            "Epoch: 223 | train_loss: 0.0323 | test_loss: 0.0323 | \n",
            "Epoch: 224 | train_loss: 0.0323 | test_loss: 0.0323 | \n",
            "Epoch: 225 | train_loss: 0.0323 | test_loss: 0.0322 | \n",
            "Epoch: 226 | train_loss: 0.0322 | test_loss: 0.0322 | \n",
            "Epoch: 227 | train_loss: 0.0322 | test_loss: 0.0322 | \n",
            "Epoch: 228 | train_loss: 0.0322 | test_loss: 0.0321 | \n",
            "Epoch: 229 | train_loss: 0.0321 | test_loss: 0.0321 | \n",
            "Epoch: 230 | train_loss: 0.0321 | test_loss: 0.0321 | \n",
            "Epoch: 231 | train_loss: 0.0321 | test_loss: 0.0320 | \n",
            "Epoch: 232 | train_loss: 0.0320 | test_loss: 0.0320 | \n",
            "Epoch: 233 | train_loss: 0.0320 | test_loss: 0.0320 | \n",
            "Epoch: 234 | train_loss: 0.0320 | test_loss: 0.0319 | \n",
            "Epoch: 235 | train_loss: 0.0319 | test_loss: 0.0319 | \n",
            "Epoch: 236 | train_loss: 0.0319 | test_loss: 0.0319 | \n",
            "Epoch: 237 | train_loss: 0.0319 | test_loss: 0.0318 | \n",
            "Epoch: 238 | train_loss: 0.0318 | test_loss: 0.0318 | \n",
            "Epoch: 239 | train_loss: 0.0318 | test_loss: 0.0318 | \n",
            "Epoch: 240 | train_loss: 0.0318 | test_loss: 0.0317 | \n",
            "Epoch: 241 | train_loss: 0.0317 | test_loss: 0.0317 | \n",
            "Epoch: 242 | train_loss: 0.0317 | test_loss: 0.0316 | \n",
            "Epoch: 243 | train_loss: 0.0316 | test_loss: 0.0316 | \n",
            "Epoch: 244 | train_loss: 0.0316 | test_loss: 0.0316 | \n",
            "Epoch: 245 | train_loss: 0.0316 | test_loss: 0.0315 | \n",
            "Epoch: 246 | train_loss: 0.0315 | test_loss: 0.0315 | \n",
            "Epoch: 247 | train_loss: 0.0315 | test_loss: 0.0315 | \n",
            "Epoch: 248 | train_loss: 0.0315 | test_loss: 0.0314 | \n",
            "Epoch: 249 | train_loss: 0.0314 | test_loss: 0.0314 | \n",
            "Epoch: 250 | train_loss: 0.0314 | test_loss: 0.0314 | \n",
            "Epoch: 251 | train_loss: 0.0314 | test_loss: 0.0313 | \n",
            "Epoch: 252 | train_loss: 0.0313 | test_loss: 0.0313 | \n",
            "Epoch: 253 | train_loss: 0.0313 | test_loss: 0.0313 | \n",
            "Epoch: 254 | train_loss: 0.0313 | test_loss: 0.0312 | \n",
            "Epoch: 255 | train_loss: 0.0312 | test_loss: 0.0312 | \n",
            "Epoch: 256 | train_loss: 0.0312 | test_loss: 0.0312 | \n",
            "Epoch: 257 | train_loss: 0.0312 | test_loss: 0.0311 | \n",
            "Epoch: 258 | train_loss: 0.0311 | test_loss: 0.0311 | \n",
            "Epoch: 259 | train_loss: 0.0311 | test_loss: 0.0311 | \n",
            "Epoch: 260 | train_loss: 0.0311 | test_loss: 0.0310 | \n",
            "Epoch: 261 | train_loss: 0.0310 | test_loss: 0.0310 | \n",
            "Epoch: 262 | train_loss: 0.0310 | test_loss: 0.0310 | \n",
            "Epoch: 263 | train_loss: 0.0310 | test_loss: 0.0309 | \n",
            "Epoch: 264 | train_loss: 0.0309 | test_loss: 0.0309 | \n",
            "Epoch: 265 | train_loss: 0.0309 | test_loss: 0.0309 | \n",
            "Epoch: 266 | train_loss: 0.0309 | test_loss: 0.0308 | \n",
            "Epoch: 267 | train_loss: 0.0308 | test_loss: 0.0308 | \n",
            "Epoch: 268 | train_loss: 0.0308 | test_loss: 0.0307 | \n",
            "Epoch: 269 | train_loss: 0.0307 | test_loss: 0.0307 | \n",
            "Epoch: 270 | train_loss: 0.0307 | test_loss: 0.0307 | \n",
            "Epoch: 271 | train_loss: 0.0307 | test_loss: 0.0306 | \n",
            "Epoch: 272 | train_loss: 0.0306 | test_loss: 0.0306 | \n",
            "Epoch: 273 | train_loss: 0.0306 | test_loss: 0.0306 | \n",
            "Epoch: 274 | train_loss: 0.0306 | test_loss: 0.0305 | \n",
            "Epoch: 275 | train_loss: 0.0305 | test_loss: 0.0305 | \n",
            "Epoch: 276 | train_loss: 0.0305 | test_loss: 0.0305 | \n",
            "Epoch: 277 | train_loss: 0.0305 | test_loss: 0.0304 | \n",
            "Epoch: 278 | train_loss: 0.0304 | test_loss: 0.0304 | \n",
            "Epoch: 279 | train_loss: 0.0304 | test_loss: 0.0304 | \n",
            "Epoch: 280 | train_loss: 0.0304 | test_loss: 0.0303 | \n",
            "Epoch: 281 | train_loss: 0.0303 | test_loss: 0.0303 | \n",
            "Epoch: 282 | train_loss: 0.0303 | test_loss: 0.0302 | \n",
            "Epoch: 283 | train_loss: 0.0302 | test_loss: 0.0302 | \n",
            "Epoch: 284 | train_loss: 0.0302 | test_loss: 0.0302 | \n",
            "Epoch: 285 | train_loss: 0.0302 | test_loss: 0.0301 | \n",
            "Epoch: 286 | train_loss: 0.0301 | test_loss: 0.0301 | \n",
            "Epoch: 287 | train_loss: 0.0301 | test_loss: 0.0301 | \n",
            "Epoch: 288 | train_loss: 0.0301 | test_loss: 0.0300 | \n",
            "Epoch: 289 | train_loss: 0.0300 | test_loss: 0.0300 | \n",
            "Epoch: 290 | train_loss: 0.0300 | test_loss: 0.0300 | \n",
            "Epoch: 291 | train_loss: 0.0300 | test_loss: 0.0299 | \n",
            "Epoch: 292 | train_loss: 0.0299 | test_loss: 0.0299 | \n",
            "Epoch: 293 | train_loss: 0.0299 | test_loss: 0.0299 | \n",
            "Epoch: 294 | train_loss: 0.0299 | test_loss: 0.0298 | \n",
            "Epoch: 295 | train_loss: 0.0298 | test_loss: 0.0298 | \n",
            "Epoch: 296 | train_loss: 0.0298 | test_loss: 0.0297 | \n",
            "Epoch: 297 | train_loss: 0.0297 | test_loss: 0.0297 | \n",
            "Epoch: 298 | train_loss: 0.0297 | test_loss: 0.0297 | \n",
            "Epoch: 299 | train_loss: 0.0297 | test_loss: 0.0296 | \n",
            "Epoch: 300 | train_loss: 0.0296 | test_loss: 0.0296 | \n",
            "Epoch: 301 | train_loss: 0.0296 | test_loss: 0.0296 | \n",
            "Epoch: 302 | train_loss: 0.0296 | test_loss: 0.0295 | \n",
            "Epoch: 303 | train_loss: 0.0295 | test_loss: 0.0295 | \n",
            "Epoch: 304 | train_loss: 0.0295 | test_loss: 0.0295 | \n",
            "Epoch: 305 | train_loss: 0.0295 | test_loss: 0.0294 | \n",
            "Epoch: 306 | train_loss: 0.0294 | test_loss: 0.0294 | \n",
            "Epoch: 307 | train_loss: 0.0294 | test_loss: 0.0293 | \n",
            "Epoch: 308 | train_loss: 0.0293 | test_loss: 0.0293 | \n",
            "Epoch: 309 | train_loss: 0.0293 | test_loss: 0.0293 | \n",
            "Epoch: 310 | train_loss: 0.0293 | test_loss: 0.0292 | \n",
            "Epoch: 311 | train_loss: 0.0292 | test_loss: 0.0292 | \n",
            "Epoch: 312 | train_loss: 0.0292 | test_loss: 0.0292 | \n",
            "Epoch: 313 | train_loss: 0.0292 | test_loss: 0.0291 | \n",
            "Epoch: 314 | train_loss: 0.0291 | test_loss: 0.0291 | \n",
            "Epoch: 315 | train_loss: 0.0291 | test_loss: 0.0291 | \n",
            "Epoch: 316 | train_loss: 0.0291 | test_loss: 0.0290 | \n",
            "Epoch: 317 | train_loss: 0.0290 | test_loss: 0.0290 | \n",
            "Epoch: 318 | train_loss: 0.0290 | test_loss: 0.0289 | \n",
            "Epoch: 319 | train_loss: 0.0289 | test_loss: 0.0289 | \n",
            "Epoch: 320 | train_loss: 0.0289 | test_loss: 0.0289 | \n",
            "Epoch: 321 | train_loss: 0.0289 | test_loss: 0.0288 | \n",
            "Epoch: 322 | train_loss: 0.0288 | test_loss: 0.0288 | \n",
            "Epoch: 323 | train_loss: 0.0288 | test_loss: 0.0288 | \n",
            "Epoch: 324 | train_loss: 0.0288 | test_loss: 0.0287 | \n",
            "Epoch: 325 | train_loss: 0.0287 | test_loss: 0.0287 | \n",
            "Epoch: 326 | train_loss: 0.0287 | test_loss: 0.0287 | \n",
            "Epoch: 327 | train_loss: 0.0287 | test_loss: 0.0286 | \n",
            "Epoch: 328 | train_loss: 0.0286 | test_loss: 0.0286 | \n",
            "Epoch: 329 | train_loss: 0.0286 | test_loss: 0.0285 | \n",
            "Epoch: 330 | train_loss: 0.0285 | test_loss: 0.0285 | \n",
            "Epoch: 331 | train_loss: 0.0285 | test_loss: 0.0285 | \n",
            "Epoch: 332 | train_loss: 0.0285 | test_loss: 0.0284 | \n",
            "Epoch: 333 | train_loss: 0.0284 | test_loss: 0.0284 | \n",
            "Epoch: 334 | train_loss: 0.0284 | test_loss: 0.0284 | \n",
            "Epoch: 335 | train_loss: 0.0284 | test_loss: 0.0283 | \n",
            "Epoch: 336 | train_loss: 0.0283 | test_loss: 0.0283 | \n",
            "Epoch: 337 | train_loss: 0.0283 | test_loss: 0.0282 | \n",
            "Epoch: 338 | train_loss: 0.0282 | test_loss: 0.0282 | \n",
            "Epoch: 339 | train_loss: 0.0282 | test_loss: 0.0282 | \n",
            "Epoch: 340 | train_loss: 0.0282 | test_loss: 0.0281 | \n",
            "Epoch: 341 | train_loss: 0.0281 | test_loss: 0.0281 | \n",
            "Epoch: 342 | train_loss: 0.0281 | test_loss: 0.0281 | \n",
            "Epoch: 343 | train_loss: 0.0281 | test_loss: 0.0280 | \n",
            "Epoch: 344 | train_loss: 0.0280 | test_loss: 0.0280 | \n",
            "Epoch: 345 | train_loss: 0.0280 | test_loss: 0.0279 | \n",
            "Epoch: 346 | train_loss: 0.0279 | test_loss: 0.0279 | \n",
            "Epoch: 347 | train_loss: 0.0279 | test_loss: 0.0279 | \n",
            "Epoch: 348 | train_loss: 0.0279 | test_loss: 0.0278 | \n",
            "Epoch: 349 | train_loss: 0.0278 | test_loss: 0.0278 | \n",
            "Epoch: 350 | train_loss: 0.0278 | test_loss: 0.0278 | \n",
            "Epoch: 351 | train_loss: 0.0278 | test_loss: 0.0277 | \n",
            "Epoch: 352 | train_loss: 0.0277 | test_loss: 0.0277 | \n",
            "Epoch: 353 | train_loss: 0.0277 | test_loss: 0.0276 | \n",
            "Epoch: 354 | train_loss: 0.0276 | test_loss: 0.0276 | \n",
            "Epoch: 355 | train_loss: 0.0276 | test_loss: 0.0276 | \n",
            "Epoch: 356 | train_loss: 0.0276 | test_loss: 0.0275 | \n",
            "Epoch: 357 | train_loss: 0.0275 | test_loss: 0.0275 | \n",
            "Epoch: 358 | train_loss: 0.0275 | test_loss: 0.0275 | \n",
            "Epoch: 359 | train_loss: 0.0275 | test_loss: 0.0274 | \n",
            "Epoch: 360 | train_loss: 0.0274 | test_loss: 0.0274 | \n",
            "Epoch: 361 | train_loss: 0.0274 | test_loss: 0.0273 | \n",
            "Epoch: 362 | train_loss: 0.0273 | test_loss: 0.0273 | \n",
            "Epoch: 363 | train_loss: 0.0273 | test_loss: 0.0273 | \n",
            "Epoch: 364 | train_loss: 0.0273 | test_loss: 0.0272 | \n",
            "Epoch: 365 | train_loss: 0.0272 | test_loss: 0.0272 | \n",
            "Epoch: 366 | train_loss: 0.0272 | test_loss: 0.0272 | \n",
            "Epoch: 367 | train_loss: 0.0272 | test_loss: 0.0271 | \n",
            "Epoch: 368 | train_loss: 0.0271 | test_loss: 0.0271 | \n",
            "Epoch: 369 | train_loss: 0.0271 | test_loss: 0.0270 | \n",
            "Epoch: 370 | train_loss: 0.0270 | test_loss: 0.0270 | \n",
            "Epoch: 371 | train_loss: 0.0270 | test_loss: 0.0270 | \n",
            "Epoch: 372 | train_loss: 0.0270 | test_loss: 0.0269 | \n",
            "Epoch: 373 | train_loss: 0.0269 | test_loss: 0.0269 | \n",
            "Epoch: 374 | train_loss: 0.0269 | test_loss: 0.0269 | \n",
            "Epoch: 375 | train_loss: 0.0269 | test_loss: 0.0268 | \n",
            "Epoch: 376 | train_loss: 0.0268 | test_loss: 0.0268 | \n",
            "Epoch: 377 | train_loss: 0.0268 | test_loss: 0.0267 | \n",
            "Epoch: 378 | train_loss: 0.0267 | test_loss: 0.0267 | \n",
            "Epoch: 379 | train_loss: 0.0267 | test_loss: 0.0267 | \n",
            "Epoch: 380 | train_loss: 0.0267 | test_loss: 0.0266 | \n",
            "Epoch: 381 | train_loss: 0.0266 | test_loss: 0.0266 | \n",
            "Epoch: 382 | train_loss: 0.0266 | test_loss: 0.0265 | \n",
            "Epoch: 383 | train_loss: 0.0265 | test_loss: 0.0265 | \n",
            "Epoch: 384 | train_loss: 0.0265 | test_loss: 0.0265 | \n",
            "Epoch: 385 | train_loss: 0.0265 | test_loss: 0.0264 | \n",
            "Epoch: 386 | train_loss: 0.0264 | test_loss: 0.0264 | \n",
            "Epoch: 387 | train_loss: 0.0264 | test_loss: 0.0264 | \n",
            "Epoch: 388 | train_loss: 0.0264 | test_loss: 0.0263 | \n",
            "Epoch: 389 | train_loss: 0.0263 | test_loss: 0.0263 | \n",
            "Epoch: 390 | train_loss: 0.0263 | test_loss: 0.0262 | \n",
            "Epoch: 391 | train_loss: 0.0262 | test_loss: 0.0262 | \n",
            "Epoch: 392 | train_loss: 0.0262 | test_loss: 0.0262 | \n",
            "Epoch: 393 | train_loss: 0.0262 | test_loss: 0.0261 | \n",
            "Epoch: 394 | train_loss: 0.0261 | test_loss: 0.0261 | \n",
            "Epoch: 395 | train_loss: 0.0261 | test_loss: 0.0261 | \n",
            "Epoch: 396 | train_loss: 0.0261 | test_loss: 0.0260 | \n",
            "Epoch: 397 | train_loss: 0.0260 | test_loss: 0.0260 | \n",
            "Epoch: 398 | train_loss: 0.0260 | test_loss: 0.0259 | \n",
            "Epoch: 399 | train_loss: 0.0259 | test_loss: 0.0259 | \n",
            "Epoch: 400 | train_loss: 0.0259 | test_loss: 0.0259 | \n",
            "Epoch: 401 | train_loss: 0.0259 | test_loss: 0.0258 | \n",
            "Epoch: 402 | train_loss: 0.0258 | test_loss: 0.0258 | \n",
            "Epoch: 403 | train_loss: 0.0258 | test_loss: 0.0257 | \n",
            "Epoch: 404 | train_loss: 0.0257 | test_loss: 0.0257 | \n",
            "Epoch: 405 | train_loss: 0.0257 | test_loss: 0.0257 | \n",
            "Epoch: 406 | train_loss: 0.0257 | test_loss: 0.0256 | \n",
            "Epoch: 407 | train_loss: 0.0256 | test_loss: 0.0256 | \n",
            "Epoch: 408 | train_loss: 0.0256 | test_loss: 0.0256 | \n",
            "Epoch: 409 | train_loss: 0.0256 | test_loss: 0.0255 | \n",
            "Epoch: 410 | train_loss: 0.0255 | test_loss: 0.0255 | \n",
            "Epoch: 411 | train_loss: 0.0255 | test_loss: 0.0254 | \n",
            "Epoch: 412 | train_loss: 0.0254 | test_loss: 0.0254 | \n",
            "Epoch: 413 | train_loss: 0.0254 | test_loss: 0.0254 | \n",
            "Epoch: 414 | train_loss: 0.0254 | test_loss: 0.0253 | \n",
            "Epoch: 415 | train_loss: 0.0253 | test_loss: 0.0253 | \n",
            "Epoch: 416 | train_loss: 0.0253 | test_loss: 0.0252 | \n",
            "Epoch: 417 | train_loss: 0.0252 | test_loss: 0.0252 | \n",
            "Epoch: 418 | train_loss: 0.0252 | test_loss: 0.0252 | \n",
            "Epoch: 419 | train_loss: 0.0252 | test_loss: 0.0251 | \n",
            "Epoch: 420 | train_loss: 0.0251 | test_loss: 0.0251 | \n",
            "Epoch: 421 | train_loss: 0.0251 | test_loss: 0.0251 | \n",
            "Epoch: 422 | train_loss: 0.0251 | test_loss: 0.0250 | \n",
            "Epoch: 423 | train_loss: 0.0250 | test_loss: 0.0250 | \n",
            "Epoch: 424 | train_loss: 0.0250 | test_loss: 0.0249 | \n",
            "Epoch: 425 | train_loss: 0.0249 | test_loss: 0.0249 | \n",
            "Epoch: 426 | train_loss: 0.0249 | test_loss: 0.0249 | \n",
            "Epoch: 427 | train_loss: 0.0249 | test_loss: 0.0248 | \n",
            "Epoch: 428 | train_loss: 0.0248 | test_loss: 0.0248 | \n",
            "Epoch: 429 | train_loss: 0.0248 | test_loss: 0.0247 | \n",
            "Epoch: 430 | train_loss: 0.0247 | test_loss: 0.0247 | \n",
            "Epoch: 431 | train_loss: 0.0247 | test_loss: 0.0247 | \n",
            "Epoch: 432 | train_loss: 0.0247 | test_loss: 0.0246 | \n",
            "Epoch: 433 | train_loss: 0.0246 | test_loss: 0.0246 | \n",
            "Epoch: 434 | train_loss: 0.0246 | test_loss: 0.0246 | \n",
            "Epoch: 435 | train_loss: 0.0246 | test_loss: 0.0245 | \n",
            "Epoch: 436 | train_loss: 0.0245 | test_loss: 0.0245 | \n",
            "Epoch: 437 | train_loss: 0.0245 | test_loss: 0.0244 | \n",
            "Epoch: 438 | train_loss: 0.0244 | test_loss: 0.0244 | \n",
            "Epoch: 439 | train_loss: 0.0244 | test_loss: 0.0244 | \n",
            "Epoch: 440 | train_loss: 0.0244 | test_loss: 0.0243 | \n",
            "Epoch: 441 | train_loss: 0.0243 | test_loss: 0.0243 | \n",
            "Epoch: 442 | train_loss: 0.0243 | test_loss: 0.0242 | \n",
            "Epoch: 443 | train_loss: 0.0242 | test_loss: 0.0242 | \n",
            "Epoch: 444 | train_loss: 0.0242 | test_loss: 0.0242 | \n",
            "Epoch: 445 | train_loss: 0.0242 | test_loss: 0.0241 | \n",
            "Epoch: 446 | train_loss: 0.0241 | test_loss: 0.0241 | \n",
            "Epoch: 447 | train_loss: 0.0241 | test_loss: 0.0241 | \n",
            "Epoch: 448 | train_loss: 0.0241 | test_loss: 0.0240 | \n",
            "Epoch: 449 | train_loss: 0.0240 | test_loss: 0.0240 | \n",
            "Epoch: 450 | train_loss: 0.0240 | test_loss: 0.0239 | \n",
            "Epoch: 451 | train_loss: 0.0239 | test_loss: 0.0239 | \n",
            "Epoch: 452 | train_loss: 0.0239 | test_loss: 0.0239 | \n",
            "Epoch: 453 | train_loss: 0.0239 | test_loss: 0.0238 | \n",
            "Epoch: 454 | train_loss: 0.0238 | test_loss: 0.0238 | \n",
            "Epoch: 455 | train_loss: 0.0238 | test_loss: 0.0237 | \n",
            "Epoch: 456 | train_loss: 0.0237 | test_loss: 0.0237 | \n",
            "Epoch: 457 | train_loss: 0.0237 | test_loss: 0.0237 | \n",
            "Epoch: 458 | train_loss: 0.0237 | test_loss: 0.0236 | \n",
            "Epoch: 459 | train_loss: 0.0236 | test_loss: 0.0236 | \n",
            "Epoch: 460 | train_loss: 0.0236 | test_loss: 0.0236 | \n",
            "Epoch: 461 | train_loss: 0.0236 | test_loss: 0.0235 | \n",
            "Epoch: 462 | train_loss: 0.0235 | test_loss: 0.0235 | \n",
            "Epoch: 463 | train_loss: 0.0235 | test_loss: 0.0234 | \n",
            "Epoch: 464 | train_loss: 0.0234 | test_loss: 0.0234 | \n",
            "Epoch: 465 | train_loss: 0.0234 | test_loss: 0.0234 | \n",
            "Epoch: 466 | train_loss: 0.0234 | test_loss: 0.0233 | \n",
            "Epoch: 467 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 468 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 469 | train_loss: 0.0233 | test_loss: 0.0232 | \n",
            "Epoch: 470 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 471 | train_loss: 0.0232 | test_loss: 0.0231 | \n",
            "Epoch: 472 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 473 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 474 | train_loss: 0.0231 | test_loss: 0.0230 | \n",
            "Epoch: 475 | train_loss: 0.0230 | test_loss: 0.0230 | \n",
            "Epoch: 476 | train_loss: 0.0230 | test_loss: 0.0229 | \n",
            "Epoch: 477 | train_loss: 0.0229 | test_loss: 0.0229 | \n",
            "Epoch: 478 | train_loss: 0.0229 | test_loss: 0.0229 | \n",
            "Epoch: 479 | train_loss: 0.0229 | test_loss: 0.0228 | \n",
            "Epoch: 480 | train_loss: 0.0228 | test_loss: 0.0228 | \n",
            "Epoch: 481 | train_loss: 0.0228 | test_loss: 0.0228 | \n",
            "Epoch: 482 | train_loss: 0.0228 | test_loss: 0.0227 | \n",
            "Epoch: 483 | train_loss: 0.0227 | test_loss: 0.0227 | \n",
            "Epoch: 484 | train_loss: 0.0227 | test_loss: 0.0226 | \n",
            "Epoch: 485 | train_loss: 0.0226 | test_loss: 0.0226 | \n",
            "Epoch: 486 | train_loss: 0.0226 | test_loss: 0.0226 | \n",
            "Epoch: 487 | train_loss: 0.0226 | test_loss: 0.0225 | \n",
            "Epoch: 488 | train_loss: 0.0225 | test_loss: 0.0225 | \n",
            "Epoch: 489 | train_loss: 0.0225 | test_loss: 0.0224 | \n",
            "Epoch: 490 | train_loss: 0.0224 | test_loss: 0.0224 | \n",
            "Epoch: 491 | train_loss: 0.0224 | test_loss: 0.0224 | \n",
            "Epoch: 492 | train_loss: 0.0224 | test_loss: 0.0223 | \n",
            "Epoch: 493 | train_loss: 0.0223 | test_loss: 0.0223 | \n",
            "Epoch: 494 | train_loss: 0.0223 | test_loss: 0.0223 | \n",
            "Epoch: 495 | train_loss: 0.0223 | test_loss: 0.0222 | \n",
            "Epoch: 496 | train_loss: 0.0222 | test_loss: 0.0222 | \n",
            "Epoch: 497 | train_loss: 0.0222 | test_loss: 0.0221 | \n",
            "Epoch: 498 | train_loss: 0.0221 | test_loss: 0.0221 | \n",
            "Epoch: 499 | train_loss: 0.0221 | test_loss: 0.0221 | \n",
            "Epoch: 500 | train_loss: 0.0221 | test_loss: 0.0220 | \n",
            "Epoch: 501 | train_loss: 0.0220 | test_loss: 0.0220 | \n",
            "Epoch: 502 | train_loss: 0.0220 | test_loss: 0.0220 | \n",
            "Epoch: 503 | train_loss: 0.0220 | test_loss: 0.0219 | \n",
            "Epoch: 504 | train_loss: 0.0219 | test_loss: 0.0219 | \n",
            "Epoch: 505 | train_loss: 0.0219 | test_loss: 0.0218 | \n",
            "Epoch: 506 | train_loss: 0.0218 | test_loss: 0.0218 | \n",
            "Epoch: 507 | train_loss: 0.0218 | test_loss: 0.0218 | \n",
            "Epoch: 508 | train_loss: 0.0218 | test_loss: 0.0217 | \n",
            "Epoch: 509 | train_loss: 0.0217 | test_loss: 0.0217 | \n",
            "Epoch: 510 | train_loss: 0.0217 | test_loss: 0.0216 | \n",
            "Epoch: 511 | train_loss: 0.0216 | test_loss: 0.0216 | \n",
            "Epoch: 512 | train_loss: 0.0216 | test_loss: 0.0216 | \n",
            "Epoch: 513 | train_loss: 0.0216 | test_loss: 0.0215 | \n",
            "Epoch: 514 | train_loss: 0.0215 | test_loss: 0.0215 | \n",
            "Epoch: 515 | train_loss: 0.0215 | test_loss: 0.0215 | \n",
            "Epoch: 516 | train_loss: 0.0215 | test_loss: 0.0214 | \n",
            "Epoch: 517 | train_loss: 0.0214 | test_loss: 0.0214 | \n",
            "Epoch: 518 | train_loss: 0.0214 | test_loss: 0.0213 | \n",
            "Epoch: 519 | train_loss: 0.0213 | test_loss: 0.0213 | \n",
            "Epoch: 520 | train_loss: 0.0213 | test_loss: 0.0213 | \n",
            "Epoch: 521 | train_loss: 0.0213 | test_loss: 0.0212 | \n",
            "Epoch: 522 | train_loss: 0.0212 | test_loss: 0.0212 | \n",
            "Epoch: 523 | train_loss: 0.0212 | test_loss: 0.0212 | \n",
            "Epoch: 524 | train_loss: 0.0212 | test_loss: 0.0211 | \n",
            "Epoch: 525 | train_loss: 0.0211 | test_loss: 0.0211 | \n",
            "Epoch: 526 | train_loss: 0.0211 | test_loss: 0.0210 | \n",
            "Epoch: 527 | train_loss: 0.0210 | test_loss: 0.0210 | \n",
            "Epoch: 528 | train_loss: 0.0210 | test_loss: 0.0210 | \n",
            "Epoch: 529 | train_loss: 0.0210 | test_loss: 0.0209 | \n",
            "Epoch: 530 | train_loss: 0.0209 | test_loss: 0.0209 | \n",
            "Epoch: 531 | train_loss: 0.0209 | test_loss: 0.0209 | \n",
            "Epoch: 532 | train_loss: 0.0209 | test_loss: 0.0208 | \n",
            "Epoch: 533 | train_loss: 0.0208 | test_loss: 0.0208 | \n",
            "Epoch: 534 | train_loss: 0.0208 | test_loss: 0.0207 | \n",
            "Epoch: 535 | train_loss: 0.0207 | test_loss: 0.0207 | \n",
            "Epoch: 536 | train_loss: 0.0207 | test_loss: 0.0207 | \n",
            "Epoch: 537 | train_loss: 0.0207 | test_loss: 0.0206 | \n",
            "Epoch: 538 | train_loss: 0.0206 | test_loss: 0.0206 | \n",
            "Epoch: 539 | train_loss: 0.0206 | test_loss: 0.0206 | \n",
            "Epoch: 540 | train_loss: 0.0206 | test_loss: 0.0205 | \n",
            "Epoch: 541 | train_loss: 0.0205 | test_loss: 0.0205 | \n",
            "Epoch: 542 | train_loss: 0.0205 | test_loss: 0.0204 | \n",
            "Epoch: 543 | train_loss: 0.0204 | test_loss: 0.0204 | \n",
            "Epoch: 544 | train_loss: 0.0204 | test_loss: 0.0204 | \n",
            "Epoch: 545 | train_loss: 0.0204 | test_loss: 0.0203 | \n",
            "Epoch: 546 | train_loss: 0.0203 | test_loss: 0.0203 | \n",
            "Epoch: 547 | train_loss: 0.0203 | test_loss: 0.0203 | \n",
            "Epoch: 548 | train_loss: 0.0203 | test_loss: 0.0202 | \n",
            "Epoch: 549 | train_loss: 0.0202 | test_loss: 0.0202 | \n",
            "Epoch: 550 | train_loss: 0.0202 | test_loss: 0.0201 | \n",
            "Epoch: 551 | train_loss: 0.0201 | test_loss: 0.0201 | \n",
            "Epoch: 552 | train_loss: 0.0201 | test_loss: 0.0201 | \n",
            "Epoch: 553 | train_loss: 0.0201 | test_loss: 0.0200 | \n",
            "Epoch: 554 | train_loss: 0.0200 | test_loss: 0.0200 | \n",
            "Epoch: 555 | train_loss: 0.0200 | test_loss: 0.0200 | \n",
            "Epoch: 556 | train_loss: 0.0200 | test_loss: 0.0199 | \n",
            "Epoch: 557 | train_loss: 0.0199 | test_loss: 0.0199 | \n",
            "Epoch: 558 | train_loss: 0.0199 | test_loss: 0.0198 | \n",
            "Epoch: 559 | train_loss: 0.0198 | test_loss: 0.0198 | \n",
            "Epoch: 560 | train_loss: 0.0198 | test_loss: 0.0198 | \n",
            "Epoch: 561 | train_loss: 0.0198 | test_loss: 0.0197 | \n",
            "Epoch: 562 | train_loss: 0.0197 | test_loss: 0.0197 | \n",
            "Epoch: 563 | train_loss: 0.0197 | test_loss: 0.0197 | \n",
            "Epoch: 564 | train_loss: 0.0197 | test_loss: 0.0196 | \n",
            "Epoch: 565 | train_loss: 0.0196 | test_loss: 0.0196 | \n",
            "Epoch: 566 | train_loss: 0.0196 | test_loss: 0.0195 | \n",
            "Epoch: 567 | train_loss: 0.0195 | test_loss: 0.0195 | \n",
            "Epoch: 568 | train_loss: 0.0195 | test_loss: 0.0195 | \n",
            "Epoch: 569 | train_loss: 0.0195 | test_loss: 0.0194 | \n",
            "Epoch: 570 | train_loss: 0.0194 | test_loss: 0.0194 | \n",
            "Epoch: 571 | train_loss: 0.0194 | test_loss: 0.0194 | \n",
            "Epoch: 572 | train_loss: 0.0194 | test_loss: 0.0193 | \n",
            "Epoch: 573 | train_loss: 0.0193 | test_loss: 0.0193 | \n",
            "Epoch: 574 | train_loss: 0.0193 | test_loss: 0.0193 | \n",
            "Epoch: 575 | train_loss: 0.0193 | test_loss: 0.0192 | \n",
            "Epoch: 576 | train_loss: 0.0192 | test_loss: 0.0192 | \n",
            "Epoch: 577 | train_loss: 0.0192 | test_loss: 0.0191 | \n",
            "Epoch: 578 | train_loss: 0.0191 | test_loss: 0.0191 | \n",
            "Epoch: 579 | train_loss: 0.0191 | test_loss: 0.0191 | \n",
            "Epoch: 580 | train_loss: 0.0191 | test_loss: 0.0190 | \n",
            "Epoch: 581 | train_loss: 0.0190 | test_loss: 0.0190 | \n",
            "Epoch: 582 | train_loss: 0.0190 | test_loss: 0.0190 | \n",
            "Epoch: 583 | train_loss: 0.0190 | test_loss: 0.0189 | \n",
            "Epoch: 584 | train_loss: 0.0189 | test_loss: 0.0189 | \n",
            "Epoch: 585 | train_loss: 0.0189 | test_loss: 0.0189 | \n",
            "Epoch: 586 | train_loss: 0.0189 | test_loss: 0.0188 | \n",
            "Epoch: 587 | train_loss: 0.0188 | test_loss: 0.0188 | \n",
            "Epoch: 588 | train_loss: 0.0188 | test_loss: 0.0187 | \n",
            "Epoch: 589 | train_loss: 0.0187 | test_loss: 0.0187 | \n",
            "Epoch: 590 | train_loss: 0.0187 | test_loss: 0.0187 | \n",
            "Epoch: 591 | train_loss: 0.0187 | test_loss: 0.0186 | \n",
            "Epoch: 592 | train_loss: 0.0186 | test_loss: 0.0186 | \n",
            "Epoch: 593 | train_loss: 0.0186 | test_loss: 0.0186 | \n",
            "Epoch: 594 | train_loss: 0.0186 | test_loss: 0.0185 | \n",
            "Epoch: 595 | train_loss: 0.0185 | test_loss: 0.0185 | \n",
            "Epoch: 596 | train_loss: 0.0185 | test_loss: 0.0185 | \n",
            "Epoch: 597 | train_loss: 0.0185 | test_loss: 0.0184 | \n",
            "Epoch: 598 | train_loss: 0.0184 | test_loss: 0.0184 | \n",
            "Epoch: 599 | train_loss: 0.0184 | test_loss: 0.0183 | \n",
            "Epoch: 600 | train_loss: 0.0183 | test_loss: 0.0183 | \n",
            "Epoch: 601 | train_loss: 0.0183 | test_loss: 0.0183 | \n",
            "Epoch: 602 | train_loss: 0.0183 | test_loss: 0.0182 | \n",
            "Epoch: 603 | train_loss: 0.0182 | test_loss: 0.0182 | \n",
            "Epoch: 604 | train_loss: 0.0182 | test_loss: 0.0182 | \n",
            "Epoch: 605 | train_loss: 0.0182 | test_loss: 0.0181 | \n",
            "Epoch: 606 | train_loss: 0.0181 | test_loss: 0.0181 | \n",
            "Epoch: 607 | train_loss: 0.0181 | test_loss: 0.0181 | \n",
            "Epoch: 608 | train_loss: 0.0181 | test_loss: 0.0180 | \n",
            "Epoch: 609 | train_loss: 0.0180 | test_loss: 0.0180 | \n",
            "Epoch: 610 | train_loss: 0.0180 | test_loss: 0.0179 | \n",
            "Epoch: 611 | train_loss: 0.0179 | test_loss: 0.0179 | \n",
            "Epoch: 612 | train_loss: 0.0179 | test_loss: 0.0179 | \n",
            "Epoch: 613 | train_loss: 0.0179 | test_loss: 0.0178 | \n",
            "Epoch: 614 | train_loss: 0.0178 | test_loss: 0.0178 | \n",
            "Epoch: 615 | train_loss: 0.0178 | test_loss: 0.0178 | \n",
            "Epoch: 616 | train_loss: 0.0178 | test_loss: 0.0177 | \n",
            "Epoch: 617 | train_loss: 0.0177 | test_loss: 0.0177 | \n",
            "Epoch: 618 | train_loss: 0.0177 | test_loss: 0.0177 | \n",
            "Epoch: 619 | train_loss: 0.0177 | test_loss: 0.0176 | \n",
            "Epoch: 620 | train_loss: 0.0176 | test_loss: 0.0176 | \n",
            "Epoch: 621 | train_loss: 0.0176 | test_loss: 0.0176 | \n",
            "Epoch: 622 | train_loss: 0.0176 | test_loss: 0.0175 | \n",
            "Epoch: 623 | train_loss: 0.0175 | test_loss: 0.0175 | \n",
            "Epoch: 624 | train_loss: 0.0175 | test_loss: 0.0174 | \n",
            "Epoch: 625 | train_loss: 0.0174 | test_loss: 0.0174 | \n",
            "Epoch: 626 | train_loss: 0.0174 | test_loss: 0.0174 | \n",
            "Epoch: 627 | train_loss: 0.0174 | test_loss: 0.0173 | \n",
            "Epoch: 628 | train_loss: 0.0173 | test_loss: 0.0173 | \n",
            "Epoch: 629 | train_loss: 0.0173 | test_loss: 0.0173 | \n",
            "Epoch: 630 | train_loss: 0.0173 | test_loss: 0.0172 | \n",
            "Epoch: 631 | train_loss: 0.0172 | test_loss: 0.0172 | \n",
            "Epoch: 632 | train_loss: 0.0172 | test_loss: 0.0172 | \n",
            "Epoch: 633 | train_loss: 0.0172 | test_loss: 0.0171 | \n",
            "Epoch: 634 | train_loss: 0.0171 | test_loss: 0.0171 | \n",
            "Epoch: 635 | train_loss: 0.0171 | test_loss: 0.0171 | \n",
            "Epoch: 636 | train_loss: 0.0171 | test_loss: 0.0170 | \n",
            "Epoch: 637 | train_loss: 0.0170 | test_loss: 0.0170 | \n",
            "Epoch: 638 | train_loss: 0.0170 | test_loss: 0.0170 | \n",
            "Epoch: 639 | train_loss: 0.0170 | test_loss: 0.0169 | \n",
            "Epoch: 640 | train_loss: 0.0169 | test_loss: 0.0169 | \n",
            "Epoch: 641 | train_loss: 0.0169 | test_loss: 0.0169 | \n",
            "Epoch: 642 | train_loss: 0.0169 | test_loss: 0.0168 | \n",
            "Epoch: 643 | train_loss: 0.0168 | test_loss: 0.0168 | \n",
            "Epoch: 644 | train_loss: 0.0168 | test_loss: 0.0167 | \n",
            "Epoch: 645 | train_loss: 0.0167 | test_loss: 0.0167 | \n",
            "Epoch: 646 | train_loss: 0.0167 | test_loss: 0.0167 | \n",
            "Epoch: 647 | train_loss: 0.0167 | test_loss: 0.0166 | \n",
            "Epoch: 648 | train_loss: 0.0166 | test_loss: 0.0166 | \n",
            "Epoch: 649 | train_loss: 0.0166 | test_loss: 0.0166 | \n",
            "Epoch: 650 | train_loss: 0.0166 | test_loss: 0.0165 | \n",
            "Epoch: 651 | train_loss: 0.0165 | test_loss: 0.0165 | \n",
            "Epoch: 652 | train_loss: 0.0165 | test_loss: 0.0165 | \n",
            "Epoch: 653 | train_loss: 0.0165 | test_loss: 0.0164 | \n",
            "Epoch: 654 | train_loss: 0.0164 | test_loss: 0.0164 | \n",
            "Epoch: 655 | train_loss: 0.0164 | test_loss: 0.0164 | \n",
            "Epoch: 656 | train_loss: 0.0164 | test_loss: 0.0163 | \n",
            "Epoch: 657 | train_loss: 0.0163 | test_loss: 0.0163 | \n",
            "Epoch: 658 | train_loss: 0.0163 | test_loss: 0.0163 | \n",
            "Epoch: 659 | train_loss: 0.0163 | test_loss: 0.0162 | \n",
            "Epoch: 660 | train_loss: 0.0162 | test_loss: 0.0162 | \n",
            "Epoch: 661 | train_loss: 0.0162 | test_loss: 0.0162 | \n",
            "Epoch: 662 | train_loss: 0.0162 | test_loss: 0.0161 | \n",
            "Epoch: 663 | train_loss: 0.0161 | test_loss: 0.0161 | \n",
            "Epoch: 664 | train_loss: 0.0161 | test_loss: 0.0161 | \n",
            "Epoch: 665 | train_loss: 0.0161 | test_loss: 0.0160 | \n",
            "Epoch: 666 | train_loss: 0.0160 | test_loss: 0.0160 | \n",
            "Epoch: 667 | train_loss: 0.0160 | test_loss: 0.0160 | \n",
            "Epoch: 668 | train_loss: 0.0160 | test_loss: 0.0159 | \n",
            "Epoch: 669 | train_loss: 0.0159 | test_loss: 0.0159 | \n",
            "Epoch: 670 | train_loss: 0.0159 | test_loss: 0.0159 | \n",
            "Epoch: 671 | train_loss: 0.0159 | test_loss: 0.0158 | \n",
            "Epoch: 672 | train_loss: 0.0158 | test_loss: 0.0158 | \n",
            "Epoch: 673 | train_loss: 0.0158 | test_loss: 0.0158 | \n",
            "Epoch: 674 | train_loss: 0.0158 | test_loss: 0.0157 | \n",
            "Epoch: 675 | train_loss: 0.0157 | test_loss: 0.0157 | \n",
            "Epoch: 676 | train_loss: 0.0157 | test_loss: 0.0157 | \n",
            "Epoch: 677 | train_loss: 0.0157 | test_loss: 0.0156 | \n",
            "Epoch: 678 | train_loss: 0.0156 | test_loss: 0.0156 | \n",
            "Epoch: 679 | train_loss: 0.0156 | test_loss: 0.0156 | \n",
            "Epoch: 680 | train_loss: 0.0156 | test_loss: 0.0155 | \n",
            "Epoch: 681 | train_loss: 0.0155 | test_loss: 0.0155 | \n",
            "Epoch: 682 | train_loss: 0.0155 | test_loss: 0.0155 | \n",
            "Epoch: 683 | train_loss: 0.0155 | test_loss: 0.0154 | \n",
            "Epoch: 684 | train_loss: 0.0154 | test_loss: 0.0154 | \n",
            "Epoch: 685 | train_loss: 0.0154 | test_loss: 0.0154 | \n",
            "Epoch: 686 | train_loss: 0.0154 | test_loss: 0.0153 | \n",
            "Epoch: 687 | train_loss: 0.0153 | test_loss: 0.0153 | \n",
            "Epoch: 688 | train_loss: 0.0153 | test_loss: 0.0153 | \n",
            "Epoch: 689 | train_loss: 0.0153 | test_loss: 0.0152 | \n",
            "Epoch: 690 | train_loss: 0.0152 | test_loss: 0.0152 | \n",
            "Epoch: 691 | train_loss: 0.0152 | test_loss: 0.0152 | \n",
            "Epoch: 692 | train_loss: 0.0152 | test_loss: 0.0151 | \n",
            "Epoch: 693 | train_loss: 0.0151 | test_loss: 0.0151 | \n",
            "Epoch: 694 | train_loss: 0.0151 | test_loss: 0.0151 | \n",
            "Epoch: 695 | train_loss: 0.0151 | test_loss: 0.0150 | \n",
            "Epoch: 696 | train_loss: 0.0150 | test_loss: 0.0150 | \n",
            "Epoch: 697 | train_loss: 0.0150 | test_loss: 0.0150 | \n",
            "Epoch: 698 | train_loss: 0.0150 | test_loss: 0.0149 | \n",
            "Epoch: 699 | train_loss: 0.0149 | test_loss: 0.0149 | \n",
            "Epoch: 700 | train_loss: 0.0149 | test_loss: 0.0149 | \n",
            "Epoch: 701 | train_loss: 0.0149 | test_loss: 0.0148 | \n",
            "Epoch: 702 | train_loss: 0.0148 | test_loss: 0.0148 | \n",
            "Epoch: 703 | train_loss: 0.0148 | test_loss: 0.0148 | \n",
            "Epoch: 704 | train_loss: 0.0148 | test_loss: 0.0147 | \n",
            "Epoch: 705 | train_loss: 0.0147 | test_loss: 0.0147 | \n",
            "Epoch: 706 | train_loss: 0.0147 | test_loss: 0.0147 | \n",
            "Epoch: 707 | train_loss: 0.0147 | test_loss: 0.0146 | \n",
            "Epoch: 708 | train_loss: 0.0146 | test_loss: 0.0146 | \n",
            "Epoch: 709 | train_loss: 0.0146 | test_loss: 0.0146 | \n",
            "Epoch: 710 | train_loss: 0.0146 | test_loss: 0.0145 | \n",
            "Epoch: 711 | train_loss: 0.0145 | test_loss: 0.0145 | \n",
            "Epoch: 712 | train_loss: 0.0145 | test_loss: 0.0145 | \n",
            "Epoch: 713 | train_loss: 0.0145 | test_loss: 0.0144 | \n",
            "Epoch: 714 | train_loss: 0.0144 | test_loss: 0.0144 | \n",
            "Epoch: 715 | train_loss: 0.0144 | test_loss: 0.0144 | \n",
            "Epoch: 716 | train_loss: 0.0144 | test_loss: 0.0143 | \n",
            "Epoch: 717 | train_loss: 0.0143 | test_loss: 0.0143 | \n",
            "Epoch: 718 | train_loss: 0.0143 | test_loss: 0.0143 | \n",
            "Epoch: 719 | train_loss: 0.0143 | test_loss: 0.0142 | \n",
            "Epoch: 720 | train_loss: 0.0142 | test_loss: 0.0142 | \n",
            "Epoch: 721 | train_loss: 0.0142 | test_loss: 0.0142 | \n",
            "Epoch: 722 | train_loss: 0.0142 | test_loss: 0.0141 | \n",
            "Epoch: 723 | train_loss: 0.0141 | test_loss: 0.0141 | \n",
            "Epoch: 724 | train_loss: 0.0141 | test_loss: 0.0141 | \n",
            "Epoch: 725 | train_loss: 0.0141 | test_loss: 0.0140 | \n",
            "Epoch: 726 | train_loss: 0.0140 | test_loss: 0.0140 | \n",
            "Epoch: 727 | train_loss: 0.0140 | test_loss: 0.0140 | \n",
            "Epoch: 728 | train_loss: 0.0140 | test_loss: 0.0140 | \n",
            "Epoch: 729 | train_loss: 0.0140 | test_loss: 0.0139 | \n",
            "Epoch: 730 | train_loss: 0.0139 | test_loss: 0.0139 | \n",
            "Epoch: 731 | train_loss: 0.0139 | test_loss: 0.0139 | \n",
            "Epoch: 732 | train_loss: 0.0139 | test_loss: 0.0138 | \n",
            "Epoch: 733 | train_loss: 0.0138 | test_loss: 0.0138 | \n",
            "Epoch: 734 | train_loss: 0.0138 | test_loss: 0.0138 | \n",
            "Epoch: 735 | train_loss: 0.0138 | test_loss: 0.0137 | \n",
            "Epoch: 736 | train_loss: 0.0137 | test_loss: 0.0137 | \n",
            "Epoch: 737 | train_loss: 0.0137 | test_loss: 0.0137 | \n",
            "Epoch: 738 | train_loss: 0.0137 | test_loss: 0.0136 | \n",
            "Epoch: 739 | train_loss: 0.0136 | test_loss: 0.0136 | \n",
            "Epoch: 740 | train_loss: 0.0136 | test_loss: 0.0136 | \n",
            "Epoch: 741 | train_loss: 0.0136 | test_loss: 0.0135 | \n",
            "Epoch: 742 | train_loss: 0.0135 | test_loss: 0.0135 | \n",
            "Epoch: 743 | train_loss: 0.0135 | test_loss: 0.0135 | \n",
            "Epoch: 744 | train_loss: 0.0135 | test_loss: 0.0134 | \n",
            "Epoch: 745 | train_loss: 0.0134 | test_loss: 0.0134 | \n",
            "Epoch: 746 | train_loss: 0.0134 | test_loss: 0.0134 | \n",
            "Epoch: 747 | train_loss: 0.0134 | test_loss: 0.0134 | \n",
            "Epoch: 748 | train_loss: 0.0134 | test_loss: 0.0133 | \n",
            "Epoch: 749 | train_loss: 0.0133 | test_loss: 0.0133 | \n",
            "Epoch: 750 | train_loss: 0.0133 | test_loss: 0.0133 | \n",
            "Epoch: 751 | train_loss: 0.0133 | test_loss: 0.0132 | \n",
            "Epoch: 752 | train_loss: 0.0132 | test_loss: 0.0132 | \n",
            "Epoch: 753 | train_loss: 0.0132 | test_loss: 0.0132 | \n",
            "Epoch: 754 | train_loss: 0.0132 | test_loss: 0.0131 | \n",
            "Epoch: 755 | train_loss: 0.0131 | test_loss: 0.0131 | \n",
            "Epoch: 756 | train_loss: 0.0131 | test_loss: 0.0131 | \n",
            "Epoch: 757 | train_loss: 0.0131 | test_loss: 0.0130 | \n",
            "Epoch: 758 | train_loss: 0.0130 | test_loss: 0.0130 | \n",
            "Epoch: 759 | train_loss: 0.0130 | test_loss: 0.0130 | \n",
            "Epoch: 760 | train_loss: 0.0130 | test_loss: 0.0130 | \n",
            "Epoch: 761 | train_loss: 0.0130 | test_loss: 0.0129 | \n",
            "Epoch: 762 | train_loss: 0.0129 | test_loss: 0.0129 | \n",
            "Epoch: 763 | train_loss: 0.0129 | test_loss: 0.0129 | \n",
            "Epoch: 764 | train_loss: 0.0129 | test_loss: 0.0128 | \n",
            "Epoch: 765 | train_loss: 0.0128 | test_loss: 0.0128 | \n",
            "Epoch: 766 | train_loss: 0.0128 | test_loss: 0.0128 | \n",
            "Epoch: 767 | train_loss: 0.0128 | test_loss: 0.0127 | \n",
            "Epoch: 768 | train_loss: 0.0127 | test_loss: 0.0127 | \n",
            "Epoch: 769 | train_loss: 0.0127 | test_loss: 0.0127 | \n",
            "Epoch: 770 | train_loss: 0.0127 | test_loss: 0.0127 | \n",
            "Epoch: 771 | train_loss: 0.0127 | test_loss: 0.0126 | \n",
            "Epoch: 772 | train_loss: 0.0126 | test_loss: 0.0126 | \n",
            "Epoch: 773 | train_loss: 0.0126 | test_loss: 0.0126 | \n",
            "Epoch: 774 | train_loss: 0.0126 | test_loss: 0.0125 | \n",
            "Epoch: 775 | train_loss: 0.0125 | test_loss: 0.0125 | \n",
            "Epoch: 776 | train_loss: 0.0125 | test_loss: 0.0125 | \n",
            "Epoch: 777 | train_loss: 0.0125 | test_loss: 0.0124 | \n",
            "Epoch: 778 | train_loss: 0.0124 | test_loss: 0.0124 | \n",
            "Epoch: 779 | train_loss: 0.0124 | test_loss: 0.0124 | \n",
            "Epoch: 780 | train_loss: 0.0124 | test_loss: 0.0124 | \n",
            "Epoch: 781 | train_loss: 0.0124 | test_loss: 0.0123 | \n",
            "Epoch: 782 | train_loss: 0.0123 | test_loss: 0.0123 | \n",
            "Epoch: 783 | train_loss: 0.0123 | test_loss: 0.0123 | \n",
            "Epoch: 784 | train_loss: 0.0123 | test_loss: 0.0122 | \n",
            "Epoch: 785 | train_loss: 0.0122 | test_loss: 0.0122 | \n",
            "Epoch: 786 | train_loss: 0.0122 | test_loss: 0.0122 | \n",
            "Epoch: 787 | train_loss: 0.0122 | test_loss: 0.0122 | \n",
            "Epoch: 788 | train_loss: 0.0122 | test_loss: 0.0121 | \n",
            "Epoch: 789 | train_loss: 0.0121 | test_loss: 0.0121 | \n",
            "Epoch: 790 | train_loss: 0.0121 | test_loss: 0.0121 | \n",
            "Epoch: 791 | train_loss: 0.0121 | test_loss: 0.0120 | \n",
            "Epoch: 792 | train_loss: 0.0120 | test_loss: 0.0120 | \n",
            "Epoch: 793 | train_loss: 0.0120 | test_loss: 0.0120 | \n",
            "Epoch: 794 | train_loss: 0.0120 | test_loss: 0.0119 | \n",
            "Epoch: 795 | train_loss: 0.0119 | test_loss: 0.0119 | \n",
            "Epoch: 796 | train_loss: 0.0119 | test_loss: 0.0119 | \n",
            "Epoch: 797 | train_loss: 0.0119 | test_loss: 0.0119 | \n",
            "Epoch: 798 | train_loss: 0.0119 | test_loss: 0.0118 | \n",
            "Epoch: 799 | train_loss: 0.0118 | test_loss: 0.0118 | \n",
            "Epoch: 800 | train_loss: 0.0118 | test_loss: 0.0118 | \n",
            "Epoch: 801 | train_loss: 0.0118 | test_loss: 0.0117 | \n",
            "Epoch: 802 | train_loss: 0.0117 | test_loss: 0.0117 | \n",
            "Epoch: 803 | train_loss: 0.0117 | test_loss: 0.0117 | \n",
            "Epoch: 804 | train_loss: 0.0117 | test_loss: 0.0117 | \n",
            "Epoch: 805 | train_loss: 0.0117 | test_loss: 0.0116 | \n",
            "Epoch: 806 | train_loss: 0.0116 | test_loss: 0.0116 | \n",
            "Epoch: 807 | train_loss: 0.0116 | test_loss: 0.0116 | \n",
            "Epoch: 808 | train_loss: 0.0116 | test_loss: 0.0115 | \n",
            "Epoch: 809 | train_loss: 0.0115 | test_loss: 0.0115 | \n",
            "Epoch: 810 | train_loss: 0.0115 | test_loss: 0.0115 | \n",
            "Epoch: 811 | train_loss: 0.0115 | test_loss: 0.0115 | \n",
            "Epoch: 812 | train_loss: 0.0115 | test_loss: 0.0114 | \n",
            "Epoch: 813 | train_loss: 0.0114 | test_loss: 0.0114 | \n",
            "Epoch: 814 | train_loss: 0.0114 | test_loss: 0.0114 | \n",
            "Epoch: 815 | train_loss: 0.0114 | test_loss: 0.0113 | \n",
            "Epoch: 816 | train_loss: 0.0113 | test_loss: 0.0113 | \n",
            "Epoch: 817 | train_loss: 0.0113 | test_loss: 0.0113 | \n",
            "Epoch: 818 | train_loss: 0.0113 | test_loss: 0.0113 | \n",
            "Epoch: 819 | train_loss: 0.0113 | test_loss: 0.0112 | \n",
            "Epoch: 820 | train_loss: 0.0112 | test_loss: 0.0112 | \n",
            "Epoch: 821 | train_loss: 0.0112 | test_loss: 0.0112 | \n",
            "Epoch: 822 | train_loss: 0.0112 | test_loss: 0.0112 | \n",
            "Epoch: 823 | train_loss: 0.0112 | test_loss: 0.0111 | \n",
            "Epoch: 824 | train_loss: 0.0111 | test_loss: 0.0111 | \n",
            "Epoch: 825 | train_loss: 0.0111 | test_loss: 0.0111 | \n",
            "Epoch: 826 | train_loss: 0.0111 | test_loss: 0.0110 | \n",
            "Epoch: 827 | train_loss: 0.0110 | test_loss: 0.0110 | \n",
            "Epoch: 828 | train_loss: 0.0110 | test_loss: 0.0110 | \n",
            "Epoch: 829 | train_loss: 0.0110 | test_loss: 0.0110 | \n",
            "Epoch: 830 | train_loss: 0.0110 | test_loss: 0.0109 | \n",
            "Epoch: 831 | train_loss: 0.0109 | test_loss: 0.0109 | \n",
            "Epoch: 832 | train_loss: 0.0109 | test_loss: 0.0109 | \n",
            "Epoch: 833 | train_loss: 0.0109 | test_loss: 0.0108 | \n",
            "Epoch: 834 | train_loss: 0.0108 | test_loss: 0.0108 | \n",
            "Epoch: 835 | train_loss: 0.0108 | test_loss: 0.0108 | \n",
            "Epoch: 836 | train_loss: 0.0108 | test_loss: 0.0108 | \n",
            "Epoch: 837 | train_loss: 0.0108 | test_loss: 0.0107 | \n",
            "Epoch: 838 | train_loss: 0.0107 | test_loss: 0.0107 | \n",
            "Epoch: 839 | train_loss: 0.0107 | test_loss: 0.0107 | \n",
            "Epoch: 840 | train_loss: 0.0107 | test_loss: 0.0107 | \n",
            "Epoch: 841 | train_loss: 0.0107 | test_loss: 0.0106 | \n",
            "Epoch: 842 | train_loss: 0.0106 | test_loss: 0.0106 | \n",
            "Epoch: 843 | train_loss: 0.0106 | test_loss: 0.0106 | \n",
            "Epoch: 844 | train_loss: 0.0106 | test_loss: 0.0106 | \n",
            "Epoch: 845 | train_loss: 0.0106 | test_loss: 0.0105 | \n",
            "Epoch: 846 | train_loss: 0.0105 | test_loss: 0.0105 | \n",
            "Epoch: 847 | train_loss: 0.0105 | test_loss: 0.0105 | \n",
            "Epoch: 848 | train_loss: 0.0105 | test_loss: 0.0104 | \n",
            "Epoch: 849 | train_loss: 0.0104 | test_loss: 0.0104 | \n",
            "Epoch: 850 | train_loss: 0.0104 | test_loss: 0.0104 | \n",
            "Epoch: 851 | train_loss: 0.0104 | test_loss: 0.0104 | \n",
            "Epoch: 852 | train_loss: 0.0104 | test_loss: 0.0103 | \n",
            "Epoch: 853 | train_loss: 0.0103 | test_loss: 0.0103 | \n",
            "Epoch: 854 | train_loss: 0.0103 | test_loss: 0.0103 | \n",
            "Epoch: 855 | train_loss: 0.0103 | test_loss: 0.0103 | \n",
            "Epoch: 856 | train_loss: 0.0103 | test_loss: 0.0102 | \n",
            "Epoch: 857 | train_loss: 0.0102 | test_loss: 0.0102 | \n",
            "Epoch: 858 | train_loss: 0.0102 | test_loss: 0.0102 | \n",
            "Epoch: 859 | train_loss: 0.0102 | test_loss: 0.0102 | \n",
            "Epoch: 860 | train_loss: 0.0102 | test_loss: 0.0101 | \n",
            "Epoch: 861 | train_loss: 0.0101 | test_loss: 0.0101 | \n",
            "Epoch: 862 | train_loss: 0.0101 | test_loss: 0.0101 | \n",
            "Epoch: 863 | train_loss: 0.0101 | test_loss: 0.0100 | \n",
            "Epoch: 864 | train_loss: 0.0100 | test_loss: 0.0100 | \n",
            "Epoch: 865 | train_loss: 0.0100 | test_loss: 0.0100 | \n",
            "Epoch: 866 | train_loss: 0.0100 | test_loss: 0.0100 | \n",
            "Epoch: 867 | train_loss: 0.0100 | test_loss: 0.0099 | \n",
            "Epoch: 868 | train_loss: 0.0099 | test_loss: 0.0099 | \n",
            "Epoch: 869 | train_loss: 0.0099 | test_loss: 0.0099 | \n",
            "Epoch: 870 | train_loss: 0.0099 | test_loss: 0.0099 | \n",
            "Epoch: 871 | train_loss: 0.0099 | test_loss: 0.0098 | \n",
            "Epoch: 872 | train_loss: 0.0098 | test_loss: 0.0098 | \n",
            "Epoch: 873 | train_loss: 0.0098 | test_loss: 0.0098 | \n",
            "Epoch: 874 | train_loss: 0.0098 | test_loss: 0.0098 | \n",
            "Epoch: 875 | train_loss: 0.0098 | test_loss: 0.0097 | \n",
            "Epoch: 876 | train_loss: 0.0097 | test_loss: 0.0097 | \n",
            "Epoch: 877 | train_loss: 0.0097 | test_loss: 0.0097 | \n",
            "Epoch: 878 | train_loss: 0.0097 | test_loss: 0.0097 | \n",
            "Epoch: 879 | train_loss: 0.0097 | test_loss: 0.0096 | \n",
            "Epoch: 880 | train_loss: 0.0096 | test_loss: 0.0096 | \n",
            "Epoch: 881 | train_loss: 0.0096 | test_loss: 0.0096 | \n",
            "Epoch: 882 | train_loss: 0.0096 | test_loss: 0.0096 | \n",
            "Epoch: 883 | train_loss: 0.0096 | test_loss: 0.0095 | \n",
            "Epoch: 884 | train_loss: 0.0095 | test_loss: 0.0095 | \n",
            "Epoch: 885 | train_loss: 0.0095 | test_loss: 0.0095 | \n",
            "Epoch: 886 | train_loss: 0.0095 | test_loss: 0.0095 | \n",
            "Epoch: 887 | train_loss: 0.0095 | test_loss: 0.0094 | \n",
            "Epoch: 888 | train_loss: 0.0094 | test_loss: 0.0094 | \n",
            "Epoch: 889 | train_loss: 0.0094 | test_loss: 0.0094 | \n",
            "Epoch: 890 | train_loss: 0.0094 | test_loss: 0.0094 | \n",
            "Epoch: 891 | train_loss: 0.0094 | test_loss: 0.0093 | \n",
            "Epoch: 892 | train_loss: 0.0093 | test_loss: 0.0093 | \n",
            "Epoch: 893 | train_loss: 0.0093 | test_loss: 0.0093 | \n",
            "Epoch: 894 | train_loss: 0.0093 | test_loss: 0.0093 | \n",
            "Epoch: 895 | train_loss: 0.0093 | test_loss: 0.0092 | \n",
            "Epoch: 896 | train_loss: 0.0092 | test_loss: 0.0092 | \n",
            "Epoch: 897 | train_loss: 0.0092 | test_loss: 0.0092 | \n",
            "Epoch: 898 | train_loss: 0.0092 | test_loss: 0.0092 | \n",
            "Epoch: 899 | train_loss: 0.0092 | test_loss: 0.0091 | \n",
            "Epoch: 900 | train_loss: 0.0091 | test_loss: 0.0091 | \n",
            "Epoch: 901 | train_loss: 0.0091 | test_loss: 0.0091 | \n",
            "Epoch: 902 | train_loss: 0.0091 | test_loss: 0.0091 | \n",
            "Epoch: 903 | train_loss: 0.0091 | test_loss: 0.0090 | \n",
            "Epoch: 904 | train_loss: 0.0090 | test_loss: 0.0090 | \n",
            "Epoch: 905 | train_loss: 0.0090 | test_loss: 0.0090 | \n",
            "Epoch: 906 | train_loss: 0.0090 | test_loss: 0.0090 | \n",
            "Epoch: 907 | train_loss: 0.0090 | test_loss: 0.0089 | \n",
            "Epoch: 908 | train_loss: 0.0089 | test_loss: 0.0089 | \n",
            "Epoch: 909 | train_loss: 0.0089 | test_loss: 0.0089 | \n",
            "Epoch: 910 | train_loss: 0.0089 | test_loss: 0.0089 | \n",
            "Epoch: 911 | train_loss: 0.0089 | test_loss: 0.0089 | \n",
            "Epoch: 912 | train_loss: 0.0089 | test_loss: 0.0088 | \n",
            "Epoch: 913 | train_loss: 0.0088 | test_loss: 0.0088 | \n",
            "Epoch: 914 | train_loss: 0.0088 | test_loss: 0.0088 | \n",
            "Epoch: 915 | train_loss: 0.0088 | test_loss: 0.0088 | \n",
            "Epoch: 916 | train_loss: 0.0088 | test_loss: 0.0087 | \n",
            "Epoch: 917 | train_loss: 0.0087 | test_loss: 0.0087 | \n",
            "Epoch: 918 | train_loss: 0.0087 | test_loss: 0.0087 | \n",
            "Epoch: 919 | train_loss: 0.0087 | test_loss: 0.0087 | \n",
            "Epoch: 920 | train_loss: 0.0087 | test_loss: 0.0086 | \n",
            "Epoch: 921 | train_loss: 0.0086 | test_loss: 0.0086 | \n",
            "Epoch: 922 | train_loss: 0.0086 | test_loss: 0.0086 | \n",
            "Epoch: 923 | train_loss: 0.0086 | test_loss: 0.0086 | \n",
            "Epoch: 924 | train_loss: 0.0086 | test_loss: 0.0085 | \n",
            "Epoch: 925 | train_loss: 0.0085 | test_loss: 0.0085 | \n",
            "Epoch: 926 | train_loss: 0.0085 | test_loss: 0.0085 | \n",
            "Epoch: 927 | train_loss: 0.0085 | test_loss: 0.0085 | \n",
            "Epoch: 928 | train_loss: 0.0085 | test_loss: 0.0085 | \n",
            "Epoch: 929 | train_loss: 0.0085 | test_loss: 0.0084 | \n",
            "Epoch: 930 | train_loss: 0.0084 | test_loss: 0.0084 | \n",
            "Epoch: 931 | train_loss: 0.0084 | test_loss: 0.0084 | \n",
            "Epoch: 932 | train_loss: 0.0084 | test_loss: 0.0084 | \n",
            "Epoch: 933 | train_loss: 0.0084 | test_loss: 0.0083 | \n",
            "Epoch: 934 | train_loss: 0.0083 | test_loss: 0.0083 | \n",
            "Epoch: 935 | train_loss: 0.0083 | test_loss: 0.0083 | \n",
            "Epoch: 936 | train_loss: 0.0083 | test_loss: 0.0083 | \n",
            "Epoch: 937 | train_loss: 0.0083 | test_loss: 0.0082 | \n",
            "Epoch: 938 | train_loss: 0.0082 | test_loss: 0.0082 | \n",
            "Epoch: 939 | train_loss: 0.0082 | test_loss: 0.0082 | \n",
            "Epoch: 940 | train_loss: 0.0082 | test_loss: 0.0082 | \n",
            "Epoch: 941 | train_loss: 0.0082 | test_loss: 0.0082 | \n",
            "Epoch: 942 | train_loss: 0.0082 | test_loss: 0.0081 | \n",
            "Epoch: 943 | train_loss: 0.0081 | test_loss: 0.0081 | \n",
            "Epoch: 944 | train_loss: 0.0081 | test_loss: 0.0081 | \n",
            "Epoch: 945 | train_loss: 0.0081 | test_loss: 0.0081 | \n",
            "Epoch: 946 | train_loss: 0.0081 | test_loss: 0.0080 | \n",
            "Epoch: 947 | train_loss: 0.0080 | test_loss: 0.0080 | \n",
            "Epoch: 948 | train_loss: 0.0080 | test_loss: 0.0080 | \n",
            "Epoch: 949 | train_loss: 0.0080 | test_loss: 0.0080 | \n",
            "Epoch: 950 | train_loss: 0.0080 | test_loss: 0.0080 | \n",
            "Epoch: 951 | train_loss: 0.0080 | test_loss: 0.0079 | \n",
            "Epoch: 952 | train_loss: 0.0079 | test_loss: 0.0079 | \n",
            "Epoch: 953 | train_loss: 0.0079 | test_loss: 0.0079 | \n",
            "Epoch: 954 | train_loss: 0.0079 | test_loss: 0.0079 | \n",
            "Epoch: 955 | train_loss: 0.0079 | test_loss: 0.0078 | \n",
            "Epoch: 956 | train_loss: 0.0078 | test_loss: 0.0078 | \n",
            "Epoch: 957 | train_loss: 0.0078 | test_loss: 0.0078 | \n",
            "Epoch: 958 | train_loss: 0.0078 | test_loss: 0.0078 | \n",
            "Epoch: 959 | train_loss: 0.0078 | test_loss: 0.0078 | \n",
            "Epoch: 960 | train_loss: 0.0078 | test_loss: 0.0077 | \n",
            "Epoch: 961 | train_loss: 0.0077 | test_loss: 0.0077 | \n",
            "Epoch: 962 | train_loss: 0.0077 | test_loss: 0.0077 | \n",
            "Epoch: 963 | train_loss: 0.0077 | test_loss: 0.0077 | \n",
            "Epoch: 964 | train_loss: 0.0077 | test_loss: 0.0076 | \n",
            "Epoch: 965 | train_loss: 0.0076 | test_loss: 0.0076 | \n",
            "Epoch: 966 | train_loss: 0.0076 | test_loss: 0.0076 | \n",
            "Epoch: 967 | train_loss: 0.0076 | test_loss: 0.0076 | \n",
            "Epoch: 968 | train_loss: 0.0076 | test_loss: 0.0076 | \n",
            "Epoch: 969 | train_loss: 0.0076 | test_loss: 0.0075 | \n",
            "Epoch: 970 | train_loss: 0.0075 | test_loss: 0.0075 | \n",
            "Epoch: 971 | train_loss: 0.0075 | test_loss: 0.0075 | \n",
            "Epoch: 972 | train_loss: 0.0075 | test_loss: 0.0075 | \n",
            "Epoch: 973 | train_loss: 0.0075 | test_loss: 0.0075 | \n",
            "Epoch: 974 | train_loss: 0.0075 | test_loss: 0.0074 | \n",
            "Epoch: 975 | train_loss: 0.0074 | test_loss: 0.0074 | \n",
            "Epoch: 976 | train_loss: 0.0074 | test_loss: 0.0074 | \n",
            "Epoch: 977 | train_loss: 0.0074 | test_loss: 0.0074 | \n",
            "Epoch: 978 | train_loss: 0.0074 | test_loss: 0.0073 | \n",
            "Epoch: 979 | train_loss: 0.0073 | test_loss: 0.0073 | \n",
            "Epoch: 980 | train_loss: 0.0073 | test_loss: 0.0073 | \n",
            "Epoch: 981 | train_loss: 0.0073 | test_loss: 0.0073 | \n",
            "Epoch: 982 | train_loss: 0.0073 | test_loss: 0.0073 | \n",
            "Epoch: 983 | train_loss: 0.0073 | test_loss: 0.0072 | \n",
            "Epoch: 984 | train_loss: 0.0072 | test_loss: 0.0072 | \n",
            "Epoch: 985 | train_loss: 0.0072 | test_loss: 0.0072 | \n",
            "Epoch: 986 | train_loss: 0.0072 | test_loss: 0.0072 | \n",
            "Epoch: 987 | train_loss: 0.0072 | test_loss: 0.0072 | \n",
            "Epoch: 988 | train_loss: 0.0072 | test_loss: 0.0071 | \n",
            "Epoch: 989 | train_loss: 0.0071 | test_loss: 0.0071 | \n",
            "Epoch: 990 | train_loss: 0.0071 | test_loss: 0.0071 | \n",
            "Epoch: 991 | train_loss: 0.0071 | test_loss: 0.0071 | \n",
            "Epoch: 992 | train_loss: 0.0071 | test_loss: 0.0071 | \n",
            "Epoch: 993 | train_loss: 0.0071 | test_loss: 0.0070 | \n",
            "Epoch: 994 | train_loss: 0.0070 | test_loss: 0.0070 | \n",
            "Epoch: 995 | train_loss: 0.0070 | test_loss: 0.0070 | \n",
            "Epoch: 996 | train_loss: 0.0070 | test_loss: 0.0070 | \n",
            "Epoch: 997 | train_loss: 0.0070 | test_loss: 0.0070 | \n",
            "Epoch: 998 | train_loss: 0.0070 | test_loss: 0.0069 | \n",
            "Epoch: 999 | train_loss: 0.0069 | test_loss: 0.0069 | \n",
            "Epoch: 1000 | train_loss: 0.0069 | test_loss: 0.0069 | \n",
            "Epoch: 1001 | train_loss: 0.0069 | test_loss: 0.0069 | \n",
            "Epoch: 1002 | train_loss: 0.0069 | test_loss: 0.0069 | \n",
            "Epoch: 1003 | train_loss: 0.0069 | test_loss: 0.0068 | \n",
            "Epoch: 1004 | train_loss: 0.0068 | test_loss: 0.0068 | \n",
            "Epoch: 1005 | train_loss: 0.0068 | test_loss: 0.0068 | \n",
            "Epoch: 1006 | train_loss: 0.0068 | test_loss: 0.0068 | \n",
            "Epoch: 1007 | train_loss: 0.0068 | test_loss: 0.0068 | \n",
            "Epoch: 1008 | train_loss: 0.0068 | test_loss: 0.0067 | \n",
            "Epoch: 1009 | train_loss: 0.0067 | test_loss: 0.0067 | \n",
            "Epoch: 1010 | train_loss: 0.0067 | test_loss: 0.0067 | \n",
            "Epoch: 1011 | train_loss: 0.0067 | test_loss: 0.0067 | \n",
            "Epoch: 1012 | train_loss: 0.0067 | test_loss: 0.0067 | \n",
            "Epoch: 1013 | train_loss: 0.0067 | test_loss: 0.0066 | \n",
            "Epoch: 1014 | train_loss: 0.0066 | test_loss: 0.0066 | \n",
            "Epoch: 1015 | train_loss: 0.0066 | test_loss: 0.0066 | \n",
            "Epoch: 1016 | train_loss: 0.0066 | test_loss: 0.0066 | \n",
            "Epoch: 1017 | train_loss: 0.0066 | test_loss: 0.0066 | \n",
            "Epoch: 1018 | train_loss: 0.0066 | test_loss: 0.0065 | \n",
            "Epoch: 1019 | train_loss: 0.0065 | test_loss: 0.0065 | \n",
            "Epoch: 1020 | train_loss: 0.0065 | test_loss: 0.0065 | \n",
            "Epoch: 1021 | train_loss: 0.0065 | test_loss: 0.0065 | \n",
            "Epoch: 1022 | train_loss: 0.0065 | test_loss: 0.0065 | \n",
            "Epoch: 1023 | train_loss: 0.0065 | test_loss: 0.0064 | \n",
            "Epoch: 1024 | train_loss: 0.0064 | test_loss: 0.0064 | \n",
            "Epoch: 1025 | train_loss: 0.0064 | test_loss: 0.0064 | \n",
            "Epoch: 1026 | train_loss: 0.0064 | test_loss: 0.0064 | \n",
            "Epoch: 1027 | train_loss: 0.0064 | test_loss: 0.0064 | \n",
            "Epoch: 1028 | train_loss: 0.0064 | test_loss: 0.0063 | \n",
            "Epoch: 1029 | train_loss: 0.0063 | test_loss: 0.0063 | \n",
            "Epoch: 1030 | train_loss: 0.0063 | test_loss: 0.0063 | \n",
            "Epoch: 1031 | train_loss: 0.0063 | test_loss: 0.0063 | \n",
            "Epoch: 1032 | train_loss: 0.0063 | test_loss: 0.0063 | \n",
            "Epoch: 1033 | train_loss: 0.0063 | test_loss: 0.0062 | \n",
            "Epoch: 1034 | train_loss: 0.0062 | test_loss: 0.0062 | \n",
            "Epoch: 1035 | train_loss: 0.0062 | test_loss: 0.0062 | \n",
            "Epoch: 1036 | train_loss: 0.0062 | test_loss: 0.0062 | \n",
            "Epoch: 1037 | train_loss: 0.0062 | test_loss: 0.0062 | \n",
            "Epoch: 1038 | train_loss: 0.0062 | test_loss: 0.0062 | \n",
            "Epoch: 1039 | train_loss: 0.0062 | test_loss: 0.0061 | \n",
            "Epoch: 1040 | train_loss: 0.0061 | test_loss: 0.0061 | \n",
            "Epoch: 1041 | train_loss: 0.0061 | test_loss: 0.0061 | \n",
            "Epoch: 1042 | train_loss: 0.0061 | test_loss: 0.0061 | \n",
            "Epoch: 1043 | train_loss: 0.0061 | test_loss: 0.0061 | \n",
            "Epoch: 1044 | train_loss: 0.0061 | test_loss: 0.0060 | \n",
            "Epoch: 1045 | train_loss: 0.0060 | test_loss: 0.0060 | \n",
            "Epoch: 1046 | train_loss: 0.0060 | test_loss: 0.0060 | \n",
            "Epoch: 1047 | train_loss: 0.0060 | test_loss: 0.0060 | \n",
            "Epoch: 1048 | train_loss: 0.0060 | test_loss: 0.0060 | \n",
            "Epoch: 1049 | train_loss: 0.0060 | test_loss: 0.0060 | \n",
            "Epoch: 1050 | train_loss: 0.0060 | test_loss: 0.0059 | \n",
            "Epoch: 1051 | train_loss: 0.0059 | test_loss: 0.0059 | \n",
            "Epoch: 1052 | train_loss: 0.0059 | test_loss: 0.0059 | \n",
            "Epoch: 1053 | train_loss: 0.0059 | test_loss: 0.0059 | \n",
            "Epoch: 1054 | train_loss: 0.0059 | test_loss: 0.0059 | \n",
            "Epoch: 1055 | train_loss: 0.0059 | test_loss: 0.0058 | \n",
            "Epoch: 1056 | train_loss: 0.0058 | test_loss: 0.0058 | \n",
            "Epoch: 1057 | train_loss: 0.0058 | test_loss: 0.0058 | \n",
            "Epoch: 1058 | train_loss: 0.0058 | test_loss: 0.0058 | \n",
            "Epoch: 1059 | train_loss: 0.0058 | test_loss: 0.0058 | \n",
            "Epoch: 1060 | train_loss: 0.0058 | test_loss: 0.0058 | \n",
            "Epoch: 1061 | train_loss: 0.0058 | test_loss: 0.0057 | \n",
            "Epoch: 1062 | train_loss: 0.0057 | test_loss: 0.0057 | \n",
            "Epoch: 1063 | train_loss: 0.0057 | test_loss: 0.0057 | \n",
            "Epoch: 1064 | train_loss: 0.0057 | test_loss: 0.0057 | \n",
            "Epoch: 1065 | train_loss: 0.0057 | test_loss: 0.0057 | \n",
            "Epoch: 1066 | train_loss: 0.0057 | test_loss: 0.0057 | \n",
            "Epoch: 1067 | train_loss: 0.0057 | test_loss: 0.0056 | \n",
            "Epoch: 1068 | train_loss: 0.0056 | test_loss: 0.0056 | \n",
            "Epoch: 1069 | train_loss: 0.0056 | test_loss: 0.0056 | \n",
            "Epoch: 1070 | train_loss: 0.0056 | test_loss: 0.0056 | \n",
            "Epoch: 1071 | train_loss: 0.0056 | test_loss: 0.0056 | \n",
            "Epoch: 1072 | train_loss: 0.0056 | test_loss: 0.0055 | \n",
            "Epoch: 1073 | train_loss: 0.0055 | test_loss: 0.0055 | \n",
            "Epoch: 1074 | train_loss: 0.0055 | test_loss: 0.0055 | \n",
            "Epoch: 1075 | train_loss: 0.0055 | test_loss: 0.0055 | \n",
            "Epoch: 1076 | train_loss: 0.0055 | test_loss: 0.0055 | \n",
            "Epoch: 1077 | train_loss: 0.0055 | test_loss: 0.0055 | \n",
            "Epoch: 1078 | train_loss: 0.0055 | test_loss: 0.0054 | \n",
            "Epoch: 1079 | train_loss: 0.0054 | test_loss: 0.0054 | \n",
            "Epoch: 1080 | train_loss: 0.0054 | test_loss: 0.0054 | \n",
            "Epoch: 1081 | train_loss: 0.0054 | test_loss: 0.0054 | \n",
            "Epoch: 1082 | train_loss: 0.0054 | test_loss: 0.0054 | \n",
            "Epoch: 1083 | train_loss: 0.0054 | test_loss: 0.0054 | \n",
            "Epoch: 1084 | train_loss: 0.0054 | test_loss: 0.0053 | \n",
            "Epoch: 1085 | train_loss: 0.0053 | test_loss: 0.0053 | \n",
            "Epoch: 1086 | train_loss: 0.0053 | test_loss: 0.0053 | \n",
            "Epoch: 1087 | train_loss: 0.0053 | test_loss: 0.0053 | \n",
            "Epoch: 1088 | train_loss: 0.0053 | test_loss: 0.0053 | \n",
            "Epoch: 1089 | train_loss: 0.0053 | test_loss: 0.0053 | \n",
            "Epoch: 1090 | train_loss: 0.0053 | test_loss: 0.0052 | \n",
            "Epoch: 1091 | train_loss: 0.0052 | test_loss: 0.0052 | \n",
            "Epoch: 1092 | train_loss: 0.0052 | test_loss: 0.0052 | \n",
            "Epoch: 1093 | train_loss: 0.0052 | test_loss: 0.0052 | \n",
            "Epoch: 1094 | train_loss: 0.0052 | test_loss: 0.0052 | \n",
            "Epoch: 1095 | train_loss: 0.0052 | test_loss: 0.0052 | \n",
            "Epoch: 1096 | train_loss: 0.0052 | test_loss: 0.0051 | \n",
            "Epoch: 1097 | train_loss: 0.0051 | test_loss: 0.0051 | \n",
            "Epoch: 1098 | train_loss: 0.0051 | test_loss: 0.0051 | \n",
            "Epoch: 1099 | train_loss: 0.0051 | test_loss: 0.0051 | \n",
            "Epoch: 1100 | train_loss: 0.0051 | test_loss: 0.0051 | \n",
            "Epoch: 1101 | train_loss: 0.0051 | test_loss: 0.0051 | \n",
            "Epoch: 1102 | train_loss: 0.0051 | test_loss: 0.0050 | \n",
            "Epoch: 1103 | train_loss: 0.0050 | test_loss: 0.0050 | \n",
            "Epoch: 1104 | train_loss: 0.0050 | test_loss: 0.0050 | \n",
            "Epoch: 1105 | train_loss: 0.0050 | test_loss: 0.0050 | \n",
            "Epoch: 1106 | train_loss: 0.0050 | test_loss: 0.0050 | \n",
            "Epoch: 1107 | train_loss: 0.0050 | test_loss: 0.0050 | \n",
            "Epoch: 1108 | train_loss: 0.0050 | test_loss: 0.0050 | \n",
            "Epoch: 1109 | train_loss: 0.0050 | test_loss: 0.0049 | \n",
            "Epoch: 1110 | train_loss: 0.0049 | test_loss: 0.0049 | \n",
            "Epoch: 1111 | train_loss: 0.0049 | test_loss: 0.0049 | \n",
            "Epoch: 1112 | train_loss: 0.0049 | test_loss: 0.0049 | \n",
            "Epoch: 1113 | train_loss: 0.0049 | test_loss: 0.0049 | \n",
            "Epoch: 1114 | train_loss: 0.0049 | test_loss: 0.0049 | \n",
            "Epoch: 1115 | train_loss: 0.0049 | test_loss: 0.0048 | \n",
            "Epoch: 1116 | train_loss: 0.0048 | test_loss: 0.0048 | \n",
            "Epoch: 1117 | train_loss: 0.0048 | test_loss: 0.0048 | \n",
            "Epoch: 1118 | train_loss: 0.0048 | test_loss: 0.0048 | \n",
            "Epoch: 1119 | train_loss: 0.0048 | test_loss: 0.0048 | \n",
            "Epoch: 1120 | train_loss: 0.0048 | test_loss: 0.0048 | \n",
            "Epoch: 1121 | train_loss: 0.0048 | test_loss: 0.0048 | \n",
            "Epoch: 1122 | train_loss: 0.0048 | test_loss: 0.0047 | \n",
            "Epoch: 1123 | train_loss: 0.0047 | test_loss: 0.0047 | \n",
            "Epoch: 1124 | train_loss: 0.0047 | test_loss: 0.0047 | \n",
            "Epoch: 1125 | train_loss: 0.0047 | test_loss: 0.0047 | \n",
            "Epoch: 1126 | train_loss: 0.0047 | test_loss: 0.0047 | \n",
            "Epoch: 1127 | train_loss: 0.0047 | test_loss: 0.0047 | \n",
            "Epoch: 1128 | train_loss: 0.0047 | test_loss: 0.0046 | \n",
            "Epoch: 1129 | train_loss: 0.0046 | test_loss: 0.0046 | \n",
            "Epoch: 1130 | train_loss: 0.0046 | test_loss: 0.0046 | \n",
            "Epoch: 1131 | train_loss: 0.0046 | test_loss: 0.0046 | \n",
            "Epoch: 1132 | train_loss: 0.0046 | test_loss: 0.0046 | \n",
            "Epoch: 1133 | train_loss: 0.0046 | test_loss: 0.0046 | \n",
            "Epoch: 1134 | train_loss: 0.0046 | test_loss: 0.0046 | \n",
            "Epoch: 1135 | train_loss: 0.0046 | test_loss: 0.0045 | \n",
            "Epoch: 1136 | train_loss: 0.0045 | test_loss: 0.0045 | \n",
            "Epoch: 1137 | train_loss: 0.0045 | test_loss: 0.0045 | \n",
            "Epoch: 1138 | train_loss: 0.0045 | test_loss: 0.0045 | \n",
            "Epoch: 1139 | train_loss: 0.0045 | test_loss: 0.0045 | \n",
            "Epoch: 1140 | train_loss: 0.0045 | test_loss: 0.0045 | \n",
            "Epoch: 1141 | train_loss: 0.0045 | test_loss: 0.0045 | \n",
            "Epoch: 1142 | train_loss: 0.0045 | test_loss: 0.0044 | \n",
            "Epoch: 1143 | train_loss: 0.0044 | test_loss: 0.0044 | \n",
            "Epoch: 1144 | train_loss: 0.0044 | test_loss: 0.0044 | \n",
            "Epoch: 1145 | train_loss: 0.0044 | test_loss: 0.0044 | \n",
            "Epoch: 1146 | train_loss: 0.0044 | test_loss: 0.0044 | \n",
            "Epoch: 1147 | train_loss: 0.0044 | test_loss: 0.0044 | \n",
            "Epoch: 1148 | train_loss: 0.0044 | test_loss: 0.0043 | \n",
            "Epoch: 1149 | train_loss: 0.0043 | test_loss: 0.0043 | \n",
            "Epoch: 1150 | train_loss: 0.0043 | test_loss: 0.0043 | \n",
            "Epoch: 1151 | train_loss: 0.0043 | test_loss: 0.0043 | \n",
            "Epoch: 1152 | train_loss: 0.0043 | test_loss: 0.0043 | \n",
            "Epoch: 1153 | train_loss: 0.0043 | test_loss: 0.0043 | \n",
            "Epoch: 1154 | train_loss: 0.0043 | test_loss: 0.0043 | \n",
            "Epoch: 1155 | train_loss: 0.0043 | test_loss: 0.0042 | \n",
            "Epoch: 1156 | train_loss: 0.0042 | test_loss: 0.0042 | \n",
            "Epoch: 1157 | train_loss: 0.0042 | test_loss: 0.0042 | \n",
            "Epoch: 1158 | train_loss: 0.0042 | test_loss: 0.0042 | \n",
            "Epoch: 1159 | train_loss: 0.0042 | test_loss: 0.0042 | \n",
            "Epoch: 1160 | train_loss: 0.0042 | test_loss: 0.0042 | \n",
            "Epoch: 1161 | train_loss: 0.0042 | test_loss: 0.0042 | \n",
            "Epoch: 1162 | train_loss: 0.0042 | test_loss: 0.0042 | \n",
            "Epoch: 1163 | train_loss: 0.0042 | test_loss: 0.0041 | \n",
            "Epoch: 1164 | train_loss: 0.0041 | test_loss: 0.0041 | \n",
            "Epoch: 1165 | train_loss: 0.0041 | test_loss: 0.0041 | \n",
            "Epoch: 1166 | train_loss: 0.0041 | test_loss: 0.0041 | \n",
            "Epoch: 1167 | train_loss: 0.0041 | test_loss: 0.0041 | \n",
            "Epoch: 1168 | train_loss: 0.0041 | test_loss: 0.0041 | \n",
            "Epoch: 1169 | train_loss: 0.0041 | test_loss: 0.0041 | \n",
            "Epoch: 1170 | train_loss: 0.0041 | test_loss: 0.0040 | \n",
            "Epoch: 1171 | train_loss: 0.0040 | test_loss: 0.0040 | \n",
            "Epoch: 1172 | train_loss: 0.0040 | test_loss: 0.0040 | \n",
            "Epoch: 1173 | train_loss: 0.0040 | test_loss: 0.0040 | \n",
            "Epoch: 1174 | train_loss: 0.0040 | test_loss: 0.0040 | \n",
            "Epoch: 1175 | train_loss: 0.0040 | test_loss: 0.0040 | \n",
            "Epoch: 1176 | train_loss: 0.0040 | test_loss: 0.0040 | \n",
            "Epoch: 1177 | train_loss: 0.0040 | test_loss: 0.0039 | \n",
            "Epoch: 1178 | train_loss: 0.0039 | test_loss: 0.0039 | \n",
            "Epoch: 1179 | train_loss: 0.0039 | test_loss: 0.0039 | \n",
            "Epoch: 1180 | train_loss: 0.0039 | test_loss: 0.0039 | \n",
            "Epoch: 1181 | train_loss: 0.0039 | test_loss: 0.0039 | \n",
            "Epoch: 1182 | train_loss: 0.0039 | test_loss: 0.0039 | \n",
            "Epoch: 1183 | train_loss: 0.0039 | test_loss: 0.0039 | \n",
            "Epoch: 1184 | train_loss: 0.0039 | test_loss: 0.0039 | \n",
            "Epoch: 1185 | train_loss: 0.0039 | test_loss: 0.0038 | \n",
            "Epoch: 1186 | train_loss: 0.0038 | test_loss: 0.0038 | \n",
            "Epoch: 1187 | train_loss: 0.0038 | test_loss: 0.0038 | \n",
            "Epoch: 1188 | train_loss: 0.0038 | test_loss: 0.0038 | \n",
            "Epoch: 1189 | train_loss: 0.0038 | test_loss: 0.0038 | \n",
            "Epoch: 1190 | train_loss: 0.0038 | test_loss: 0.0038 | \n",
            "Epoch: 1191 | train_loss: 0.0038 | test_loss: 0.0038 | \n",
            "Epoch: 1192 | train_loss: 0.0038 | test_loss: 0.0038 | \n",
            "Epoch: 1193 | train_loss: 0.0038 | test_loss: 0.0037 | \n",
            "Epoch: 1194 | train_loss: 0.0037 | test_loss: 0.0037 | \n",
            "Epoch: 1195 | train_loss: 0.0037 | test_loss: 0.0037 | \n",
            "Epoch: 1196 | train_loss: 0.0037 | test_loss: 0.0037 | \n",
            "Epoch: 1197 | train_loss: 0.0037 | test_loss: 0.0037 | \n",
            "Epoch: 1198 | train_loss: 0.0037 | test_loss: 0.0037 | \n",
            "Epoch: 1199 | train_loss: 0.0037 | test_loss: 0.0037 | \n",
            "Epoch: 1200 | train_loss: 0.0037 | test_loss: 0.0036 | \n",
            "Epoch: 1201 | train_loss: 0.0036 | test_loss: 0.0036 | \n",
            "Epoch: 1202 | train_loss: 0.0036 | test_loss: 0.0036 | \n",
            "Epoch: 1203 | train_loss: 0.0036 | test_loss: 0.0036 | \n",
            "Epoch: 1204 | train_loss: 0.0036 | test_loss: 0.0036 | \n",
            "Epoch: 1205 | train_loss: 0.0036 | test_loss: 0.0036 | \n",
            "Epoch: 1206 | train_loss: 0.0036 | test_loss: 0.0036 | \n",
            "Epoch: 1207 | train_loss: 0.0036 | test_loss: 0.0036 | \n",
            "Epoch: 1208 | train_loss: 0.0036 | test_loss: 0.0035 | \n",
            "Epoch: 1209 | train_loss: 0.0035 | test_loss: 0.0035 | \n",
            "Epoch: 1210 | train_loss: 0.0035 | test_loss: 0.0035 | \n",
            "Epoch: 1211 | train_loss: 0.0035 | test_loss: 0.0035 | \n",
            "Epoch: 1212 | train_loss: 0.0035 | test_loss: 0.0035 | \n",
            "Epoch: 1213 | train_loss: 0.0035 | test_loss: 0.0035 | \n",
            "Epoch: 1214 | train_loss: 0.0035 | test_loss: 0.0035 | \n",
            "Epoch: 1215 | train_loss: 0.0035 | test_loss: 0.0035 | \n",
            "Epoch: 1216 | train_loss: 0.0035 | test_loss: 0.0035 | \n",
            "Epoch: 1217 | train_loss: 0.0035 | test_loss: 0.0034 | \n",
            "Epoch: 1218 | train_loss: 0.0034 | test_loss: 0.0034 | \n",
            "Epoch: 1219 | train_loss: 0.0034 | test_loss: 0.0034 | \n",
            "Epoch: 1220 | train_loss: 0.0034 | test_loss: 0.0034 | \n",
            "Epoch: 1221 | train_loss: 0.0034 | test_loss: 0.0034 | \n",
            "Epoch: 1222 | train_loss: 0.0034 | test_loss: 0.0034 | \n",
            "Epoch: 1223 | train_loss: 0.0034 | test_loss: 0.0034 | \n",
            "Epoch: 1224 | train_loss: 0.0034 | test_loss: 0.0034 | \n",
            "Epoch: 1225 | train_loss: 0.0034 | test_loss: 0.0033 | \n",
            "Epoch: 1226 | train_loss: 0.0033 | test_loss: 0.0033 | \n",
            "Epoch: 1227 | train_loss: 0.0033 | test_loss: 0.0033 | \n",
            "Epoch: 1228 | train_loss: 0.0033 | test_loss: 0.0033 | \n",
            "Epoch: 1229 | train_loss: 0.0033 | test_loss: 0.0033 | \n",
            "Epoch: 1230 | train_loss: 0.0033 | test_loss: 0.0033 | \n",
            "Epoch: 1231 | train_loss: 0.0033 | test_loss: 0.0033 | \n",
            "Epoch: 1232 | train_loss: 0.0033 | test_loss: 0.0033 | \n",
            "Epoch: 1233 | train_loss: 0.0033 | test_loss: 0.0033 | \n",
            "Epoch: 1234 | train_loss: 0.0033 | test_loss: 0.0032 | \n",
            "Epoch: 1235 | train_loss: 0.0032 | test_loss: 0.0032 | \n",
            "Epoch: 1236 | train_loss: 0.0032 | test_loss: 0.0032 | \n",
            "Epoch: 1237 | train_loss: 0.0032 | test_loss: 0.0032 | \n",
            "Epoch: 1238 | train_loss: 0.0032 | test_loss: 0.0032 | \n",
            "Epoch: 1239 | train_loss: 0.0032 | test_loss: 0.0032 | \n",
            "Epoch: 1240 | train_loss: 0.0032 | test_loss: 0.0032 | \n",
            "Epoch: 1241 | train_loss: 0.0032 | test_loss: 0.0032 | \n",
            "Epoch: 1242 | train_loss: 0.0032 | test_loss: 0.0031 | \n",
            "Epoch: 1243 | train_loss: 0.0031 | test_loss: 0.0031 | \n",
            "Epoch: 1244 | train_loss: 0.0031 | test_loss: 0.0031 | \n",
            "Epoch: 1245 | train_loss: 0.0031 | test_loss: 0.0031 | \n",
            "Epoch: 1246 | train_loss: 0.0031 | test_loss: 0.0031 | \n",
            "Epoch: 1247 | train_loss: 0.0031 | test_loss: 0.0031 | \n",
            "Epoch: 1248 | train_loss: 0.0031 | test_loss: 0.0031 | \n",
            "Epoch: 1249 | train_loss: 0.0031 | test_loss: 0.0031 | \n",
            "Epoch: 1250 | train_loss: 0.0031 | test_loss: 0.0031 | \n",
            "Epoch: 1251 | train_loss: 0.0031 | test_loss: 0.0030 | \n",
            "Epoch: 1252 | train_loss: 0.0030 | test_loss: 0.0030 | \n",
            "Epoch: 1253 | train_loss: 0.0030 | test_loss: 0.0030 | \n",
            "Epoch: 1254 | train_loss: 0.0030 | test_loss: 0.0030 | \n",
            "Epoch: 1255 | train_loss: 0.0030 | test_loss: 0.0030 | \n",
            "Epoch: 1256 | train_loss: 0.0030 | test_loss: 0.0030 | \n",
            "Epoch: 1257 | train_loss: 0.0030 | test_loss: 0.0030 | \n",
            "Epoch: 1258 | train_loss: 0.0030 | test_loss: 0.0030 | \n",
            "Epoch: 1259 | train_loss: 0.0030 | test_loss: 0.0030 | \n",
            "Epoch: 1260 | train_loss: 0.0030 | test_loss: 0.0030 | \n",
            "Epoch: 1261 | train_loss: 0.0030 | test_loss: 0.0029 | \n",
            "Epoch: 1262 | train_loss: 0.0029 | test_loss: 0.0029 | \n",
            "Epoch: 1263 | train_loss: 0.0029 | test_loss: 0.0029 | \n",
            "Epoch: 1264 | train_loss: 0.0029 | test_loss: 0.0029 | \n",
            "Epoch: 1265 | train_loss: 0.0029 | test_loss: 0.0029 | \n",
            "Epoch: 1266 | train_loss: 0.0029 | test_loss: 0.0029 | \n",
            "Epoch: 1267 | train_loss: 0.0029 | test_loss: 0.0029 | \n",
            "Epoch: 1268 | train_loss: 0.0029 | test_loss: 0.0029 | \n",
            "Epoch: 1269 | train_loss: 0.0029 | test_loss: 0.0029 | \n",
            "Epoch: 1270 | train_loss: 0.0029 | test_loss: 0.0028 | \n",
            "Epoch: 1271 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 1272 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 1273 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 1274 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 1275 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 1276 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 1277 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 1278 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 1279 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 1280 | train_loss: 0.0028 | test_loss: 0.0027 | \n",
            "Epoch: 1281 | train_loss: 0.0027 | test_loss: 0.0027 | \n",
            "Epoch: 1282 | train_loss: 0.0027 | test_loss: 0.0027 | \n",
            "Epoch: 1283 | train_loss: 0.0027 | test_loss: 0.0027 | \n",
            "Epoch: 1284 | train_loss: 0.0027 | test_loss: 0.0027 | \n",
            "Epoch: 1285 | train_loss: 0.0027 | test_loss: 0.0027 | \n",
            "Epoch: 1286 | train_loss: 0.0027 | test_loss: 0.0027 | \n",
            "Epoch: 1287 | train_loss: 0.0027 | test_loss: 0.0027 | \n",
            "Epoch: 1288 | train_loss: 0.0027 | test_loss: 0.0027 | \n",
            "Epoch: 1289 | train_loss: 0.0027 | test_loss: 0.0027 | \n",
            "Epoch: 1290 | train_loss: 0.0027 | test_loss: 0.0026 | \n",
            "Epoch: 1291 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 1292 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 1293 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 1294 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 1295 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 1296 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 1297 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 1298 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 1299 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 1300 | train_loss: 0.0026 | test_loss: 0.0025 | \n",
            "Epoch: 1301 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 1302 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 1303 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 1304 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 1305 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 1306 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 1307 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 1308 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 1309 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 1310 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 1311 | train_loss: 0.0025 | test_loss: 0.0024 | \n",
            "Epoch: 1312 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 1313 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 1314 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 1315 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 1316 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 1317 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 1318 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 1319 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 1320 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 1321 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 1322 | train_loss: 0.0024 | test_loss: 0.0023 | \n",
            "Epoch: 1323 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 1324 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 1325 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 1326 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 1327 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 1328 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 1329 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 1330 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 1331 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 1332 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 1333 | train_loss: 0.0023 | test_loss: 0.0022 | \n",
            "Epoch: 1334 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 1335 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 1336 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 1337 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 1338 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 1339 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 1340 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 1341 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 1342 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 1343 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 1344 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 1345 | train_loss: 0.0022 | test_loss: 0.0021 | \n",
            "Epoch: 1346 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 1347 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 1348 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 1349 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 1350 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 1351 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 1352 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 1353 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 1354 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 1355 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 1356 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 1357 | train_loss: 0.0021 | test_loss: 0.0020 | \n",
            "Epoch: 1358 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1359 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1360 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1361 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1362 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1363 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1364 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1365 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1366 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1367 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1368 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1369 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 1370 | train_loss: 0.0020 | test_loss: 0.0019 | \n",
            "Epoch: 1371 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1372 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1373 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1374 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1375 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1376 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1377 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1378 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1379 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1380 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1381 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1382 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 1383 | train_loss: 0.0019 | test_loss: 0.0018 | \n",
            "Epoch: 1384 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1385 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1386 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1387 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1388 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1389 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1390 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1391 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1392 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1393 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1394 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1395 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1396 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 1397 | train_loss: 0.0018 | test_loss: 0.0017 | \n",
            "Epoch: 1398 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1399 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1400 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1401 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1402 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1403 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1404 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1405 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1406 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1407 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1408 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1409 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1410 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1411 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 1412 | train_loss: 0.0017 | test_loss: 0.0016 | \n",
            "Epoch: 1413 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1414 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1415 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1416 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1417 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1418 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1419 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1420 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1421 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1422 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1423 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1424 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1425 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1426 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 1427 | train_loss: 0.0016 | test_loss: 0.0015 | \n",
            "Epoch: 1428 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1429 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1430 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1431 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1432 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1433 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1434 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1435 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1436 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1437 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1438 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1439 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1440 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1441 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1442 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1443 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 1444 | train_loss: 0.0015 | test_loss: 0.0014 | \n",
            "Epoch: 1445 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1446 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1447 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1448 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1449 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1450 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1451 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1452 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1453 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1454 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1455 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1456 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1457 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1458 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1459 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1460 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 1461 | train_loss: 0.0014 | test_loss: 0.0013 | \n",
            "Epoch: 1462 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1463 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1464 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1465 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1466 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1467 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1468 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1469 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1470 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1471 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1472 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1473 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1474 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1475 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1476 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1477 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1478 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 1479 | train_loss: 0.0013 | test_loss: 0.0012 | \n",
            "Epoch: 1480 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1481 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1482 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1483 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1484 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1485 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1486 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1487 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1488 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1489 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1490 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1491 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1492 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1493 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1494 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1495 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1496 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1497 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 1498 | train_loss: 0.0012 | test_loss: 0.0011 | \n",
            "Epoch: 1499 | train_loss: 0.0011 | test_loss: 0.0011 | \n",
            "Epoch: 1500 | train_loss: 0.0011 | test_loss: 0.0011 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([i+1 for i in range(1500)],model_resulrs[\"train_loss\"])\n",
        "plt.title(\"train_loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "-BQ6aiO7cY9H",
        "outputId": "00a2b680-7e18-4a4c-8efa-f17ad41308fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_hat=model(y)"
      ],
      "metadata": {
        "id": "ARjKs32cc3mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_hat=x_hat.detach().numpy()"
      ],
      "metadata": {
        "id": "cZ9j5aHpdoVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([i for i in range(n)],x)\n",
        "plt.scatter([i for i in range(n)], x_hat,color='r')\n",
        "plt.legend([\"x\", \"x_hat\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Syh6ysY5dpZ_",
        "outputId": "84c31092-130c-402f-ab35-135f3a463d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fu4MXvUgnSf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}