{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "030f4e306360467d944585480ac4c2a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b08fa9d144764024a5885e69d86487b1",
              "IPY_MODEL_abe0f52a6da2463985fec98ceaa68391",
              "IPY_MODEL_d9e25a39e5934cde8c986fa81c1f3bd7"
            ],
            "layout": "IPY_MODEL_9e247d50f99840cebe8707986a65f4e6"
          }
        },
        "b08fa9d144764024a5885e69d86487b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c12300fb8b4f458b8c0dbe0ac38b844b",
            "placeholder": "​",
            "style": "IPY_MODEL_76241f329c144081aa502c1c0a4c85a7",
            "value": "100%"
          }
        },
        "abe0f52a6da2463985fec98ceaa68391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1694c6f7c874d459ff7fa4181ead011",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cab1b9a248e4c9a83c9b69a68b20974",
            "value": 1000
          }
        },
        "d9e25a39e5934cde8c986fa81c1f3bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_831913d4849f443bb7519cc795f7d65c",
            "placeholder": "​",
            "style": "IPY_MODEL_de76584795484e638b2323a419a9d78d",
            "value": " 1000/1000 [00:02&lt;00:00, 346.18it/s]"
          }
        },
        "9e247d50f99840cebe8707986a65f4e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c12300fb8b4f458b8c0dbe0ac38b844b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76241f329c144081aa502c1c0a4c85a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1694c6f7c874d459ff7fa4181ead011": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cab1b9a248e4c9a83c9b69a68b20974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "831913d4849f443bb7519cc795f7d65c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de76584795484e638b2323a419a9d78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries & dependancies"
      ],
      "metadata": {
        "id": "_Fx7YQ7q46kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n"
      ],
      "metadata": {
        "id": "NLFnTO_LFBe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data"
      ],
      "metadata": {
        "id": "gSC0DwRwCI1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=500\n",
        "m=50\n",
        "k=10\n",
        "np.random.seed(123)\n",
        "A = np.random.rand(m, n)\n",
        "x = np.zeros(n)\n",
        "indices = np.random.choice(n, size=k, replace=False)\n",
        "values = np.random.randn(k)\n",
        "x[indices] = values\n",
        "\n",
        "def f(x):\n",
        "  return x+np.cos(x)\n",
        "\n",
        "def f1(x):\n",
        "  return x**2+np.tan(x)\n",
        "\n",
        "y=f(A@x)+ np.random.normal(0, 1, size=(m,))"
      ],
      "metadata": {
        "id": "HI0iaYO4He-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZHWmmLvd19p",
        "outputId": "cd96ae8c-7459-4986-ae66-5605bd293d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.96469186e-01, 2.86139335e-01, 2.26851454e-01, ...,\n",
              "        9.19472466e-01, 4.15503551e-01, 7.44615462e-01],\n",
              "       [2.12831499e-01, 3.92304071e-01, 8.51548051e-01, ...,\n",
              "        3.47146060e-01, 4.16848861e-03, 2.94894709e-01],\n",
              "       [8.18944391e-02, 4.95039632e-01, 2.88890069e-01, ...,\n",
              "        1.32896171e-01, 9.38071647e-01, 3.55359720e-01],\n",
              "       ...,\n",
              "       [2.01719636e-01, 2.57115662e-01, 3.58485163e-01, ...,\n",
              "        9.38150476e-01, 5.08211335e-04, 9.08868262e-01],\n",
              "       [4.01837959e-01, 3.72953902e-01, 7.53139774e-02, ...,\n",
              "        5.41103094e-01, 8.75145753e-02, 7.55583890e-01],\n",
              "       [2.73696789e-01, 5.58794739e-01, 3.73961243e-01, ...,\n",
              "        1.66331380e-01, 4.47537773e-01, 4.98904893e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvalues = np.linalg.eigvals(A.T@A)\n",
        "max_eig_val=np.max(eigenvalues)\n",
        "max_eig_val"
      ],
      "metadata": {
        "id": "nkiN7e8qefid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85297c6a-4d3e-4125-b525-dbc0a8c42121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6286.1355184455615+0j)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fwyRYmAC9ar",
        "outputId": "61dd00d8-d797-43f6-9a41-26fbe7ea59ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.321525  ,  1.79944356,  0.60334248, -0.42184335,  1.56814484,\n",
              "        0.66592268,  1.20425459,  1.65801453, -1.19291763,  1.7171894 ,\n",
              "        1.1271925 ,  0.97658039,  1.29899626,  1.64733131,  1.84760325,\n",
              "        1.37107513,  0.08860489,  1.41448283, -0.36830002,  0.22818901,\n",
              "        1.008177  ,  1.62108393, -0.27521696,  1.08347976,  0.79708197,\n",
              "        0.5301349 ,  0.44285146,  1.18002812,  1.8492984 ,  2.16432603,\n",
              "        0.88454951,  1.58754792,  2.08664952,  1.27594895,  1.86091357,\n",
              "        2.07736331,  0.7115356 ,  1.12129726,  2.2207015 ,  2.23451374,\n",
              "        1.55342728,  3.6152811 ,  1.43215066,  1.403655  , -0.78897997,\n",
              "        0.44555626,  1.95072513,  2.13327781,  2.28853572,  1.70892077])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWQ9VaXJDI7u",
        "outputId": "f180c0ba-ec70-4ad0-f644-236a42400afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  1.33194488,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.91756875,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        , -0.67092755,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , -2.00870453,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  1.48906165,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  1.04454086,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  1.03159348,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.09584389,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.36928112,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        , -0.15828097,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A=torch.Tensor(A)\n",
        "x=torch.Tensor(x)\n",
        "y=torch.Tensor(y)"
      ],
      "metadata": {
        "id": "f1mSLbf9IkRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqXAFs3SIsfe",
        "outputId": "c942864a-539f-4928-da5b-a6a894786498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         1.3319,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.9176,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6709,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000, -2.0087,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.4891,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0445,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0316,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0958,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3693,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1583,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         0.0000,  0.0000,  0.0000,  0.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model"
      ],
      "metadata": {
        "id": "3RnO9vYFB8AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NLISTAStep(nn.Module):\n",
        "    def __init__(self, A=A,y=y,input_size=m, hidden_size=n,alpha=6287):\n",
        "        super(NLISTAStep, self).__init__()\n",
        "        self.A = A\n",
        "        self.dense = nn.Linear(input_size, hidden_size) \n",
        "        self.theta = nn.Parameter(torch.tensor(0.5))\n",
        "        self.beta = nn.Parameter(torch.tensor(1/alpha))\n",
        "        self.y = y\n",
        "    \n",
        "    def soft_thresholding(self, u):\n",
        "        sign = torch.sign(u)\n",
        "        abs_u = torch.abs(u)\n",
        "        thresholded_values = torch.max(abs_u - self.theta, torch.zeros_like(abs_u))\n",
        "        thresholded_values = sign * thresholded_values\n",
        "        return thresholded_values\n",
        "\n",
        "    def f(self, x):\n",
        "        Ax = torch.matmul(self.A, x)\n",
        "        f_Ax = Ax + torch.cos(Ax)\n",
        "        return f_Ax  \n",
        "\n",
        "    def f_prime(self, x):\n",
        "        Ax = torch.matmul(self.A, x)\n",
        "        f_Ax = torch.ones((m,))  - torch.sin(Ax)\n",
        "        return f_Ax  \n",
        "\n",
        "    def grad_f(self,x):\n",
        "        return torch.diag(self.f_prime(x))\n",
        "\n",
        "    def gamma(self,x):\n",
        "        numerator = torch.norm(torch.matmul(self.grad_f(x), (self.y - self.f(x))))\n",
        "        if numerator <= 1:\n",
        "            return 1\n",
        "        else:\n",
        "            return 1 / numerator    \n",
        "\n",
        "    \n",
        "    def forward(self, x ):\n",
        "        u= self.dense(torch.matmul(self.grad_f(x), (self.y - self.f(x))))\n",
        "        x = x + self.beta*u\n",
        "        return x\n",
        "\n",
        "\n",
        "class  NLISTA(nn.Module):\n",
        "    def __init__(self, T=2, **dnet_args ):\n",
        "        super(NLISTA, self).__init__()\n",
        "        self.dnets = nn.ModuleList([NLISTAStep( **dnet_args) for i in range(T)])\n",
        "    \n",
        "    def step(self, u, i):\n",
        "        return self.dnets[i].forward(u)\n",
        "    \n",
        "    def forward(self, u):\n",
        "        u=torch.zeros(n)\n",
        "        for i in range(len(self.dnets)):\n",
        "            u = self.step(u, i)\n",
        "        return u"
      ],
      "metadata": {
        "id": "z1BzR5fyB6l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLISTAStep1(nn.Module):\n",
        "    def __init__(self, A=A,y=y,input_size=m, hidden_size=n,alpha=6287):\n",
        "        super(NLISTAStep1, self).__init__()\n",
        "        self.A = A\n",
        "        self.dense = nn.Linear(input_size, hidden_size) \n",
        "        self.theta = nn.Parameter(torch.tensor(0.5))\n",
        "        self.beta = nn.Parameter(torch.tensor(1/alpha))\n",
        "        self.y = y\n",
        "    \n",
        "    def soft_thresholding(self, u):\n",
        "        sign = torch.sign(u)\n",
        "        abs_u = torch.abs(u)\n",
        "        thresholded_values = torch.max(abs_u - self.theta, torch.zeros_like(abs_u))\n",
        "        thresholded_values = sign * thresholded_values\n",
        "        return thresholded_values\n",
        "\n",
        "    def f(self, x):\n",
        "        Ax = torch.matmul(self.A, x)\n",
        "        f_Ax = Ax + torch.tan(Ax)\n",
        "        return f_Ax  \n",
        "\n",
        "    def f_prime(self, x):\n",
        "        Ax = torch.matmul(self.A, x)\n",
        "        f_Ax = 2*Ax + torch.ones((m,))  - torch.pow(torch.tan(Ax),2)\n",
        "        return f_Ax  \n",
        "\n",
        "    def grad_f(self,x):\n",
        "        return torch.diag(self.f_prime(x))\n",
        "\n",
        "    def gamma(self,x):\n",
        "        numerator = torch.norm(torch.matmul(self.grad_f(x), (self.y - self.f(x))))\n",
        "        if numerator <= 1:\n",
        "            return 1\n",
        "        else:\n",
        "            return 1 / numerator    \n",
        "\n",
        "    \n",
        "    def forward(self, x ):\n",
        "        u= self.dense(torch.matmul(self.grad_f(x), (self.y - self.f(x))))\n",
        "        x = x + self.beta*u\n",
        "        return x\n",
        "\n",
        "\n",
        "class  NLISTA1(nn.Module):\n",
        "    def __init__(self, T=2, **dnet_args ):\n",
        "        super(NLISTA1, self).__init__()\n",
        "        self.dnets = nn.ModuleList([NLISTAStep1( **dnet_args) for i in range(T)])\n",
        "    \n",
        "    def step(self, u, i):\n",
        "        return self.dnets[i].forward(u)\n",
        "    \n",
        "    def forward(self, u):\n",
        "        u=torch.zeros(n)\n",
        "        for i in range(len(self.dnets)):\n",
        "            u = self.step(u, i)\n",
        "        return u"
      ],
      "metadata": {
        "id": "s0b_uoU5UOnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=NLISTA()"
      ],
      "metadata": {
        "id": "cjae41K1X86L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "summary(model,(10,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAnvKyxVXiE4",
        "outputId": "a267020c-7953-4710-d960-a09291db0eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                       [-1]          25,500\n",
            "            Linear-2                       [-1]          25,500\n",
            "================================================================\n",
            "Total params: 51,000\n",
            "Trainable params: 51,000\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.19\n",
            "Estimated Total Size (MB): 0.19\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train"
      ],
      "metadata": {
        "id": "-xBvgEtVM7-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(x,y,model: torch.nn.Module, \n",
        "               loss_fn: torch.nn.Module, \n",
        "               optimizer: torch.optim.Optimizer):\n",
        "    model.train()\n",
        "  \n",
        "    train_loss= 0\n",
        "  \n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(y)\n",
        "\n",
        "    loss = loss_fn(output, x)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item() \n",
        "\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "lU_JDetNfDaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(x,y,model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module):\n",
        "    model.eval() \n",
        "  \n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "            output = model(y)\n",
        "            loss = loss_fn(output, x)\n",
        "            test_loss += loss.item()\n",
        " \n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "bl6lX-ThZEMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(x,y,model: torch.nn.Module,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs):\n",
        "    \n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "      \"test_loss\": []\n",
        "    }\n",
        "  \n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss = train_step(x,y,model=model,\n",
        "                                loss_fn=loss_fn,\n",
        "                                optimizer=optimizer)\n",
        "                                \n",
        "        \n",
        "        test_loss = test_step(x,y,model=model,\n",
        "                              loss_fn=loss_fn\n",
        "                              )\n",
        "      \n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ],
      "metadata": {
        "id": "AHb1aNrdZf1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn =   nn.MSELoss()\n",
        "model_resulrs = train(x,y,model=model, \n",
        "     optimizer=optimizer,\n",
        "     loss_fn=loss_fn,\n",
        "     epochs=1000,\n",
        "     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "030f4e306360467d944585480ac4c2a4",
            "b08fa9d144764024a5885e69d86487b1",
            "abe0f52a6da2463985fec98ceaa68391",
            "d9e25a39e5934cde8c986fa81c1f3bd7",
            "9e247d50f99840cebe8707986a65f4e6",
            "c12300fb8b4f458b8c0dbe0ac38b844b",
            "76241f329c144081aa502c1c0a4c85a7",
            "a1694c6f7c874d459ff7fa4181ead011",
            "8cab1b9a248e4c9a83c9b69a68b20974",
            "831913d4849f443bb7519cc795f7d65c",
            "de76584795484e638b2323a419a9d78d"
          ]
        },
        "id": "WVJOjUdJaCLm",
        "outputId": "8f552abb-2265-4e85-9c91-8ec78396d462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "030f4e306360467d944585480ac4c2a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 2 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 3 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 4 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 5 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 6 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 7 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 8 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 9 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 10 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 11 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 12 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 13 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 14 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 15 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 16 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 17 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 18 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 19 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 20 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 21 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 22 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 23 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 24 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 25 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 26 | train_loss: 0.0233 | test_loss: 0.0233 | \n",
            "Epoch: 27 | train_loss: 0.0233 | test_loss: 0.0232 | \n",
            "Epoch: 28 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 29 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 30 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 31 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 32 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 33 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 34 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 35 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 36 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 37 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 38 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 39 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 40 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 41 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 42 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 43 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 44 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 45 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 46 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 47 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 48 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 49 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 50 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 51 | train_loss: 0.0232 | test_loss: 0.0232 | \n",
            "Epoch: 52 | train_loss: 0.0232 | test_loss: 0.0231 | \n",
            "Epoch: 53 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 54 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 55 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 56 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 57 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 58 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 59 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 60 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 61 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 62 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 63 | train_loss: 0.0231 | test_loss: 0.0231 | \n",
            "Epoch: 64 | train_loss: 0.0231 | test_loss: 0.0230 | \n",
            "Epoch: 65 | train_loss: 0.0230 | test_loss: 0.0230 | \n",
            "Epoch: 66 | train_loss: 0.0230 | test_loss: 0.0230 | \n",
            "Epoch: 67 | train_loss: 0.0230 | test_loss: 0.0230 | \n",
            "Epoch: 68 | train_loss: 0.0230 | test_loss: 0.0230 | \n",
            "Epoch: 69 | train_loss: 0.0230 | test_loss: 0.0230 | \n",
            "Epoch: 70 | train_loss: 0.0230 | test_loss: 0.0230 | \n",
            "Epoch: 71 | train_loss: 0.0230 | test_loss: 0.0230 | \n",
            "Epoch: 72 | train_loss: 0.0230 | test_loss: 0.0230 | \n",
            "Epoch: 73 | train_loss: 0.0230 | test_loss: 0.0229 | \n",
            "Epoch: 74 | train_loss: 0.0229 | test_loss: 0.0229 | \n",
            "Epoch: 75 | train_loss: 0.0229 | test_loss: 0.0229 | \n",
            "Epoch: 76 | train_loss: 0.0229 | test_loss: 0.0229 | \n",
            "Epoch: 77 | train_loss: 0.0229 | test_loss: 0.0229 | \n",
            "Epoch: 78 | train_loss: 0.0229 | test_loss: 0.0229 | \n",
            "Epoch: 79 | train_loss: 0.0229 | test_loss: 0.0228 | \n",
            "Epoch: 80 | train_loss: 0.0228 | test_loss: 0.0228 | \n",
            "Epoch: 81 | train_loss: 0.0228 | test_loss: 0.0228 | \n",
            "Epoch: 82 | train_loss: 0.0228 | test_loss: 0.0228 | \n",
            "Epoch: 83 | train_loss: 0.0228 | test_loss: 0.0228 | \n",
            "Epoch: 84 | train_loss: 0.0228 | test_loss: 0.0227 | \n",
            "Epoch: 85 | train_loss: 0.0227 | test_loss: 0.0227 | \n",
            "Epoch: 86 | train_loss: 0.0227 | test_loss: 0.0227 | \n",
            "Epoch: 87 | train_loss: 0.0227 | test_loss: 0.0227 | \n",
            "Epoch: 88 | train_loss: 0.0227 | test_loss: 0.0226 | \n",
            "Epoch: 89 | train_loss: 0.0226 | test_loss: 0.0226 | \n",
            "Epoch: 90 | train_loss: 0.0226 | test_loss: 0.0226 | \n",
            "Epoch: 91 | train_loss: 0.0226 | test_loss: 0.0226 | \n",
            "Epoch: 92 | train_loss: 0.0226 | test_loss: 0.0225 | \n",
            "Epoch: 93 | train_loss: 0.0225 | test_loss: 0.0225 | \n",
            "Epoch: 94 | train_loss: 0.0225 | test_loss: 0.0225 | \n",
            "Epoch: 95 | train_loss: 0.0225 | test_loss: 0.0224 | \n",
            "Epoch: 96 | train_loss: 0.0224 | test_loss: 0.0224 | \n",
            "Epoch: 97 | train_loss: 0.0224 | test_loss: 0.0223 | \n",
            "Epoch: 98 | train_loss: 0.0223 | test_loss: 0.0223 | \n",
            "Epoch: 99 | train_loss: 0.0223 | test_loss: 0.0223 | \n",
            "Epoch: 100 | train_loss: 0.0223 | test_loss: 0.0222 | \n",
            "Epoch: 101 | train_loss: 0.0222 | test_loss: 0.0222 | \n",
            "Epoch: 102 | train_loss: 0.0222 | test_loss: 0.0221 | \n",
            "Epoch: 103 | train_loss: 0.0221 | test_loss: 0.0221 | \n",
            "Epoch: 104 | train_loss: 0.0221 | test_loss: 0.0220 | \n",
            "Epoch: 105 | train_loss: 0.0220 | test_loss: 0.0220 | \n",
            "Epoch: 106 | train_loss: 0.0220 | test_loss: 0.0219 | \n",
            "Epoch: 107 | train_loss: 0.0219 | test_loss: 0.0219 | \n",
            "Epoch: 108 | train_loss: 0.0219 | test_loss: 0.0218 | \n",
            "Epoch: 109 | train_loss: 0.0218 | test_loss: 0.0218 | \n",
            "Epoch: 110 | train_loss: 0.0218 | test_loss: 0.0217 | \n",
            "Epoch: 111 | train_loss: 0.0217 | test_loss: 0.0216 | \n",
            "Epoch: 112 | train_loss: 0.0216 | test_loss: 0.0216 | \n",
            "Epoch: 113 | train_loss: 0.0216 | test_loss: 0.0215 | \n",
            "Epoch: 114 | train_loss: 0.0215 | test_loss: 0.0214 | \n",
            "Epoch: 115 | train_loss: 0.0214 | test_loss: 0.0213 | \n",
            "Epoch: 116 | train_loss: 0.0213 | test_loss: 0.0213 | \n",
            "Epoch: 117 | train_loss: 0.0213 | test_loss: 0.0212 | \n",
            "Epoch: 118 | train_loss: 0.0212 | test_loss: 0.0211 | \n",
            "Epoch: 119 | train_loss: 0.0211 | test_loss: 0.0210 | \n",
            "Epoch: 120 | train_loss: 0.0210 | test_loss: 0.0209 | \n",
            "Epoch: 121 | train_loss: 0.0209 | test_loss: 0.0209 | \n",
            "Epoch: 122 | train_loss: 0.0209 | test_loss: 0.0208 | \n",
            "Epoch: 123 | train_loss: 0.0208 | test_loss: 0.0207 | \n",
            "Epoch: 124 | train_loss: 0.0207 | test_loss: 0.0206 | \n",
            "Epoch: 125 | train_loss: 0.0206 | test_loss: 0.0205 | \n",
            "Epoch: 126 | train_loss: 0.0205 | test_loss: 0.0204 | \n",
            "Epoch: 127 | train_loss: 0.0204 | test_loss: 0.0203 | \n",
            "Epoch: 128 | train_loss: 0.0203 | test_loss: 0.0202 | \n",
            "Epoch: 129 | train_loss: 0.0202 | test_loss: 0.0201 | \n",
            "Epoch: 130 | train_loss: 0.0201 | test_loss: 0.0200 | \n",
            "Epoch: 131 | train_loss: 0.0200 | test_loss: 0.0199 | \n",
            "Epoch: 132 | train_loss: 0.0199 | test_loss: 0.0198 | \n",
            "Epoch: 133 | train_loss: 0.0198 | test_loss: 0.0197 | \n",
            "Epoch: 134 | train_loss: 0.0197 | test_loss: 0.0196 | \n",
            "Epoch: 135 | train_loss: 0.0196 | test_loss: 0.0195 | \n",
            "Epoch: 136 | train_loss: 0.0195 | test_loss: 0.0194 | \n",
            "Epoch: 137 | train_loss: 0.0194 | test_loss: 0.0193 | \n",
            "Epoch: 138 | train_loss: 0.0193 | test_loss: 0.0191 | \n",
            "Epoch: 139 | train_loss: 0.0191 | test_loss: 0.0190 | \n",
            "Epoch: 140 | train_loss: 0.0190 | test_loss: 0.0189 | \n",
            "Epoch: 141 | train_loss: 0.0189 | test_loss: 0.0188 | \n",
            "Epoch: 142 | train_loss: 0.0188 | test_loss: 0.0187 | \n",
            "Epoch: 143 | train_loss: 0.0187 | test_loss: 0.0186 | \n",
            "Epoch: 144 | train_loss: 0.0186 | test_loss: 0.0185 | \n",
            "Epoch: 145 | train_loss: 0.0185 | test_loss: 0.0184 | \n",
            "Epoch: 146 | train_loss: 0.0184 | test_loss: 0.0182 | \n",
            "Epoch: 147 | train_loss: 0.0182 | test_loss: 0.0181 | \n",
            "Epoch: 148 | train_loss: 0.0181 | test_loss: 0.0180 | \n",
            "Epoch: 149 | train_loss: 0.0180 | test_loss: 0.0179 | \n",
            "Epoch: 150 | train_loss: 0.0179 | test_loss: 0.0178 | \n",
            "Epoch: 151 | train_loss: 0.0178 | test_loss: 0.0177 | \n",
            "Epoch: 152 | train_loss: 0.0177 | test_loss: 0.0176 | \n",
            "Epoch: 153 | train_loss: 0.0176 | test_loss: 0.0174 | \n",
            "Epoch: 154 | train_loss: 0.0174 | test_loss: 0.0173 | \n",
            "Epoch: 155 | train_loss: 0.0173 | test_loss: 0.0172 | \n",
            "Epoch: 156 | train_loss: 0.0172 | test_loss: 0.0171 | \n",
            "Epoch: 157 | train_loss: 0.0171 | test_loss: 0.0170 | \n",
            "Epoch: 158 | train_loss: 0.0170 | test_loss: 0.0169 | \n",
            "Epoch: 159 | train_loss: 0.0169 | test_loss: 0.0167 | \n",
            "Epoch: 160 | train_loss: 0.0167 | test_loss: 0.0166 | \n",
            "Epoch: 161 | train_loss: 0.0166 | test_loss: 0.0165 | \n",
            "Epoch: 162 | train_loss: 0.0165 | test_loss: 0.0164 | \n",
            "Epoch: 163 | train_loss: 0.0164 | test_loss: 0.0163 | \n",
            "Epoch: 164 | train_loss: 0.0163 | test_loss: 0.0162 | \n",
            "Epoch: 165 | train_loss: 0.0162 | test_loss: 0.0160 | \n",
            "Epoch: 166 | train_loss: 0.0160 | test_loss: 0.0159 | \n",
            "Epoch: 167 | train_loss: 0.0159 | test_loss: 0.0158 | \n",
            "Epoch: 168 | train_loss: 0.0158 | test_loss: 0.0157 | \n",
            "Epoch: 169 | train_loss: 0.0157 | test_loss: 0.0156 | \n",
            "Epoch: 170 | train_loss: 0.0156 | test_loss: 0.0155 | \n",
            "Epoch: 171 | train_loss: 0.0155 | test_loss: 0.0153 | \n",
            "Epoch: 172 | train_loss: 0.0153 | test_loss: 0.0152 | \n",
            "Epoch: 173 | train_loss: 0.0152 | test_loss: 0.0151 | \n",
            "Epoch: 174 | train_loss: 0.0151 | test_loss: 0.0150 | \n",
            "Epoch: 175 | train_loss: 0.0150 | test_loss: 0.0149 | \n",
            "Epoch: 176 | train_loss: 0.0149 | test_loss: 0.0148 | \n",
            "Epoch: 177 | train_loss: 0.0148 | test_loss: 0.0147 | \n",
            "Epoch: 178 | train_loss: 0.0147 | test_loss: 0.0145 | \n",
            "Epoch: 179 | train_loss: 0.0145 | test_loss: 0.0144 | \n",
            "Epoch: 180 | train_loss: 0.0144 | test_loss: 0.0143 | \n",
            "Epoch: 181 | train_loss: 0.0143 | test_loss: 0.0142 | \n",
            "Epoch: 182 | train_loss: 0.0142 | test_loss: 0.0141 | \n",
            "Epoch: 183 | train_loss: 0.0141 | test_loss: 0.0140 | \n",
            "Epoch: 184 | train_loss: 0.0140 | test_loss: 0.0139 | \n",
            "Epoch: 185 | train_loss: 0.0139 | test_loss: 0.0137 | \n",
            "Epoch: 186 | train_loss: 0.0137 | test_loss: 0.0136 | \n",
            "Epoch: 187 | train_loss: 0.0136 | test_loss: 0.0135 | \n",
            "Epoch: 188 | train_loss: 0.0135 | test_loss: 0.0134 | \n",
            "Epoch: 189 | train_loss: 0.0134 | test_loss: 0.0133 | \n",
            "Epoch: 190 | train_loss: 0.0133 | test_loss: 0.0132 | \n",
            "Epoch: 191 | train_loss: 0.0132 | test_loss: 0.0131 | \n",
            "Epoch: 192 | train_loss: 0.0131 | test_loss: 0.0130 | \n",
            "Epoch: 193 | train_loss: 0.0130 | test_loss: 0.0129 | \n",
            "Epoch: 194 | train_loss: 0.0129 | test_loss: 0.0128 | \n",
            "Epoch: 195 | train_loss: 0.0128 | test_loss: 0.0126 | \n",
            "Epoch: 196 | train_loss: 0.0126 | test_loss: 0.0125 | \n",
            "Epoch: 197 | train_loss: 0.0125 | test_loss: 0.0124 | \n",
            "Epoch: 198 | train_loss: 0.0124 | test_loss: 0.0123 | \n",
            "Epoch: 199 | train_loss: 0.0123 | test_loss: 0.0122 | \n",
            "Epoch: 200 | train_loss: 0.0122 | test_loss: 0.0121 | \n",
            "Epoch: 201 | train_loss: 0.0121 | test_loss: 0.0120 | \n",
            "Epoch: 202 | train_loss: 0.0120 | test_loss: 0.0119 | \n",
            "Epoch: 203 | train_loss: 0.0119 | test_loss: 0.0118 | \n",
            "Epoch: 204 | train_loss: 0.0118 | test_loss: 0.0117 | \n",
            "Epoch: 205 | train_loss: 0.0117 | test_loss: 0.0116 | \n",
            "Epoch: 206 | train_loss: 0.0116 | test_loss: 0.0115 | \n",
            "Epoch: 207 | train_loss: 0.0115 | test_loss: 0.0114 | \n",
            "Epoch: 208 | train_loss: 0.0114 | test_loss: 0.0113 | \n",
            "Epoch: 209 | train_loss: 0.0113 | test_loss: 0.0112 | \n",
            "Epoch: 210 | train_loss: 0.0112 | test_loss: 0.0111 | \n",
            "Epoch: 211 | train_loss: 0.0111 | test_loss: 0.0110 | \n",
            "Epoch: 212 | train_loss: 0.0110 | test_loss: 0.0109 | \n",
            "Epoch: 213 | train_loss: 0.0109 | test_loss: 0.0108 | \n",
            "Epoch: 214 | train_loss: 0.0108 | test_loss: 0.0107 | \n",
            "Epoch: 215 | train_loss: 0.0107 | test_loss: 0.0106 | \n",
            "Epoch: 216 | train_loss: 0.0106 | test_loss: 0.0105 | \n",
            "Epoch: 217 | train_loss: 0.0105 | test_loss: 0.0104 | \n",
            "Epoch: 218 | train_loss: 0.0104 | test_loss: 0.0103 | \n",
            "Epoch: 219 | train_loss: 0.0103 | test_loss: 0.0102 | \n",
            "Epoch: 220 | train_loss: 0.0102 | test_loss: 0.0101 | \n",
            "Epoch: 221 | train_loss: 0.0101 | test_loss: 0.0100 | \n",
            "Epoch: 222 | train_loss: 0.0100 | test_loss: 0.0099 | \n",
            "Epoch: 223 | train_loss: 0.0099 | test_loss: 0.0098 | \n",
            "Epoch: 224 | train_loss: 0.0098 | test_loss: 0.0097 | \n",
            "Epoch: 225 | train_loss: 0.0097 | test_loss: 0.0096 | \n",
            "Epoch: 226 | train_loss: 0.0096 | test_loss: 0.0095 | \n",
            "Epoch: 227 | train_loss: 0.0095 | test_loss: 0.0094 | \n",
            "Epoch: 228 | train_loss: 0.0094 | test_loss: 0.0093 | \n",
            "Epoch: 229 | train_loss: 0.0093 | test_loss: 0.0093 | \n",
            "Epoch: 230 | train_loss: 0.0093 | test_loss: 0.0092 | \n",
            "Epoch: 231 | train_loss: 0.0092 | test_loss: 0.0091 | \n",
            "Epoch: 232 | train_loss: 0.0091 | test_loss: 0.0090 | \n",
            "Epoch: 233 | train_loss: 0.0090 | test_loss: 0.0089 | \n",
            "Epoch: 234 | train_loss: 0.0089 | test_loss: 0.0088 | \n",
            "Epoch: 235 | train_loss: 0.0088 | test_loss: 0.0087 | \n",
            "Epoch: 236 | train_loss: 0.0087 | test_loss: 0.0086 | \n",
            "Epoch: 237 | train_loss: 0.0086 | test_loss: 0.0086 | \n",
            "Epoch: 238 | train_loss: 0.0086 | test_loss: 0.0085 | \n",
            "Epoch: 239 | train_loss: 0.0085 | test_loss: 0.0084 | \n",
            "Epoch: 240 | train_loss: 0.0084 | test_loss: 0.0083 | \n",
            "Epoch: 241 | train_loss: 0.0083 | test_loss: 0.0082 | \n",
            "Epoch: 242 | train_loss: 0.0082 | test_loss: 0.0081 | \n",
            "Epoch: 243 | train_loss: 0.0081 | test_loss: 0.0081 | \n",
            "Epoch: 244 | train_loss: 0.0081 | test_loss: 0.0080 | \n",
            "Epoch: 245 | train_loss: 0.0080 | test_loss: 0.0079 | \n",
            "Epoch: 246 | train_loss: 0.0079 | test_loss: 0.0078 | \n",
            "Epoch: 247 | train_loss: 0.0078 | test_loss: 0.0077 | \n",
            "Epoch: 248 | train_loss: 0.0077 | test_loss: 0.0077 | \n",
            "Epoch: 249 | train_loss: 0.0077 | test_loss: 0.0076 | \n",
            "Epoch: 250 | train_loss: 0.0076 | test_loss: 0.0075 | \n",
            "Epoch: 251 | train_loss: 0.0075 | test_loss: 0.0074 | \n",
            "Epoch: 252 | train_loss: 0.0074 | test_loss: 0.0074 | \n",
            "Epoch: 253 | train_loss: 0.0074 | test_loss: 0.0073 | \n",
            "Epoch: 254 | train_loss: 0.0073 | test_loss: 0.0072 | \n",
            "Epoch: 255 | train_loss: 0.0072 | test_loss: 0.0071 | \n",
            "Epoch: 256 | train_loss: 0.0071 | test_loss: 0.0071 | \n",
            "Epoch: 257 | train_loss: 0.0071 | test_loss: 0.0070 | \n",
            "Epoch: 258 | train_loss: 0.0070 | test_loss: 0.0069 | \n",
            "Epoch: 259 | train_loss: 0.0069 | test_loss: 0.0069 | \n",
            "Epoch: 260 | train_loss: 0.0069 | test_loss: 0.0068 | \n",
            "Epoch: 261 | train_loss: 0.0068 | test_loss: 0.0067 | \n",
            "Epoch: 262 | train_loss: 0.0067 | test_loss: 0.0067 | \n",
            "Epoch: 263 | train_loss: 0.0067 | test_loss: 0.0066 | \n",
            "Epoch: 264 | train_loss: 0.0066 | test_loss: 0.0065 | \n",
            "Epoch: 265 | train_loss: 0.0065 | test_loss: 0.0064 | \n",
            "Epoch: 266 | train_loss: 0.0064 | test_loss: 0.0064 | \n",
            "Epoch: 267 | train_loss: 0.0064 | test_loss: 0.0063 | \n",
            "Epoch: 268 | train_loss: 0.0063 | test_loss: 0.0063 | \n",
            "Epoch: 269 | train_loss: 0.0063 | test_loss: 0.0062 | \n",
            "Epoch: 270 | train_loss: 0.0062 | test_loss: 0.0061 | \n",
            "Epoch: 271 | train_loss: 0.0061 | test_loss: 0.0061 | \n",
            "Epoch: 272 | train_loss: 0.0061 | test_loss: 0.0060 | \n",
            "Epoch: 273 | train_loss: 0.0060 | test_loss: 0.0059 | \n",
            "Epoch: 274 | train_loss: 0.0059 | test_loss: 0.0059 | \n",
            "Epoch: 275 | train_loss: 0.0059 | test_loss: 0.0058 | \n",
            "Epoch: 276 | train_loss: 0.0058 | test_loss: 0.0058 | \n",
            "Epoch: 277 | train_loss: 0.0058 | test_loss: 0.0057 | \n",
            "Epoch: 278 | train_loss: 0.0057 | test_loss: 0.0056 | \n",
            "Epoch: 279 | train_loss: 0.0056 | test_loss: 0.0056 | \n",
            "Epoch: 280 | train_loss: 0.0056 | test_loss: 0.0055 | \n",
            "Epoch: 281 | train_loss: 0.0055 | test_loss: 0.0055 | \n",
            "Epoch: 282 | train_loss: 0.0055 | test_loss: 0.0054 | \n",
            "Epoch: 283 | train_loss: 0.0054 | test_loss: 0.0053 | \n",
            "Epoch: 284 | train_loss: 0.0053 | test_loss: 0.0053 | \n",
            "Epoch: 285 | train_loss: 0.0053 | test_loss: 0.0052 | \n",
            "Epoch: 286 | train_loss: 0.0052 | test_loss: 0.0052 | \n",
            "Epoch: 287 | train_loss: 0.0052 | test_loss: 0.0051 | \n",
            "Epoch: 288 | train_loss: 0.0051 | test_loss: 0.0051 | \n",
            "Epoch: 289 | train_loss: 0.0051 | test_loss: 0.0050 | \n",
            "Epoch: 290 | train_loss: 0.0050 | test_loss: 0.0050 | \n",
            "Epoch: 291 | train_loss: 0.0050 | test_loss: 0.0049 | \n",
            "Epoch: 292 | train_loss: 0.0049 | test_loss: 0.0049 | \n",
            "Epoch: 293 | train_loss: 0.0049 | test_loss: 0.0048 | \n",
            "Epoch: 294 | train_loss: 0.0048 | test_loss: 0.0048 | \n",
            "Epoch: 295 | train_loss: 0.0048 | test_loss: 0.0047 | \n",
            "Epoch: 296 | train_loss: 0.0047 | test_loss: 0.0047 | \n",
            "Epoch: 297 | train_loss: 0.0047 | test_loss: 0.0046 | \n",
            "Epoch: 298 | train_loss: 0.0046 | test_loss: 0.0046 | \n",
            "Epoch: 299 | train_loss: 0.0046 | test_loss: 0.0045 | \n",
            "Epoch: 300 | train_loss: 0.0045 | test_loss: 0.0045 | \n",
            "Epoch: 301 | train_loss: 0.0045 | test_loss: 0.0044 | \n",
            "Epoch: 302 | train_loss: 0.0044 | test_loss: 0.0044 | \n",
            "Epoch: 303 | train_loss: 0.0044 | test_loss: 0.0043 | \n",
            "Epoch: 304 | train_loss: 0.0043 | test_loss: 0.0043 | \n",
            "Epoch: 305 | train_loss: 0.0043 | test_loss: 0.0042 | \n",
            "Epoch: 306 | train_loss: 0.0042 | test_loss: 0.0042 | \n",
            "Epoch: 307 | train_loss: 0.0042 | test_loss: 0.0042 | \n",
            "Epoch: 308 | train_loss: 0.0042 | test_loss: 0.0041 | \n",
            "Epoch: 309 | train_loss: 0.0041 | test_loss: 0.0041 | \n",
            "Epoch: 310 | train_loss: 0.0041 | test_loss: 0.0040 | \n",
            "Epoch: 311 | train_loss: 0.0040 | test_loss: 0.0040 | \n",
            "Epoch: 312 | train_loss: 0.0040 | test_loss: 0.0039 | \n",
            "Epoch: 313 | train_loss: 0.0039 | test_loss: 0.0039 | \n",
            "Epoch: 314 | train_loss: 0.0039 | test_loss: 0.0039 | \n",
            "Epoch: 315 | train_loss: 0.0039 | test_loss: 0.0038 | \n",
            "Epoch: 316 | train_loss: 0.0038 | test_loss: 0.0038 | \n",
            "Epoch: 317 | train_loss: 0.0038 | test_loss: 0.0037 | \n",
            "Epoch: 318 | train_loss: 0.0037 | test_loss: 0.0037 | \n",
            "Epoch: 319 | train_loss: 0.0037 | test_loss: 0.0037 | \n",
            "Epoch: 320 | train_loss: 0.0037 | test_loss: 0.0036 | \n",
            "Epoch: 321 | train_loss: 0.0036 | test_loss: 0.0036 | \n",
            "Epoch: 322 | train_loss: 0.0036 | test_loss: 0.0035 | \n",
            "Epoch: 323 | train_loss: 0.0035 | test_loss: 0.0035 | \n",
            "Epoch: 324 | train_loss: 0.0035 | test_loss: 0.0035 | \n",
            "Epoch: 325 | train_loss: 0.0035 | test_loss: 0.0034 | \n",
            "Epoch: 326 | train_loss: 0.0034 | test_loss: 0.0034 | \n",
            "Epoch: 327 | train_loss: 0.0034 | test_loss: 0.0034 | \n",
            "Epoch: 328 | train_loss: 0.0034 | test_loss: 0.0033 | \n",
            "Epoch: 329 | train_loss: 0.0033 | test_loss: 0.0033 | \n",
            "Epoch: 330 | train_loss: 0.0033 | test_loss: 0.0033 | \n",
            "Epoch: 331 | train_loss: 0.0033 | test_loss: 0.0032 | \n",
            "Epoch: 332 | train_loss: 0.0032 | test_loss: 0.0032 | \n",
            "Epoch: 333 | train_loss: 0.0032 | test_loss: 0.0032 | \n",
            "Epoch: 334 | train_loss: 0.0032 | test_loss: 0.0031 | \n",
            "Epoch: 335 | train_loss: 0.0031 | test_loss: 0.0031 | \n",
            "Epoch: 336 | train_loss: 0.0031 | test_loss: 0.0031 | \n",
            "Epoch: 337 | train_loss: 0.0031 | test_loss: 0.0030 | \n",
            "Epoch: 338 | train_loss: 0.0030 | test_loss: 0.0030 | \n",
            "Epoch: 339 | train_loss: 0.0030 | test_loss: 0.0030 | \n",
            "Epoch: 340 | train_loss: 0.0030 | test_loss: 0.0029 | \n",
            "Epoch: 341 | train_loss: 0.0029 | test_loss: 0.0029 | \n",
            "Epoch: 342 | train_loss: 0.0029 | test_loss: 0.0029 | \n",
            "Epoch: 343 | train_loss: 0.0029 | test_loss: 0.0028 | \n",
            "Epoch: 344 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 345 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 346 | train_loss: 0.0028 | test_loss: 0.0028 | \n",
            "Epoch: 347 | train_loss: 0.0028 | test_loss: 0.0027 | \n",
            "Epoch: 348 | train_loss: 0.0027 | test_loss: 0.0027 | \n",
            "Epoch: 349 | train_loss: 0.0027 | test_loss: 0.0027 | \n",
            "Epoch: 350 | train_loss: 0.0027 | test_loss: 0.0026 | \n",
            "Epoch: 351 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 352 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 353 | train_loss: 0.0026 | test_loss: 0.0026 | \n",
            "Epoch: 354 | train_loss: 0.0026 | test_loss: 0.0025 | \n",
            "Epoch: 355 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 356 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 357 | train_loss: 0.0025 | test_loss: 0.0025 | \n",
            "Epoch: 358 | train_loss: 0.0025 | test_loss: 0.0024 | \n",
            "Epoch: 359 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 360 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 361 | train_loss: 0.0024 | test_loss: 0.0024 | \n",
            "Epoch: 362 | train_loss: 0.0024 | test_loss: 0.0023 | \n",
            "Epoch: 363 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 364 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 365 | train_loss: 0.0023 | test_loss: 0.0023 | \n",
            "Epoch: 366 | train_loss: 0.0023 | test_loss: 0.0022 | \n",
            "Epoch: 367 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 368 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 369 | train_loss: 0.0022 | test_loss: 0.0022 | \n",
            "Epoch: 370 | train_loss: 0.0022 | test_loss: 0.0021 | \n",
            "Epoch: 371 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 372 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 373 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 374 | train_loss: 0.0021 | test_loss: 0.0021 | \n",
            "Epoch: 375 | train_loss: 0.0021 | test_loss: 0.0020 | \n",
            "Epoch: 376 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 377 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 378 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 379 | train_loss: 0.0020 | test_loss: 0.0020 | \n",
            "Epoch: 380 | train_loss: 0.0020 | test_loss: 0.0019 | \n",
            "Epoch: 381 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 382 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 383 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 384 | train_loss: 0.0019 | test_loss: 0.0019 | \n",
            "Epoch: 385 | train_loss: 0.0019 | test_loss: 0.0018 | \n",
            "Epoch: 386 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 387 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 388 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 389 | train_loss: 0.0018 | test_loss: 0.0018 | \n",
            "Epoch: 390 | train_loss: 0.0018 | test_loss: 0.0017 | \n",
            "Epoch: 391 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 392 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 393 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 394 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 395 | train_loss: 0.0017 | test_loss: 0.0017 | \n",
            "Epoch: 396 | train_loss: 0.0017 | test_loss: 0.0016 | \n",
            "Epoch: 397 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 398 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 399 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 400 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 401 | train_loss: 0.0016 | test_loss: 0.0016 | \n",
            "Epoch: 402 | train_loss: 0.0016 | test_loss: 0.0015 | \n",
            "Epoch: 403 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 404 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 405 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 406 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 407 | train_loss: 0.0015 | test_loss: 0.0015 | \n",
            "Epoch: 408 | train_loss: 0.0015 | test_loss: 0.0014 | \n",
            "Epoch: 409 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 410 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 411 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 412 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 413 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 414 | train_loss: 0.0014 | test_loss: 0.0014 | \n",
            "Epoch: 415 | train_loss: 0.0014 | test_loss: 0.0013 | \n",
            "Epoch: 416 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 417 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 418 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 419 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 420 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 421 | train_loss: 0.0013 | test_loss: 0.0013 | \n",
            "Epoch: 422 | train_loss: 0.0013 | test_loss: 0.0012 | \n",
            "Epoch: 423 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 424 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 425 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 426 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 427 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 428 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 429 | train_loss: 0.0012 | test_loss: 0.0012 | \n",
            "Epoch: 430 | train_loss: 0.0012 | test_loss: 0.0011 | \n",
            "Epoch: 431 | train_loss: 0.0011 | test_loss: 0.0011 | \n",
            "Epoch: 432 | train_loss: 0.0011 | test_loss: 0.0011 | \n",
            "Epoch: 433 | train_loss: 0.0011 | test_loss: 0.0011 | \n",
            "Epoch: 434 | train_loss: 0.0011 | test_loss: 0.0011 | \n",
            "Epoch: 435 | train_loss: 0.0011 | test_loss: 0.0011 | \n",
            "Epoch: 436 | train_loss: 0.0011 | test_loss: 0.0011 | \n",
            "Epoch: 437 | train_loss: 0.0011 | test_loss: 0.0011 | \n",
            "Epoch: 438 | train_loss: 0.0011 | test_loss: 0.0010 | \n",
            "Epoch: 439 | train_loss: 0.0010 | test_loss: 0.0010 | \n",
            "Epoch: 440 | train_loss: 0.0010 | test_loss: 0.0010 | \n",
            "Epoch: 441 | train_loss: 0.0010 | test_loss: 0.0010 | \n",
            "Epoch: 442 | train_loss: 0.0010 | test_loss: 0.0010 | \n",
            "Epoch: 443 | train_loss: 0.0010 | test_loss: 0.0010 | \n",
            "Epoch: 444 | train_loss: 0.0010 | test_loss: 0.0010 | \n",
            "Epoch: 445 | train_loss: 0.0010 | test_loss: 0.0010 | \n",
            "Epoch: 446 | train_loss: 0.0010 | test_loss: 0.0010 | \n",
            "Epoch: 447 | train_loss: 0.0010 | test_loss: 0.0009 | \n",
            "Epoch: 448 | train_loss: 0.0009 | test_loss: 0.0009 | \n",
            "Epoch: 449 | train_loss: 0.0009 | test_loss: 0.0009 | \n",
            "Epoch: 450 | train_loss: 0.0009 | test_loss: 0.0009 | \n",
            "Epoch: 451 | train_loss: 0.0009 | test_loss: 0.0009 | \n",
            "Epoch: 452 | train_loss: 0.0009 | test_loss: 0.0009 | \n",
            "Epoch: 453 | train_loss: 0.0009 | test_loss: 0.0009 | \n",
            "Epoch: 454 | train_loss: 0.0009 | test_loss: 0.0009 | \n",
            "Epoch: 455 | train_loss: 0.0009 | test_loss: 0.0009 | \n",
            "Epoch: 456 | train_loss: 0.0009 | test_loss: 0.0009 | \n",
            "Epoch: 457 | train_loss: 0.0009 | test_loss: 0.0008 | \n",
            "Epoch: 458 | train_loss: 0.0008 | test_loss: 0.0008 | \n",
            "Epoch: 459 | train_loss: 0.0008 | test_loss: 0.0008 | \n",
            "Epoch: 460 | train_loss: 0.0008 | test_loss: 0.0008 | \n",
            "Epoch: 461 | train_loss: 0.0008 | test_loss: 0.0008 | \n",
            "Epoch: 462 | train_loss: 0.0008 | test_loss: 0.0008 | \n",
            "Epoch: 463 | train_loss: 0.0008 | test_loss: 0.0008 | \n",
            "Epoch: 464 | train_loss: 0.0008 | test_loss: 0.0008 | \n",
            "Epoch: 465 | train_loss: 0.0008 | test_loss: 0.0008 | \n",
            "Epoch: 466 | train_loss: 0.0008 | test_loss: 0.0008 | \n",
            "Epoch: 467 | train_loss: 0.0008 | test_loss: 0.0008 | \n",
            "Epoch: 468 | train_loss: 0.0008 | test_loss: 0.0007 | \n",
            "Epoch: 469 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 470 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 471 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 472 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 473 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 474 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 475 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 476 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 477 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 478 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 479 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 480 | train_loss: 0.0007 | test_loss: 0.0007 | \n",
            "Epoch: 481 | train_loss: 0.0007 | test_loss: 0.0006 | \n",
            "Epoch: 482 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 483 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 484 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 485 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 486 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 487 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 488 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 489 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 490 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 491 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 492 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 493 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 494 | train_loss: 0.0006 | test_loss: 0.0006 | \n",
            "Epoch: 495 | train_loss: 0.0006 | test_loss: 0.0005 | \n",
            "Epoch: 496 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 497 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 498 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 499 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 500 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 501 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 502 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 503 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 504 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 505 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 506 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 507 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 508 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 509 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 510 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 511 | train_loss: 0.0005 | test_loss: 0.0005 | \n",
            "Epoch: 512 | train_loss: 0.0005 | test_loss: 0.0004 | \n",
            "Epoch: 513 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 514 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 515 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 516 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 517 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 518 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 519 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 520 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 521 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 522 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 523 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 524 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 525 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 526 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 527 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 528 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 529 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 530 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 531 | train_loss: 0.0004 | test_loss: 0.0004 | \n",
            "Epoch: 532 | train_loss: 0.0004 | test_loss: 0.0003 | \n",
            "Epoch: 533 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 534 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 535 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 536 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 537 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 538 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 539 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 540 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 541 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 542 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 543 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 544 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 545 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 546 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 547 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 548 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 549 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 550 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 551 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 552 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 553 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 554 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 555 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 556 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 557 | train_loss: 0.0003 | test_loss: 0.0003 | \n",
            "Epoch: 558 | train_loss: 0.0003 | test_loss: 0.0002 | \n",
            "Epoch: 559 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 560 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 561 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 562 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 563 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 564 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 565 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 566 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 567 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 568 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 569 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 570 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 571 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 572 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 573 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 574 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 575 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 576 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 577 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 578 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 579 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 580 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 581 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 582 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 583 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 584 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 585 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 586 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 587 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 588 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 589 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 590 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 591 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 592 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 593 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 594 | train_loss: 0.0002 | test_loss: 0.0002 | \n",
            "Epoch: 595 | train_loss: 0.0002 | test_loss: 0.0001 | \n",
            "Epoch: 596 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 597 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 598 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 599 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 600 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 601 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 602 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 603 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 604 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 605 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 606 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 607 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 608 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 609 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 610 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 611 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 612 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 613 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 614 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 615 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 616 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 617 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 618 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 619 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 620 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 621 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 622 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 623 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 624 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 625 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 626 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 627 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 628 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 629 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 630 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 631 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 632 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 633 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 634 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 635 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 636 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 637 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 638 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 639 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 640 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 641 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 642 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 643 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 644 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 645 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 646 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 647 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 648 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 649 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 650 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 651 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 652 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 653 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 654 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 655 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 656 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 657 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 658 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 659 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 660 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 661 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 662 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 663 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 664 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 665 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 666 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 667 | train_loss: 0.0001 | test_loss: 0.0001 | \n",
            "Epoch: 668 | train_loss: 0.0001 | test_loss: 0.0000 | \n",
            "Epoch: 669 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 670 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 671 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 672 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 673 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 674 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 675 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 676 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 677 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 678 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 679 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 680 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 681 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 682 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 683 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 684 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 685 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 686 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 687 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 688 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 689 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 690 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 691 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 692 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 693 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 694 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 695 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 696 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 697 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 698 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 699 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 700 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 701 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 702 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 703 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 704 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 705 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 706 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 707 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 708 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 709 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 710 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 711 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 712 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 713 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 714 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 715 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 716 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 717 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 718 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 719 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 720 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 721 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 722 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 723 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 724 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 725 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 726 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 727 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 728 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 729 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 730 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 731 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 732 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 733 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 734 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 735 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 736 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 737 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 738 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 739 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 740 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 741 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 742 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 743 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 744 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 745 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 746 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 747 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 748 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 749 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 750 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 751 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 752 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 753 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 754 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 755 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 756 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 757 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 758 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 759 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 760 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 761 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 762 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 763 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 764 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 765 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 766 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 767 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 768 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 769 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 770 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 771 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 772 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 773 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 774 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 775 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 776 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 777 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 778 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 779 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 780 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 781 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 782 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 783 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 784 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 785 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 786 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 787 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 788 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 789 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 790 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 791 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 792 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 793 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 794 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 795 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 796 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 797 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 798 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 799 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 800 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 801 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 802 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 803 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 804 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 805 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 806 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 807 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 808 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 809 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 810 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 811 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 812 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 813 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 814 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 815 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 816 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 817 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 818 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 819 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 820 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 821 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 822 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 823 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 824 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 825 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 826 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 827 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 828 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 829 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 830 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 831 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 832 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 833 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 834 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 835 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 836 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 837 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 838 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 839 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 840 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 841 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 842 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 843 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 844 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 845 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 846 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 847 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 848 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 849 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 850 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 851 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 852 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 853 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 854 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 855 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 856 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 857 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 858 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 859 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 860 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 861 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 862 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 863 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 864 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 865 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 866 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 867 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 868 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 869 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 870 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 871 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 872 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 873 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 874 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 875 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 876 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 877 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 878 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 879 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 880 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 881 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 882 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 883 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 884 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 885 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 886 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 887 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 888 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 889 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 890 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 891 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 892 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 893 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 894 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 895 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 896 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 897 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 898 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 899 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 900 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 901 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 902 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 903 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 904 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 905 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 906 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 907 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 908 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 909 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 910 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 911 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 912 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 913 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 914 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 915 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 916 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 917 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 918 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 919 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 920 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 921 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 922 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 923 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 924 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 925 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 926 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 927 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 928 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 929 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 930 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 931 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 932 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 933 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 934 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 935 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 936 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 937 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 938 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 939 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 940 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 941 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 942 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 943 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 944 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 945 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 946 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 947 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 948 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 949 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 950 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 951 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 952 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 953 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 954 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 955 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 956 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 957 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 958 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 959 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 960 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 961 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 962 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 963 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 964 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 965 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 966 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 967 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 968 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 969 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 970 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 971 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 972 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 973 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 974 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 975 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 976 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 977 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 978 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 979 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 980 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 981 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 982 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 983 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 984 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 985 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 986 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 987 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 988 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 989 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 990 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 991 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 992 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 993 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 994 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 995 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 996 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 997 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 998 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 999 | train_loss: 0.0000 | test_loss: 0.0000 | \n",
            "Epoch: 1000 | train_loss: 0.0000 | test_loss: 0.0000 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([i+1 for i in range(1000)],model_resulrs[\"train_loss\"])\n",
        "plt.title(\"train_loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "-BQ6aiO7cY9H",
        "outputId": "5ca37a50-0e9c-4ca7-a78a-ddbd46848731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_hat=model(y)"
      ],
      "metadata": {
        "id": "ARjKs32cc3mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_hat=x_hat.detach().numpy()"
      ],
      "metadata": {
        "id": "cZ9j5aHpdoVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([i for i in range(n)],x)\n",
        "plt.scatter([i for i in range(n)], x_hat,color='r')\n",
        "plt.legend([\"x\", \"x_hat\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Syh6ysY5dpZ_",
        "outputId": "36162cc4-87b3-4ebe-8cee-d0e5c01bd343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfW0lEQVR4nO3df3xddZ3n8dcn6Y80tVBtCxRCkiIolB92NC2izo4/EAGxzIzVAeIPHotERR7q7uxi2bK4g9vHyuzjoQwLMmaUQcc8RKzDUKEzWCiurqvYIIW2FLCwTUlBG8KvljT9lc/+cU6Se5ObNr3n3PPj3vfz8cjjnvO9J/d8z03yvt98z/d8j7k7IiJS/erSroCIiCRDgS8iUiMU+CIiNUKBLyJSIxT4IiI1YkraFTiUuXPnemtra9rVEBHJjUceeeRFd59X6rlMB35rayvd3d1pV0NEJDfMrGei59SlIyJSIxT4IiI1QoEvIlIjMt2HLyJyOPv376e3t5fBwcG0q5KohoYGmpqamDp16qS/J5bAN7PbgYuAne5+Ronn3wvcA/y/sOif3f2GOPYtIrWtt7eXWbNm0draipmlXZ1EuDv9/f309vayYMGCSX9fXF06dwDnH2abX7r7ovBLYS/x6uqC1laoqwseu7rSrpEkZHBwkDlz5tRM2AOYGXPmzDni/2piaeG7+y/MrDWO1xI5Yl1d0NEBAwPBek9PsA7Q3p5evSQxtRT2w8o55iRP2p5jZo+Z2b+a2ekJ7leq3YoVMDDA03Ob2TD/LUHZwEBQLiIjkjpp+zugxd13m9mFwL8Ap5Ta0Mw6gA6A5ubmhKonubZ9OwDnXfEtALbdeFFRuYgEEmnhu/tr7r47XF4DTDWzuRNs2+nube7eNm9eyauDRYpN1DBQg0GkSCKBb2bHWdjhZGZLwv32J7FvqQErV0JjY3FZY2NQLlJh69ev56yzzmJwcJDXX3+d008/nU2bNqVdrZLiGpb5Q+C9wFwz6wW+CkwFcPe/B5YBnzezA8Ae4BLXvRUlLsMnZjeG6y0tQdjrhG3N+ZufbuaJ51+L9TUXHn8UX/3IxKcdFy9ezNKlS7nuuuvYs2cPn/jEJzjjjHGj0zMhrlE6lx7m+VuAW+LYl0hJ7e2w/L5gedu2VKsitef6669n8eLFNDQ0cPPNN6ddnQnpSlsRqRqHaolXUn9/P7t372b//v0MDg4yc+bMVOpxOJpLJ0m6OEikKn32s5/la1/7Gu3t7XzlK19JuzoTUgs/Kbo4SKQqff/732fq1KlcdtllHDx4kHe9612sW7eO97///WlXbRy18JMSXhz0Lwvfy5Z5rUGZLg4Syb1PfepT/OQnPwGgvr6ehx9+OJNhD2rhJye8COjLH/lPgC4OEpHkqYWfFF0cJCIpU+AnRRcHiUjK1KWTFF0cJCIpUws/SYXhvm2bwl5EEqXAFxGpEQp8EZEaocAXEUnAz3/+cy666KIj+p477riD559/PrY6KPBFpLbkaIoTBX6OaUZokZQNT3HS0wPuo1OcRAj9I5kPf/fu3SxbtoxTTz2V9vb2kUy44YYbWLx4MWeccQYdHR24O6tWraK7u5v29nYWLVrEnj17yq7jMAV+gpT3IikLpzgpEnGKk8L58K+55ppDzof/6KOPctNNN/HEE0/w7LPP8qtf/QqAq6++mvXr17Np0yb27NnDvffey7Jly2hra6Orq4sNGzYwY8aMsus4TIGfoCElvki6JprKJOIUJ9dffz1r166lu7uba665ZsLtlixZQlNTE3V1dSxatIht4b0bHnroIc4++2zOPPNM1q1bx+bNmyPVZyIK/AQNKe9F0lWhKU6G58PftWsXg4ODE243ffr0keX6+noOHDjA4OAgV111FatWrWLjxo1ceeWVh3yNKBT4CVILP0E5OjEnCarQFCdR5sMfDve5c+eye/duVq1aNfLcrFmz2LVrV6S6FVLgJ0h5n5AKnJiLXB99+GRDezt0dgZTm5gFj52dka56L5wPf/ny5axfv55169ZN+vtnz57NlVdeyRlnnMGHPvQhFi9ePPLc5Zdfzuc+97nYTtpalkeOtLW1eXd3d9rViM3rew9w+lfvB2Db1z+ccm2qT2t4T9ttd34BenrYeOybOemlHczcH/573NKS/P1ux974BoIWZcSQkVFbtmzhtNNOS7saqSh17Gb2iLu3ldo+lha+md1uZjvNrORYJAvcbGZbzexxM3t7HPvNG3XpJGT7dnZNm8FHLv87vrj0mqLyxIWjQn7bdDr3vfXdQZlufCMpiWu2zDuAW4DvT/D8BcAp4dfZwG3hY03RSduENDezb+fLADx6/FuLyhMXfsh8vP1GAD6sG99UvY0bN/LJT36yqGz69Ok8/PDDKdVoVCyB7+6/MLPWQ2xyMfB9D/qPfmNms81svru/EMf+8yLL3WdVZeVK+OJfF5elde+B5ubgHEKpcomNu2NmaVcDgDPPPJMNGzZUfD/l5ElSJ21PAJ4rWO8Ny8Yxsw4z6zaz7r6+vkQql5SDauInwi+7DL7xjdGCGE7MlU03vqm4hoYG+vv7a6pB5e709/fT0NBwRN+XuRuguHsn0AnBSduUqxMr5X0yhhz8ox+F//4AzJuX/InaQrrxTcU1NTXR29tLtTUQD6ehoYGmpqYj+p6kAn8HcGLBelNYVlNibYF0dQUn/rZvD7oHFCIjhtyzdYK8vR3CEUSpfvhUqalTp7JgwYK0q5ELSXXprAY+FY7WeSfwaq3130OMLfxwqN9Qz3ZemT4z/XHmGTPkztBQ2rUQyZ64hmX+EPg18FYz6zWzK8zsc2b2uXCTNcCzwFbgH4Cr4thv3sTW6gyH+v2vd/0Vi750JztnztZQvwLucDBLLXyRjIhrlM6lh3negS/Esa88iy3wwyF997/lHAB2znwTx7z+iob6hdxhSCdMRMbR1AoJiq3RWaEJoKpF5vrwRTJCgZ+g2EJIQ/0OachdQ2BFSsjcsMxqFlsGDY/G+UU4i978+fCfb9AondCQawisSCkK/ATF2s3Q3g4v/hJeeA3uuw9OODq+1845V5eOSEnq0klQLV0JmKagha/3WmQsBX6C1M2QDFcfvkhJCvwEqdWZjCFHF16JlKDAT1ClQkgfJMXdZVnuw1e3nqRJgZ+gSoWQui+Kr3EYyvCVtvpRSZoU+AmqVOBntTWbpMJ3IJhLJ5vviX5WkiYFfoIqlUEH1V9dFKTBlbYpVuYQ9N+YpEmBnyB16VTOUFEffnbfE7XwJU0K/ARV6oSdQmRsH352T9pm9HNIaoQCP0GV69JRioxt4Wc18PWzkjQp8BNUqROJWR2RkqShMS38rAZrVk8mS21Q4CeoUn/rCpHi7rIsT62Q1XpJbVDgJ6hSffhZbc0mqfAt8Azf4lD/jUmaFPgJqlgLXyEyroWf1WDN6geR1AYFfoIqNyyzIi+bK2P78LM6hYE+nCVNcd3E/Hwze8rMtprZ8hLPX25mfWa2Ifz6TBz7zZuKBb5CZNyFV1n9EFT3m6Qp8g1QzKweuBX4INALrDez1e7+xJhNf+TuV0fdX55VKpd10rbEhVcZ/RBUC1/SFEcLfwmw1d2fdfd9wJ3AxTG8btXRlbaV4168nN0unbRrILUsjsA/AXiuYL03LBvro2b2uJmtMrMTJ3oxM+sws24z6+7r64uhetlRsQuvMhpuSRp7pW1WPwSzWi+pDUmdtP0p0OruZwFrge9NtKG7d7p7m7u3zZs3L6HqJaNis2UqREr04WfzPVGXjqQpjsDfARS22JvCshHu3u/ue8PV7wDviGG/uaMrbStnaMywzCy9JYXdS1n9IJLaEEfgrwdOMbMFZjYNuARYXbiBmc0vWF0KbIlhv7mjK20rp7gP3zP1ITh2yKhIWiKP0nH3A2Z2NXA/UA/c7u6bzewGoNvdVwNfNLOlwAHgJeDyqPvNI520rZyxLfwsvSeFddGFV5KmWPrw3X2Nu7/F3d/s7ivDsuvDsMfdr3X30939be7+Pnd/Mo795k3x0MH4AulgdrItNWOnVsjSKJ3Cn3uW/vPIvK4uaG2Furrgsasr7RrlXuQWvkze2Puu1ls8r6sunWy38MeeUJZJ6OqCjg522jSOcYeeHujoCJ5rb0+3bjmmqRUSVKk/fLUaS/Xhp1eXsYq7dDJUsSxbsYLfzT6RJVf/gHtO+7OgbGAAVqxIt145p8BPUKVO3mWpNZuWsZOnZapLp6DfXj+rSdq+nSeOOQmA3zSfWVQu5VPgJ2js5f+xva5CJNM3QBnb3SST0Nx8ZOUyKQr8BLm6dCpmbHdZloL1oPrwj9zKldjUqcVljY2wcmU69akSOmmboOJWaByvF7yIWvjZvqdt4c8nS/95ZFp7O/RPg+cBA1pagrDXCdtI1MJPUNwnbYdfQy38MSdtyVqXTuFyduqVeWcvCR6vvBK2bVPYx0CBn6CiseIxXIAzHGpZnfs9SUVDXoeyFazq0pGsUOAnKO4+/OEPEIVIiT788M3JwntT3KWTYkVyJkP/pFUNBX6CCv/w4wii0Ra+/jLGXXjl2XlvdOFVefYf0Kdj3BT4CYr7pK0Cf9TYqRWG17OQr7rwqjx7FfixU+AnKO65dEZG6WQh1VI29sKroQx9GGounfLsU+DHToGfoLgvwFELf9REF15lIWDj/s+uVuw9cBCA/VmaJyPnFPgJKvxjjyOI1MIfVfTfExR06aT/3qhLpzzDLXx17cRHgZ+gohZ+DH/4auGPGjt52lCGTtoW1qHi9amiKYX3hUOa9oUtfYlOV9omqDiUor+exuGPGjvkdfi9GZ5IzSymuajLMPYG6xUTTinMwECwnvMphffuVws/bmrhJyjuYZkahz+qqJ98aOwJ8hQqVCCxC69WrICBAb7btpSfnvqnQVmOpxQebeEr8OOiFn6C4r7EXl06o8ZPnlY8MqaO9Fr4xV06FdxROHXw1z4QtOo/8uQvi8rzZvikrVr48VELP0Gxj9LRXDojxk2eNlT6uTRUapbUcZqbOWAl/qRzOqXwcMteLfz4xBL4Zna+mT1lZlvNbHmJ56eb2Y/C5x82s9Y49lvSVVcFJ6zMMvflN9wwUk1fuDDy6w3t3QfA0I9XpX5saX/5hR8eeW+HPvMZDv7j7aPrM2amWreDf/rvRuty1Rcqt6+eHv74hjmjv2PDCz09qf98yvnau+bfANi74fHU65LKV319kGcxihz4ZlYP3ApcACwELjWzhWM2uwJ42d1PBr4J3Bh1vyVddRXcdlv6nbYTGDIrWI7+WXuwLniNgzG8Vt4V/sTdjCGrH1kffp/SUrj/StflD0eNBv4rDbMquq9K21sfzIe/r37qYbasUkNDQZ7FGPpx9OEvAba6+7MAZnYncDHwRME2FwP/LVxeBdxiZuZxD5Lu7ARgzVvfnckQfGpuy8jyQye18fTc8v/VdjM8PMbnj5o3epIuJsft6mf39BnsntZY8nnDOal/B8/MaYp1v+XadNzJI8uPzX8Lzx197Mj6mre+mxn796ZRLQCenjf6c9947Mmx/6wKPTb/LSPLPz7rXOa/9mLF9lVpO8P/Vl5tmHnE79mCl3aw7U3H4ymeu4li+oF9nLf14WClsxO+9a1YXteiZq6ZLQPOd/fPhOufBM5296sLttkUbtMbrj8TbjPut9HMOoAOgObm5nf09PQcSWUAOO0/rGLPtIZyD0lEJFVzd79M962fHC04gpw2s0fcva3Uc5kbpePunUAnQFtb25F9GtXXw8GD/PR7XyarH+zzdr/MrumNDE6dHvm16oaGOG53P88fNS+Gmo26e+H7uPVdfwXA3665ibc//+S4bT5+2Y281Hg0b+5/jm/fnY3bzs3aO0D90EFemRF0ZRy3q58XG2dzoL7+MN9ZeTP37WHagf283HhUxfc1e88u9tdP4fVpMyq+r0prenUnvUcdc0R/z//lvC/w2+Yzmf9aH/9013+tXOUqqK5w1EGMv79xBP4O4MSC9aawrNQ2vWY2BTga6I9h38U6OuC22zj5pd7YXzpOR+99PdbXO7k/3uM9bvfoj6bl5RdKvv6UgwcAaNi/N/b9RzV34NWR5Tfs25NiTcabs+e1tKuQO0f693z8rqDjYO7rr2Tud7MswxfPxSCOju71wClmtsDMpgGXAKvHbLMa+HS4vAxYF3v/PQT9XJ///EjXjpSnfmj0UvYpXvqydhvzKJIVRw/uDh737k65JhHV1QV5FlP/PcTQwnf3A2Z2NXA/UA/c7u6bzewGoNvdVwPfBf7JzLYCLxF8KFTGt74V6xtUi6asfw5+8jgA9b/+NZw4e9w2df/jQXh1kLq2d8Ad2RwVJbXp6LVPw4O/56iPXAB3Xpd2dTIllj58d18DrBlTdn3B8iDwsTj2JZVXXzfabp9SV7oNrxa+ZNXRM4JhnGnOn5RV2Ru7KKmbUj/6h1I/UeCHf0z6o5KsOXrD+mDhrrtyP2No3BT4Ms6UgouDptZPFPjFjyKZ0NXF1O98J1h2H50xVKEPKPClhMJWff0EV4aOBH4SFRKZrBUrYN+Yi+xyPGNo3DI3Dl/SN2USffh1YeLXqYkvWbJ9O2fN3g8UzBYalosCX0qon0wf/vCj8l6ypLmZBT09PPO3S6n3oaJyUZeOlDCZFv7ISVt16kiWrFwJjY3FYd/YGJSLAl/GK+7D10lbyZH29mCysZaW4JezpSVYz+EtHitBXToyztT60XbAlIlO2g4/KvAla9rbFfATUAtfxilq4U8wLFMnbUXyR4Ev40yuD7/4UUSyT4Ev40xuagWdtBXJGwW+jFPYb6+TtiLVQ4Ev4xSG/ERz5WguHZH8UeDLOBN14xQa3kRxL5IfCnwZZ8oEI3MKDTfsJ/HZICIZocCXcSYae19o5KStunREckOBL+NMdKK2UJ1a+CK5o8CXcSbThz86PEeJL5IXCnwZZ6KrawtpagWR/FHgyzhHMkpHXToi+REp8M3sTWa21sx+Hz6+cYLtDprZhvBrdZR9SuVNpg9f0yOL5E/UFv5y4EF3PwV4MFwvZY+7Lwq/lkbcp1TY1EmM0qnTlbYiuRM18C8Gvhcufw/484ivJxlQN5kWPpotUyRvogb+se7+Qrj8B+DYCbZrMLNuM/uNmf35oV7QzDrCbbv7+voiVk8qRoN0RHLnsDdAMbMHgONKPFV0G3h3dzPzCV6mxd13mNlJwDoz2+juz5Ta0N07gU6Atra2iV5PUjZ60laJL5IXhw18dz93oufM7I9mNt/dXzCz+cDOCV5jR/j4rJn9HPgToGTgSz6MTo8sInkRtUtnNfDpcPnTwD1jNzCzN5rZ9HB5LvBu4ImI+5WUDZ/XVQNfJD+iBv7XgQ+a2e+Bc8N1zKzNzL4TbnMa0G1mjwEPAV93dwV+zumkrUj+RLqJubv3Ax8oUd4NfCZc/r/AmVH2I9kzcgOUdKshIkdAV9rKeF1do8utrcXrIVPii+SOAl+KdXVBR8foek9PsD4m9IdzXl06IvmhwJdiK1bAwEBx2cBAUF5ADXyR/InUhy9VaPt2AGbuHWDRC0+PKx823LJXC18kPxT4Uqy5GXp62HzTx8eXF9D0yCL5oy4dKbZyJTQ2Fpc1NgblBUZmy1Tgi+SGAl+KtbdDZye0tARp3tISrLe3F2020oevxBfJDXXpyHjt7eMCfizNnSaSP2rhS1nq1KUjkjsKfCmLabZMkdxR4EtZRlr4KddDRCZPgS/l0UlbkdxR4EtZNA5fJH8U+FKW0S4dJb5IXijwpSyjJ23TrYeITJ4CX8qiLh2R/FHgS1lGx+Er8UXyQoEv5RkZpZNuNURk8hT4UhadtBXJn0iBb2YfM7PNZjZkZm2H2O58M3vKzLaa2fIo+5RsGL3jVarVEJEjELWFvwn4S+AXE21gZvXArcAFwELgUjNbGHG/kjJTl45I7kSaLdPdt8BhT9wtAba6+7PhtncCFwNPRNm3pEtdOiL5k0Qf/gnAcwXrvWFZSWbWYWbdZtbd19dX8cpJeTQOXyR/DtvCN7MHgONKPLXC3e+Ju0Lu3gl0ArS1tXncry/xMPXpiOTOYQPf3c+NuI8dwIkF601hmeSYboAikj9JdOmsB04xswVmNg24BFidwH6lgob/9dJ8+CL5EXVY5l+YWS9wDnCfmd0flh9vZmsA3P0AcDVwP7AFuMvdN0ertmSF8l4kP6KO0rkbuLtE+fPAhQXra4A1UfYl2aSTtiL5oSttJRLNpSOSHwp8EZEaocCXSHTSViQ/FPgSifJeJD8U+FIWjcMXyR8FvpRF4/BF8keBL5Eo70XyQ4EvkWhYpkh+KPAlEsW9SH4o8KUsIydtlfgiuaHAl7LopK1I/ijwJRLlvUh+KPClLBqHL5I/Cnwpy8ityNTEF8kNBb6ISI1Q4IuI1AgFvohIjVDgS1nUcy+SPwp8KYsffhMRyZioNzH/mJltNrMhM2s7xHbbzGyjmW0ws+4o+xQRkfJEuok5sAn4S+Dbk9j2fe7+YsT9SUaoS0ckfyIFvrtvAc2YWIvUpSOSP0n14TvwMzN7xMw6DrWhmXWYWbeZdff19SVUPRGR6nfYFr6ZPQAcV+KpFe5+zyT38x5332FmxwBrzexJd/9FqQ3dvRPoBGhra1NDUkQkJocNfHc/N+pO3H1H+LjTzO4GlgAlA19ERCqj4l06ZjbTzGYNLwPnEZzslRzTWRuR/Ik6LPMvzKwXOAe4z8zuD8uPN7M14WbHAv/HzB4Dfgvc5+7/FmW/kj71tYnkT9RROncDd5cofx64MFx+FnhblP2IiEh0utJWyqIuHZH8UeCLiNQIBb6ISI1Q4EtZdNJWJH8U+CIiNUKBL2XRSVuR/FHgS1nUpSOSPwp8EZEaocCXsqhLRyR/FPgiIjVCgS8iUiMU+FIWnbQVyR8FvohIjVDgS1l00lYkfxT4IiI1QoEvIlIjFPgiIjVCgS8iUiMU+CIiNSLqTcz/p5k9aWaPm9ndZjZ7gu3ON7OnzGyrmS2Psk8RESlP1Bb+WuAMdz8LeBq4duwGZlYP3ApcACwELjWzhRH3K2nq6oIfdAXL110XrItI5kUKfHf/mbsfCFd/AzSV2GwJsNXdn3X3fcCdwMVR9isp6uqCjg7YvStY7+8P1hX6IpkXZx/+vwf+tUT5CcBzBeu9YVlJZtZhZt1m1t3X1xdj9SQWK1bAwADTDu4HYIoPwcBAUC4imTblcBuY2QPAcSWeWuHu94TbrAAOAJGbee7eCXQCtLW1acqWrNm+HYD/+MsfMO3Afj668cGichHJrsMGvrufe6jnzexy4CLgA+5eKqB3ACcWrDeFZZJHzc3Q08OsfXu49n/fUVwuIpkWdZTO+cA1wFJ3H5hgs/XAKWa2wMymAZcAq6PsV1K0ciU0NhaXNTYG5SKSaVH78G8BZgFrzWyDmf09gJkdb2ZrAMKTulcD9wNbgLvcfXPE/Upa2tuhsxNaWsAseOzsDMpFJNOsdC9MNrS1tXl3d3fa1RARyQ0ze8Td20o9pyttRURqhAJfRKRGKPBFRGqEAl9EpEYo8EVEakSmR+mYWR/QU+a3zwVejLE6eaBjrg065tpQ7jG3uPu8Uk9kOvCjMLPuiYYmVSsdc23QMdeGShyzunRERGqEAl9EpEZUc+B3pl2BFOiYa4OOuTbEfsxV24cvIiLFqrmFLyIiBRT4IiI1ouoC38zON7OnzGyrmS1Puz5xMbPbzWynmW0qKHuTma01s9+Hj28My83Mbg7fg8fN7O3p1bx8ZnaimT1kZk+Y2WYz+1JYXrXHbWYNZvZbM3ssPOa/CcsXmNnD4bH9KLy3BGY2PVzfGj7fmuoBRGBm9Wb2qJndG65X9TGb2TYz2xhOLd8dllX0d7uqAt/M6oFbgQuAhcClZrYw3VrF5g7g/DFly4EH3f0U4MFwHYLjPyX86gBuS6iOcTsA/LW7LwTeCXwh/HlW83HvBd7v7m8DFgHnm9k7gRuBb7r7ycDLwBXh9lcAL4fl3wy3y6svEdwzY1gtHPP73H1RwXj7yv5uu3vVfAHnAPcXrF8LXJt2vWI8vlZgU8H6U8D8cHk+8FS4/G3g0lLb5fkLuAf4YK0cN9AI/A44m+CKyylh+cjvOcGNhc4Jl6eE21nadS/jWJvCgHs/cC9gNXDM24C5Y8oq+rtdVS184ATguYL13rCsWh3r7i+Ey38Ajg2Xq+59CP9t/xPgYar8uMOujQ3ATmAt8Azwigd3j4Pi4xo55vD5V4E5iVY4HjcR3C51KFyfQ/UfswM/M7NHzKwjLKvo7/Zhb2Iu+eDubmZVOcbWzN4A/AT4sru/ZmYjz1Xjcbv7QWCRmc0G7gZOTbdGlWVmFwE73f0RM3tvytVJ0nvcfYeZHUNwm9gnC5+sxO92tbXwdwAnFqw3hWXV6o9mNh8gfNwZllfN+2BmUwnCvsvd/zksrvrjBnD3V4CHCLozZpvZcAOt8LhGjjl8/migP9maRvZuYKmZbQPuJOjW+Tuq+5hx9x3h406CD/YlVPh3u9oCfz1wSnh2fxpwCbA65TpV0mrg0+Hypwn6uIfLPxWe2X8n8GrBv4m5YUFT/rvAFnf/RsFTVXvcZjYvbNljZjMIzllsIQj+ZeFmY495+L1YBqzzsJM3L9z9WndvcvdWgr/Zde7eThUfs5nNNLNZw8vAecAmKv27nfaJiwqcCLkQeJqg33NF2vWJ8bh+CLwA7Cfov7uCoN/yQeD3wAPAm8JtjWC00jPARqAt7fqXeczvIejnfBzYEH5dWM3HDZwFPBoe8ybg+rD8JOC3wFbgx8D0sLwhXN8aPn9S2scQ8fjfC9xb7cccHttj4dfm4ayq9O+2plYQEakR1dalIyIiE1Dgi4jUCAW+iEiNUOCLiNQIBb6ISI1Q4IuI1AgFvohIjfj/svJjvV8trxUAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fu4MXvUgnSf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}